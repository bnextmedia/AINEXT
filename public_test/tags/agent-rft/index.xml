<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Agent RFT on AINEXT</title>
    <link>https://bnextmedia.github.io/AINEXT/tags/agent-rft/</link>
    <description>Recent content in Agent RFT on AINEXT</description>
    <generator>Hugo</generator>
    <language>zh-TW</language>
    <lastBuildDate>Thu, 25 Dec 2025 15:30:00 +0800</lastBuildDate>
    <atom:link href="https://bnextmedia.github.io/AINEXT/tags/agent-rft/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>訓練 AI 代理的四個不能妥協——OpenAI 的 Agent RFT 實戰指南</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-principles-guide/</link>
      <pubDate>Thu, 25 Dec 2025 15:30:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-principles-guide/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 OpenAI DevDay 的技術分享。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/p1CmPZ2j6Lk&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;OpenAI 的 Agent RFT 是一個強大的工具，但強大不代表簡單。在與多家企業合作的過程中，OpenAI 團隊累積了一些關於「什麼情況下 Agent RFT 會成功、什麼情況下會失敗」的經驗。這些經驗被濃縮成四個核心原則。這不是教條式的規則，而是從實戰中提煉出來的指引——違反這些原則不一定會失敗，但遵循這些原則會讓成功的機率大幅提高。&lt;/p&gt;&#xA;&lt;h2 id=&#34;原則一任務要明確可評分&#34;&gt;原則一：任務要明確可評分&lt;/h2&gt;&#xA;&lt;p&gt;「你的任務應該有一個清楚、不含糊的成功定義。」這是 OpenAI 給出的第一個原則，聽起來像是廢話，但實際上很多團隊在這一步就出問題。&lt;/p&gt;&#xA;&lt;p&gt;什麼叫「明確可評分」？意思是，給定一個模型的輸出，你應該能夠用程式碼自動判斷它是成功還是失敗，中間不需要人類的主觀判斷。如果你需要一個人看過輸出之後「憑感覺」決定分數，那這個任務對 Agent RFT 來說就不夠明確。&lt;/p&gt;&#xA;&lt;p&gt;這個原則背後的邏輯是：強化學習需要大量的嘗試和反饋。在訓練過程中，模型會產生成千上萬個軌跡（trajectory），每一個都需要被評分。如果評分需要人工介入，這個規模就不可能達成。更重要的是，人類的判斷有內在的不一致性——同一個輸出，今天和明天可能會給出不同的分數。這種不一致性會讓模型學到錯誤的東西。&lt;/p&gt;&#xA;&lt;p&gt;Cosine 的案例提供了一個很好的例子。他們一開始嘗試給模型「部分分數」，根據程式碼風格和嘗試的努力程度來給分。結果發現模型開始優化這些次要指標，而不是專注在「寫出能跑的程式碼」這個核心目標。他們最後改成一個完全二元的標準：程式碼通過測試就給分，沒通過就是零分。這個改變讓模型的行為變得更加對齊實際目標。&lt;/p&gt;&#xA;&lt;p&gt;「移除所有主觀性」——這是 OpenAI 給的建議。如果你的評分標準需要用到「好」、「適當」、「專業」這類詞彙，你可能需要重新思考怎麼把這些抽象概念轉換成具體、可測量的指標。品味不應該是評分的必要條件。&lt;/p&gt;&#xA;&lt;h2 id=&#34;原則二訓練資料要像生產環境&#34;&gt;原則二：訓練資料要像生產環境&lt;/h2&gt;&#xA;&lt;p&gt;「你不希望模型在生產環境中感到驚訝。」這是第二個原則的核心概念。你的訓練資料集和評估資料集應該準確反映你的生產流量分布，不能有任何偏移。&lt;/p&gt;&#xA;&lt;p&gt;這個原則說起來簡單，做起來卻經常出錯。常見的問題包括：訓練資料是手動捏造的、或者來自早期測試使用者、或者是從某個特定場景收集的。這些資料可能在技術上「正確」，但它們的分布和真實使用情境不同。模型在這種資料上表現很好，到了生產環境卻一塌糊塗。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Devin 背後的秘密：Cognition 如何用 Agent RFT 讓 AI 學會平行處理</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251225-cognition-devin-agent-rft-parallel-learning/</link>
      <pubDate>Thu, 25 Dec 2025 15:00:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251225-cognition-devin-agent-rft-parallel-learning/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 OpenAI DevDay 的技術分享。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/p1CmPZ2j6Lk&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Devin 是 Cognition 推出的 AI 程式碼代理，它能夠自主完成複雜的程式開發任務。但「自主完成」這件事背後有一個核心挑戰：代理要怎麼知道該修改哪些檔案？一個大型程式碼庫可能有成千上萬個檔案，選錯了檔案，後續的所有工作都是白費。Cognition 用 OpenAI 的 Agent RFT 來訓練這個「程式碼編輯規劃」的能力，結果不只是效能提升，而是讓 Devin 學會了一種全新的工作方式。&lt;/p&gt;&#xA;&lt;h2 id=&#34;問題的本質在龐大的程式碼庫中精準定位&#34;&gt;問題的本質：在龐大的程式碼庫中精準定位&lt;/h2&gt;&#xA;&lt;p&gt;當使用者給 Devin 一個任務——比如「修復這個 bug」或「加入這個功能」——Devin 需要先搞清楚要動哪些檔案。這個階段叫做「程式碼編輯規劃」，Devin 會檢視整個程式碼庫，執行一系列的 shell 工具（像是 grep、檔案讀取等），然後決定一個檔案清單。&lt;/p&gt;&#xA;&lt;p&gt;這個任務的困難之處在於，你必須在兩個方向上都做對。選太少，你會漏掉關鍵的依賴檔案或相關程式碼，導致後續的修改不完整。選太多，你會給下游的編輯模組製造額外的雜訊，拖慢處理速度，甚至可能導致不必要的改動。&lt;/p&gt;&#xA;&lt;p&gt;傳統的做法是透過提示詞工程來引導模型的行為，告訴它「要全面」或「要精確」。但這種方式有天花板——模型對你的程式碼庫沒有任何實際經驗，它只能依賴通用的程式設計知識來做判斷。Agent RFT 提供了一個不同的路徑：讓模型在你的真實環境中反覆嘗試，從成功和失敗中學習什麼才是「選對檔案」。&lt;/p&gt;</description>
    </item>
    <item>
      <title>10 個範例就能訓練出更強的 AI 代理——四家公司怎麼做到的？</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-companies-case-studies/</link>
      <pubDate>Thu, 25 Dec 2025 14:30:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-companies-case-studies/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 OpenAI DevDay 的技術分享。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/p1CmPZ2j6Lk&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;「我們看到有人只用 10 個範例就成功了。」OpenAI 微調團隊的 Will Hang 在介紹 Agent RFT 時提到這個數字。這聽起來有點不可思議——傳統的機器學習需要大量標註資料，怎麼可能 10 個範例就夠？但 Agent RFT 的邏輯不太一樣：你不是在教模型「看到 A 要回答 B」，而是讓模型自己探索你的環境，從成功和失敗中學習。這種方式對資料量的需求，確實可以非常低。&lt;/p&gt;&#xA;&lt;p&gt;OpenAI 分享了四家公司使用 Agent RFT 的實戰經驗。這些案例涵蓋了不同的應用場景和技術挑戰，從中可以看到一些共通的模式和值得借鏡的做法。&lt;/p&gt;&#xA;&lt;h2 id=&#34;cognition讓-devin-學會平行處理&#34;&gt;Cognition：讓 Devin 學會平行處理&lt;/h2&gt;&#xA;&lt;p&gt;Cognition 是開發 Devin 的公司，Devin 是一個能夠自主完成程式開發任務的 AI 代理。他們把 Agent RFT 用在 Devin 的「程式碼編輯規劃」階段——這是 Devin 檢視程式碼庫、決定要修改哪些檔案的環節。&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI 讓 AI 在訓練時操作真實世界——Agent RFT 是什麼？</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251225-openai-agent-rft-training-breakthrough/</link>
      <pubDate>Thu, 25 Dec 2025 14:00:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251225-openai-agent-rft-training-breakthrough/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 OpenAI DevDay 的技術分享。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/p1CmPZ2j6Lk&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;「這是我們第一次讓模型在訓練過程中與外部世界互動。」OpenAI 微調團隊的 Will Hang 在介紹 Agent RFT 時這樣說。這句話聽起來或許平淡，但它標誌著 AI 訓練範式的一個重要轉變：模型不再只是被動地學習靜態資料，而是在訓練過程中主動操作真實環境、接收真實反饋。&lt;/p&gt;&#xA;&lt;h2 id=&#34;agent-與一般模型有什麼不同&#34;&gt;Agent 與一般模型有什麼不同&lt;/h2&gt;&#xA;&lt;p&gt;要理解 Agent RFT 為什麼重要，首先得搞清楚什麼是 AI Agent。一般的語言模型像是一個知識淵博的顧問，你問它問題，它給你答案，但它無法替你「做事」。Agent 則不同，它能夠與外部世界互動，使用各種工具來完成任務，而且整個過程不需要你一步步指導。&lt;/p&gt;&#xA;&lt;p&gt;具體來說，一個 Agent 需要具備兩種能力的交織：工具呼叫和推理。想像你在使用一個程式碼 Agent，它不只是告訴你「應該這樣寫」，而是直接打開終端機、執行指令、讀取程式碼庫、甚至提交修改。在這個過程中，它的工具呼叫和思考過程是交錯進行的——呼叫一個工具、看到結果、思考下一步、再呼叫另一個工具。OpenAI 內部的 Codex 就是按照這個範式打造的，它能端對端地完成程式碼任務，從寫單元測試到提交大規模的程式碼變更。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
