---
title: "人類文明倒數計時：哈拉瑞預警AI將在數年內顛覆一切，重建信任是最後希望"
date: 2025-06-04T16:40:59+08:00
description: "以色列歷史學家尤瓦爾．哈拉瑞（Yuval Noah Harari）日前在慶應義塾大學發出震撼警告：人工智慧革命的速度遠超所有人預期，AGI（人工通用智慧）可能在數年內實現，而人類面臨的最大威脅並非AI本身，而是人類之間信任網絡的全面崩潰。這場對談揭露了一個驚人事實：當我們不信任彼此卻願意信任A..."
tags: ["AI KOL", "產業新知"]
categories: ["AI 技術前沿"]
source_url: ""
source_name: ""
draft: false
---

![封面圖](/images/posts/20250604-5189c5c6.jpg)

以色列歷史學家尤瓦爾．哈拉瑞（Yuval Noah Harari）日前在慶應義塾大學發出震撼警告：人工智慧革命的速度遠超所有人預期，AGI（人工通用智慧）可能在數年內實現，而人類面臨的最大威脅並非AI本身，而是人類之間信任網絡的全面崩潰。這場對談揭露了一個驚人事實：當我們不信任彼此卻願意信任AI時，文明正走向前所未有的轉折點。


{{< youtube 2QXGDj9SAnI >}}


## 四年前的警告：三大威脅中最被低估的那一個

四年前，當慶應義塾大學校長伊藤公平就任時，他在大學圖書館設立了「校長推薦必讀書」專區，哈拉瑞的《21世紀的21堂課》被擺在最顯眼的位置。在這本2018年出版的著作中，哈拉瑞提出了威脅人類文明的三大危機：核戰爭（特別是普丁的俄羅斯）、失控的生物科技（包括基因編輯）、以及資訊網絡和AI的暴走。

![](/images/posts/20250604-8f5d6b6c.jpg)

慶應義塾大學校長伊藤公平

當時，多數人認為AI是三者中最遙遠的威脅。核武的危險性在廣島長崎之後已深植人心，生物科技的風險也因其涉及人類本質而備受關注，但AI？那似乎還是科幻小說的情節。然而，在最新著作《連結》中，哈拉瑞選擇專注於資訊技術和AI，原因令人震驚：「如果比較AI和生物科技，最明顯的差異是AI的進化速度快得太多了。」

他解釋道：「生物科技當然也能劇烈改變世界，但需要更長時間。如果你改變人類嬰兒的基因組，想要看到對人類行為、智能、心理的影響，需要等待20、30、40年。但AI的一個世代可能只需要幾天。數位進化比有機進化快了數百萬倍。」

而與核武威脅相比，AI更加複雜難解：「核戰爭完全沒有正面效益，所以沒有人支持。但AI有巨大的正面潛力，這讓人們很難理解威脅在哪裡。」

## AGI倒數計時：從百年後到數年內的驚人加速

十年前撰寫《人類大歷史》時，哈拉瑞坦承：「感覺像是在談論百年後才會發生的事，某種哲學思辨，對當代生活沒有什麼直接影響。但現在，AI就在我們身邊，發展速度簡直令人震驚。」

這種速度有多驚人？連向來樂觀的未來學家Ray Kurzweil都成了「保守派」。Kurzweil曾預測AGI將在2029年實現，當時哈拉瑞認為「他太誇張了，不可能在2029年就實現」。如今，Kurzweil堅持2029年的預測，卻被認為是「較為保守的思考者」。

「你聽聽美國和中國企業開發人員的說法，他們談論的是一年內、五年內就會有AGI，」哈拉瑞說道。這不僅僅是技術進步的問題，更關乎人類監管能力的徹底失敗：「所有對AI進行監管、達成某種協議的希望，現在看來都極其天真。特別是在美國大選之後，達成全球協議來管理AI發展風險的希望，基本上已經破滅了。」

但更關鍵的是，AI的本質發生了根本性突破。哈拉瑞強調：「AI不是工具，它是代理人。這是人類史上第一個不是工具而是代理人的技術。」

他用核能作為對比：「原子彈或核反應爐都是我們手中的工具。我們決定如何使用——用核技術發電還是摧毀城市，都是人類的決定。核反應爐本身無法做任何決定。」但AI不同：「AI是世界上第一個無需我們幫助、無需我們介入就能自主做決定的技術。AI武器不需要人類告訴它該怎麼做，它可以自己決定轟炸什麼目標。」

人們常問AI何時能達到人類智能水準，哈拉瑞的回答令人深思：「答案是永遠不會。因為它根本不像我們。這就像問飛機何時能達到鳥類的飛行水準一樣——飛機不是鳥，它們的飛行方式完全不同。AI將超越人類智能，但它本質上是異質的。」

## 信任網絡的全面崩潰：精英、大眾與AI的三角困局

在AI革命的核心，哈拉瑞發現了一個令人不安的悖論——「信任的悖論」。

當他詢問AI革命的領導者們為何如此急速推進時，得到的回答總是相同：「我們理解巨大的風險，大到可能導致人類滅絕。我們希望能放慢腳步，但我們不能。因為如果我們放慢而其他公司、其他國家不放慢，他們就會贏得競賽。最無情的人將主宰世界，因為他們擁有了AI這項神奇技術。」

![](/images/posts/20250604-e61efb35.jpg)

但緊接著，當Harari問及是否信任他們正在開發的超級智能AI時，同一群人會說：「是的，我們能夠信任AI。」

「一分鐘前還說無法信任其他人類的人，突然變得極其信賴，說我們可以信任AI。這真的接近瘋狂，」哈拉瑞評論道。

這種矛盾暴露了更深層的信任危機。對人類而言，信任建立有其歷史基礎：「我們對其他人類有數千年的經驗。我們了解他們的心理、動機，我們知道人類對權力的渴望，但也了解制衡權力的機制。」人類社會從數萬年前只能信任幾十人的小部落，發展到今天日本這樣數億人口可以彼此信任的國家，全球貿易網絡連接80億人。

「但我們對超級智能AI的經驗是零，」哈拉瑞警告，「我們不知道它們會發展出什麼樣的目標和手段。」

這種信任危機也體現在社會分裂上。談到所謂的精英與民眾對立，哈拉瑞指出這本身就是一個虛假的二分法：「世界上最富有的人，像馬斯克和川普這樣的億萬富翁，聲稱他們反對精英。但如果他們都不是精英，那誰是精英？」

真正的問題不在於精英的存在——「每個人類群體都有精英，你無法經營任何團體，甚至足球俱樂部都一樣」——而在於精英是否為人民服務：「關鍵問題是，這是一個服務性精英還是自利性精英？」

更嚴重的是網路空間與物理空間居民的隔閡。哈拉瑞觀察到，越來越多人生活在網路空間，只閱讀280字元以下的推文，這正在摧毀深度思考的能力。教育機構面臨前所未有的挑戰：如何讓年輕世代保持閱讀厚重、引人思考書籍的能力？

關鍵在於理解資訊與真相的差異。「以前教育機構專注於提供人們資訊，因為資訊稀缺。現在完全相反，人們不需要更多資訊，我們被大量資訊淹沒了。」

哈拉瑞揭露了一個殘酷現實：「資訊不等於真相。真相是資訊中極稀少的子集合。如果你想到世界上所有的資訊，真相只佔很小很小的百分比。大部分資訊都是垃圾——虛構、幻想、謊言、宣傳。」

為什麼真相如此稀少？原因有三：「第一，真相昂貴，虛構便宜。要寫出真實的東西，你需要研究、分析、事實查核，耗費時間精力和金錢。虛構很便宜，想到什麼就寫什麼。第二，真相複雜，因為現實複雜。但人們不喜歡複雜的敘述，喜歡簡單的故事，而虛構可以做得很簡單。第三，真相經常令人痛苦，但虛構可以做得很討喜。」

國際秩序的崩潰更加劇了這種危機。1945年後建立的國際體系有一個最重要的禁忌：強國不能單方面入侵和征服弱國。「數千年來，這都是很普通的事——強國入侵鄰國建立更大的帝國。21世紀初人類享受史上最和平繁榮的時代，很大程度是因為這個禁忌得到維持。」

![](/images/posts/20250604-5b06c8bd.jpg)

但現在這個禁忌被打破了。「你可以從政府預算看得最清楚。歷史上大部分時間，每個國王、將軍、皇帝的預算50%以上用於軍事。21世紀初，政府平均軍費支出降到約6-7%，約10%用於醫療——這是人類史上第一次全世界政府在醫療上的支出超過軍費。」

但這正在逆轉。俄烏戰爭後，軍費預算飆升，犧牲的是醫療和教育支出。更糟的是川普政府的態度：「川普譴責烏克蘭引發戰爭，很明顯他腦中的世界秩序是弱者應該服從強者。如果弱者拒絕服從強者，弱者就要為由此產生的衝突負責。」

這種邏輯將把全人類帶回持續戰爭狀態，而在這種情況下，「任何監管AI的機會都將消失，因為每個國家都會說我們必須贏得AI競賽，不能做任何可能讓我們落後的事。」

## 從戰場到舞台：AI將如何重塑人類生活的每個角落

未來的圖景正在快速展開，而AI的影響將無處不在。

### 戰爭的變革：誰來決定「炸誰」？

在戰爭領域，變化已經開始。許多人期待看到好萊塢電影中的殺手機器人，但現實不是這樣：「電視上仍然是人殺人，真正改變的是誰在選擇目標——誰決定轟炸這棟建築，誰決定殺死這個人？答案越來越是：AI。」

這種變化在過去2-3年才真正開始：「以前，識別目標的工作由人類情報官員完成。他們會收集所有資訊，閱讀所有報告，花幾天時間說『這棟建築是敵人總部，轟炸它』。現在這個過程在幾秒內由AI完成。」

AI處理著人類無法分析的大量資訊，發現人類無法察覺的模式，在幾秒內就說「這是敵人總部，轟炸它」。但關鍵問題是：我們能相信AI嗎？也許它犯了錯誤，也許它告訴我們轟炸的是平民建築而非軍事目標？

哈拉瑞聽到不同的說法：「有些人說，AI指出目標後，我們會讓人類分析員檢查所有資訊，確保沒有錯誤。其他人說，人類分析員只花大約30秒，沒人能真正檢查AI，它處理太多數據點了。基本上他們只是橡皮圖章，做決定的是AI。」

### 娛樂產業的革命：突破人類想像的局限

娛樂領域的變革將更加深刻。哈拉瑞用2016年AlphaGo與李世乭的著名對戰作為例子：「比賽的驚人之處不只是AlphaGo擊敗了人類世界冠軍，而是它的下法。」

圍棋在東亞已有2000多年歷史，「數千萬人下過圍棋，它被視為一種藝術形式，不只是遊戲，還發展出了哲學。然而所有這些年，所有這些人，沒有人想過像AlphaGo那樣下棋。」

哈拉瑞用一個生動的比喻：「你可以想像成一個景觀，一個圍棋星球，所有不同的下棋方式。人類被困在圍棋星球的一個島嶼上，2000年來以為這就是整個星球。他們無法離開這個心理島嶼，因為我們的思維有限。然後AlphaGo出現說，看，你可以這樣下，可以那樣下，發現了圍棋星球上全新的區域。」

同樣的事情將在音樂、繪畫、戲劇、電視中發生：「音樂創作、影片製作都已經開始了，但這只是過程的開始。如果你覺得今天的AI很先進，與十年後相比將微不足道。」

更關鍵的是，這種變化無法預測：「預測AI會做什麼是不可能的，因為這就是AI的本質——它是一種異質智能形式。人類智能無法預測異質智能會做什麼。如果你能預測AI的一切行為，那它就不是AI，只是自動機器。」

他用咖啡機作比喻：「如果你有咖啡機，你能預測它的一切動作——按這個按鈕會出濃縮咖啡。這是自動機器，不是AI。什麼時候是AI？當你按按鈕，或者甚至不按按鈕，只是走近機器，機器告訴你『嘿，我今天早上發明了一種新飲料，我覺得你會比咖啡更喜歡，我給你做了一杯』。那才是AI。」

### 科學研究的轉型：跨領域整合的新時代

在科學研究領域，AI已經開始革命。物理學和化學的諾貝爾獎實際上都頒給了AI，特別是開發AlphaFold的團隊。

「他們是聰明絕頂的人，但我不確定他們是化學家。他們當然需要深度理解化學才能開發AlphaFold，但他們的背景不是化學。然而他們的創造完全改變了化學，也將改變生物學和藥物生產，透過解決預測蛋白質摺疊這個巨大科學謎題。」

這告訴我們的不只是AI將在越來越多科學領域超越人類，「而是它能夠打破障礙。人類能力有限，我研究歷史多年，沒時間深度學習物理或化學。這就是為什麼大學有分系——物理系、歷史系，你不能兼顧。」

![](/images/posts/20250604-fdb43d56.jpg)

尤瓦爾．哈拉瑞

但AI沒有這種限制：「AI可以帶來更全面的科學方法。也許不是AlphaFold，也許不是今天的AI，但十年後的AI。你可以有一個AI網絡，深度理解化學、經濟學和歷史，能夠結合來自這些不同學科的數據和模型。」

這引發了根本問題：「那麼人類在這種情況下的角色是什麼？我們不知道。考慮到發展的速度和潛力，不清楚十年或二十年後人類還能對研究貢獻什麼。」

### 金融系統的異化：後人類經濟的降臨

最令人不安的變化可能出現在金融領域。哈拉瑞首先分析了貨幣——「可能是人類發明的最成功故事」——的演變。

美元是人類貨幣，由銀行等人類機構發行。「但像比特幣、以太坊這樣的加密貨幣是後人類貨幣，由演算法產生。說『我不信任美元，我信任比特幣』的人，實際在說『我不信任人類，我信任演算法』。」

這種趨勢正在擴展：「同時你看到馬斯克和川普解雇數萬名人類官僚，但他們並非真正反對官僚制，只是用數位官僚——演算法——取代他們。這是權力從人類向演算法的轉移。」

社群媒體已經展現了這種轉移：「以前，世界上流傳的所有故事都來自人類大腦，是人類編輯決定什麼故事主導對話。現在這個工作也由演算法完成。如果你問世界上最重要的編輯是誰，答案是TikTok、Facebook、Twitter的編輯——他們不是人類，是演算法。」

更進一步的發展令人震驚：「下一步是數位貨幣，其價值只取決於AI和演算法的信念。今天已經有大量交易，特別是債券、股票、外匯交易，只是演算法與其他演算法交易。」

如果金融市場去監管化（川普政府的目標），「我們可能很快看到新的金融工具、新型貨幣，只有AI理解，AI與其他AI交易。當人類不再理解金融時，世界會怎樣？」

Harari用馬匹的比喻：「數千年來人們買賣馬匹，但馬從不理解發生了什麼。馬能看到一個武士給商人一塊閃亮金屬，然後得到了我這匹馬。但這塊毫無價值的金屬是什麼？馬永遠不理解金錢，只有人類理解，這就是為什麼我們控制馬而不是相反。」

「十年後，我們的生活可能被我們無法理解的演算法間的金融交易管理。我們是否能得到買房貸款，將由演算法決定，它們用對人類來說難以理解的金融語言溝通——不是因為我們沒上大學，而是因為演算法創造的數學模型對有機人腦來說太複雜了。」

這對民主制度意味著什麼？「如果總理或財政部長都不再理解金融，會怎樣？發生金融危機但沒有人類理解為什麼，也不知道如何應對。」即使對獨裁者也是威脅：「每個獨裁者最大的恐懼是下屬變得比自己更強大。如果AI變得如此強大以至於人類獨裁者無法理解，這對人類獨裁者也是威脅。」

### 共主觀現實的重構：假人類的氾濫

在共主觀現實層面，變化已經開始。「維持人類信任與合作的第一個也是最簡單的事，每個政府都應該做的，是通過法律禁止假人類，禁止偽造人類，就像禁止偽造貨幣一樣。」

「直到今天，你無法創造假人類。現在你可以了。機器人可以偽裝成人類。TikTok和Twitter的大量流量都由機器人運行，但偽裝成人類。」

解決方案很明確：「第一個最簡單的步驟是制定嚴格法律。你不能偽造人類。AI歡迎與我們對話，條件是它們表明AI身份。如果有人試圖偽造人類，或平台不對假人類採取行動，他們應該坐牢，就像有人用印刷機印假幣一樣。」

## 呼吸的隱喻：信任是生命的基礎

面對如此全面的挑戰，重建信任成為關鍵。Harari強調：「我認為最緊迫的研究主題是如何在人類之間建立信任，因為我們現在看到的是人類信任的崩潰。當我們最需要信任時，世界上最大的危險仍然不是AI，而是人類之間的不信任。」

解決這個問題需要跨學科合作：「沒有單一學科能獨自處理這個問題。你需要生物學和心理學的投入，因為我們仍然是動物，需要理解我們的動物遺傳；你可以從黑猩猩如何建立或無法建立彼此信任中學到很多。你需要經濟學輸入，當然還有電腦科學，因為今天所有社會系統都基於新的資訊技術。」

關鍵是理解新環境：「很多十年、二十年前在人類間建立信任非常成功的經驗，現在已經過時了，因為現在人類透過電腦溝通。如果我們不理解這個現在調解我們關係的新技術，我們就無法重建信任。」

過去十到二十年人類信任崩潰是一個悖論：「按照大多數數據——兒童死亡率、疾病、飢荒——一切都在下降。我們處於有史以來最好的狀況，但人們如此憤怒、沮喪，無法彼此交談。發生了什麼？新技術介入了人類之間，現在大部分人類溝通都由非人類代理調解，這在世界上造成了混亂。」

要重建信任，「我們不能回到過去，不能說『讓我們扔掉所有智慧型手機和電腦』，那行不通。我們需要重新學習如何在大量人群之間建立信任，在資訊技術的調解下。」

哈拉瑞以一個深刻的隱喻結束：「信任真正是生命的基礎。如果你不信任他人，如果你不信任外在於你的東西，你連一分鐘都活不了。因為即使我們呼吸的空氣，這個維持生命的簡單動作，是什麼？它是對外在於我們的東西的信任。」

「我們有這些當今世界最有權力的人，他們想要分離，他們想的就是分離事物、建立屏障和圍牆。但完全分離就是死亡。每一刻我們都有這個對外在事物信任的簡單姿態——我們張開嘴或鼻子，吸入外界的東西。我們信任它，然後把它還回去。這是維持生命的信任進出運動。如果你不信任外界而鎖住嘴和鼻子，你會在一兩分鐘內死去。」

## 倒數計時中的選擇：信任重建 vs 文明終結

當Harari被問及對學生的訊息時，他強調並非要製造恐慌：「談論的很多事情聽起來很可怕。我發現自己在這樣的場合主要談論危險，部分問題在於開發AI的公司裡的人談論AI的所有正面潛力。為了平衡，你需要哲學家、歷史家、批評家說『等等，也有危險』。」

「當然也有巨大的正面潛力，否則技術不會發展。從醫學到防止災難性氣候變化，都有巨大的正面潛力。問題不是如何停止AI發展——這不現實也不可取，我們想要發展它，只是希望以安全的方式發展。這裡的關鍵再次是在人類之間建立信任。」

對於個人的作用，他給出了務實的建議：「每個人在特定領域都能做一些事。有時人們感到被世界的重量壓垮——『所有這些問題，我們怎麼辦？』不要覺得整個世界都壓在你的肩膀上。地球上有80多億其他人類，他們分擔著支撐地球的重量。當然不只是人類，還有動物、植物和其他生物體，每個都做著各自的小事。」

最終，時間窗口正在縮小，但希望仍在：「只要我們還沒到那個地步，就仍有希望。如果有足夠多的人在未來幾個月和幾年裡做正確的事，人類仍會沒事。」

在這場文明的倒數計時中，人類面臨著前所未有的選擇：是重建彼此間的信任網絡，還是任由其繼續崩潰直至文明終結？答案就在我們每個人的行動中，而時間，正在一分一秒地流逝。

---

**來源：本文根據Yuval Noah Harari於慶應義塾大學的對談內容整理，內容涵蓋AI發展、人類信任危機、未來社會變革等核心議題。**