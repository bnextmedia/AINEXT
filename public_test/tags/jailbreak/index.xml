<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jailbreak on AINEXT</title>
    <link>https://bnextmedia.github.io/AINEXT/tags/jailbreak/</link>
    <description>Recent content in Jailbreak on AINEXT</description>
    <generator>Hugo</generator>
    <language>zh-TW</language>
    <lastBuildDate>Mon, 22 Dec 2025 22:25:00 +0800</lastBuildDate>
    <atom:link href="https://bnextmedia.github.io/AINEXT/tags/jailbreak/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深度學習教父的 AI 安全方案——為什麼「目標驅動架構」比微調更安全？</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251222-yann-lecun-objective-driven-ai-safety/</link>
      <pubDate>Mon, 22 Dec 2025 22:25:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251222-yann-lecun-objective-driven-ai-safety/</guid>
      <description>&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/7u-DXVADyhc&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Information Bottleneck Podcast EP20 對 Yann LeCun 的專訪。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;AI 安全是當前最熱門的話題之一。各大實驗室花費大量資源做 RLHF（人類反饋強化學習）、Constitutional AI、紅隊測試，試圖讓他們的模型更安全、更不容易說出有害的內容。&lt;/p&gt;&#xA;&lt;p&gt;但 Yann LeCun 認為，這些方法從根本上就是錯的。&lt;/p&gt;&#xA;&lt;p&gt;在最近的 Information Bottleneck 訪談中，這位圖靈獎得主提出了一個不同的思路：AI 安全不應該靠事後的微調和過濾，而應該從架構本身就保證安全。這個想法的核心是他一直在推動的「目標驅動架構」（objective-driven architecture）——一個與當前 LLM 範式根本不同的 AI 系統設計方式。&lt;/p&gt;&#xA;&lt;h2 id=&#34;llm-安全的根本困境&#34;&gt;LLM 安全的根本困境&lt;/h2&gt;&#xA;&lt;p&gt;為什麼 LLM 這麼難做到安全？LeCun 的分析直指問題核心：微調不是根本解法。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
