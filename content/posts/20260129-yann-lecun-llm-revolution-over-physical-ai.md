---
title: "楊立昆：LLM 革命結束了，下一波是理解真實世界的 AI"
date: 2026-01-29T10:00:00+08:00
description: "圖靈獎得主楊立昆在達沃斯論壇直言：LLM 已觸及天花板，用 LLM 建 Agent 是災難。他離開 Meta 創立 AMI，押注 World Models 與 Physical AI，認為這才是通往真正智慧的路徑。"
tags: ["楊立昆", "Yann LeCun", "LLM", "World Models", "Physical AI", "AMI", "JEPA", "Meta", "達沃斯"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=MWMe7yjPYpE"
source_name: "World Economic Forum"
draft: false
---

> 本文整理自 2026 年 1 月達沃斯世界經濟論壇的訪談。

{{< youtube MWMe7yjPYpE >}}

---

## 「AI 教父」的叛逆宣言

當全世界都在追逐更大的語言模型、更強的 Agent 系統時，一位 AI 界最有資格發言的人，卻站在達沃斯的舞台上說：這條路走不通。

楊立昆（Yann LeCun）是卷積神經網路（CNN）的發明者，1990 年代他開發的 LeNet 系統曾處理美國超過一成的銀行支票。2018 年，他與 Geoffrey Hinton、Yoshua Bengio 共同獲得圖靈獎，被譽為「深度學習三巨頭」。他在 Meta（原 Facebook）擔任首席 AI 科學家長達 12 年，是這家公司 AI 研究的靈魂人物。但在 2025 年 11 月，這位 65 歲的法國裔美國科學家宣布離開 Meta，創立了自己的公司 Advanced Machine Intelligence（AMI）Labs。他為什麼走？因為他認為，整個產業正在走一條錯誤的路。

這不是一時氣話。楊立昆對 LLM 路線的批評已經持續好幾年，但過去他的身份是大公司的首席科學家，說話多少要顧及公司立場。現在他自己出來創業，話說得更直接了。在達沃斯的訪談中，他開門見山地表達核心觀點：我們不會透過擴大規模或精煉現有典範，就達到人類等級的智慧，需要的是典範轉移。這話說得很重，因為過去幾年 AI 產業的主流敘事正是「Scaling Laws」——只要模型夠大、資料夠多、算力夠強，能力就會持續提升。OpenAI、Anthropic、Google 都在這條路上競速，而楊立昆說，這條路有盡頭。

---

## LLM 的天花板：預測下一個字不等於理解世界

楊立昆批評 LLM 的理由很根本：真實世界比語言世界複雜太多了。這聽起來有點反直覺，因為人類總覺得語言是智慧的巔峰——我們用語言思考、用語言傳遞知識、用語言創造文明。但楊立昆指出，預測文字中的下一個詞，其實沒那麼難。LLM 能累積大量知識，這是真的，但這不代表它們理解這個世界。真正的感官資料——影片、聲音、觸覺——是高維度、連續、充滿雜訊的，現有的生成式架構無法有效處理這類資料。

他舉了一個很有說服力的例子：一個 17 歲的青少年可以在 10 小時內學會開車，但自駕車公司用了數百萬小時的訓練資料，仍然無法達到 Level 5 全自動駕駛。這個效率差距不是小問題，而是反映了根本的架構缺陷。差別在於人類有「世界模型」（World Model）——我們的大腦會建立對物理世界的預測模型，知道如果做了某個動作，世界會怎麼變化。這讓我們能夠用極少的經驗快速學習新任務。機器目前沒有這種能力，所以需要海量資料才能勉強完成任務，而且一旦遇到訓練時沒見過的情況就會出錯。

這個論點延伸到 2025 年最熱門的「Agentic AI」。各家公司都在嘗試讓 AI 不只是回答問題，而是能自主完成複雜任務——瀏覽網頁、操作軟體、執行工作流程。但楊立昆對此潑了一大盆冷水，他說把 Agentic 系統建立在 LLM 之上，根本是災難的配方（a recipe for disaster）。他的邏輯很清晰：一個系統如果無法預測自己行動的後果，怎麼可能規劃一連串的行動？LLM 擅長的是根據上下文預測下一個 token，但它沒有內建的因果推理能力，沒有對物理世界的理解，無法想像「如果我做了 A，世界會變成什麼樣子」。這解釋了為什麼現在的 AI Agent 常常「翻車」——它們可以產出看起來合理的計畫，但執行過程中一旦遇到預期外的狀況，就容易陷入無限迴圈或做出荒謬的決定，因為它們沒有真正理解自己在做什麼。

---

## 下一波革命：Physical AI 與世界模型

楊立昆預告，下一波 AI 革命即將到來，而且來得很快。他稱之為「Physical AI」——能理解真實世界的 AI 系統。這類系統不是從文字學習，而是從影片和感測器資料學習，能處理高維度、連續、充滿雜訊的真實世界資料。更重要的是，它們能建立預測模型，預測環境將如何演變，以及自己的行動會造成什麼影響。有了這樣的世界模型，系統就能規劃一連串的行動來達成目標，而且是可控制、可預測的——你給它一個任務，它就去完成，不會偏離軌道。

楊立昆用了一個生動的比喻來解釋什麼是世界模型：如果你丟一顆球到空中，球突然停住或消失，你會覺得不對勁。這是因為你的大腦有一個物理世界的模型，知道球「應該」怎麼運動。現有的 LLM 沒有這種能力，它們可以生成關於球的文字描述，但不知道球在真實世界中會怎麼移動。Physical AI 需要有這種能力，才能真正在真實世界中行動。

他和團隊已經開發出一些原型系統，可以透過無標註的影片進行自我監督學習。這些系統能理解影片內容、預測缺失的部分，而且已經展現出某種程度的「常識」——當影片中發生不可能的事情時，系統的預測誤差會暴增，因為它「知道」這不符合它對世界的理解。這是一個很有意義的里程碑，代表系統開始獲得某種直覺式的物理常識，而不只是統計上的模式匹配。

---

## AMI：從 Meta 帶走的火種

楊立昆的新公司 AMI Labs，其實是他在 Meta 內部推動多年的研究計畫的延續。他透露了一個有趣的細節：在 Meta 的 12 年裡，他是「沒有人匯報給他」的個人貢獻者（individual contributor）。人們加入他的計畫，是因為想和他一起工作，不是因為他是老闆。他認為這才是研究環境應該有的樣子——由下而上，不是由上而下。研究這件事，你不能告訴研究者該做什麼，只能創造環境讓好的想法自然浮現。

AMI 的核心技術是 JEPA（Joint Embedding Predictive Architecture），一種非生成式的架構。它不像 LLM 那樣預測下一個 token，而是在一個抽象的表徵空間（representation space）中進行預測。這個設計有深刻的哲學意涵。楊立昆舉了一個例子：理論上，你可以用量子場論來解釋這個房間裡發生的一切，包括每個人的思維過程，但這完全不實際。我們理解房間裡發生的事，靠的是心理學、社會學、經濟學這些高層次的抽象概念，而不是粒子物理。同樣的道理，如果你想模擬一個系統卻模擬得太精確，你反而無法做出有用的預測。「數位孿生」（Digital Twin）的概念有其限制——真正有用的模型，必須在適當的抽象層次上運作。這就是為什麼生成式模型（試圖重建每一個像素）在處理真實世界時效率不彰，而 JEPA 的方法是學習如何在適當的抽象層次上表徵和預測。

AMI 的目標，是把這套方法泛化到任何模態、任何感測器資料。如果成功，就能為各種複雜系統建立現象學模型——工業製程、化學反應、噴射引擎、甚至活細胞。根據報導，AMI 的估值目標高達 35 億美元，產品都還沒推出就要募這麼多錢，可見資本市場對楊立昆的押注。公司總部設在巴黎，楊立昆擔任執行董事長（Executive Chairman），CEO 是 Alex LeBrun。雖然楊立昆離開了 Meta，但據報導兩家公司會維持合作關係——這種「分手後還是朋友」的安排，也許反映了雙方都知道徹底切斷關係對誰都沒好處。

---

## 我的觀察

楊立昆的觀點並不新鮮——他講 World Models 已經講了好幾年，2022 年就發表過一篇 60 頁的願景論文。但這次在達沃斯的發言有幾個特別之處。

首先，他現在是創業者了。過去他可以從學術高度批評產業方向，現在他必須用自己的公司證明這條路走得通。AMI 的估值目標據報導高達 35 億美元，還沒正式推出產品就要募這麼多錢，壓力可想而知。這不再只是學術辯論，而是真金白銀的賭注。其次，他的判斷正在被市場檢驗。2025 年是 AI Agent 大爆發的一年，各家公司都在推 Agentic 產品。如果這些產品真的頻繁「翻車」、無法處理真實世界的複雜性，楊立昆的警告就會被驗證；反過來說，如果 Agent 技術持續進步、找到繞過限制的方法，他的論點就會顯得過於悲觀。目前看來，Agent 的表現確實參差不齊，但也有一些令人印象深刻的應用案例，結論還言之過早。

第三，Physical AI 對台灣硬體產業是個值得注意的信號。如果下一波 AI 革命真的是從「純軟體」轉向「理解物理世界」，那麼感測器、機器人、邊緣運算這些領域就會變得更重要。台灣在這些領域有深厚的製造基礎，問題是能不能抓住這個轉向。過去幾年台灣在 AI 的角色主要是硬體供應商（晶片、伺服器），如果 Physical AI 成為主流，也許會有更多應用層面的機會。

楊立昆說，AGI 不會明年到來，也不會兩年內到來，因為還需要幾個概念突破。他同時也說，這些突破會藏在「沒人注意的論文裡」，直到五年後有人證明它們的威力。這讓我想到深度學習的歷史：卷積神經網路的核心概念在 1990 年代就有了，但要到 2012 年 AlexNet 才讓全世界驚醒。也許 JEPA 或類似的架構，也在等待它的「AlexNet 時刻」。而這個時刻到來時，很多人會說「這不是早就有了嗎？」——對，早就有了，只是沒人當回事。

---

**延伸閱讀**：
- [楊立昆 2022 年願景論文](https://openreview.net/pdf?id=BZ5a1r-kVsf)（A Path Towards Autonomous Machine Intelligence）
- [AMI Labs 相關報導](https://fortune.com/2025/12/19/yann-lecun-ami-labs-ai-startup-valuation-meta-departure/)
