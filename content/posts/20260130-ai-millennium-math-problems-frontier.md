---
title: "當 AI 遇上千禧年難題：為什麼最聰明的 AI 離真正的數學突破還很遠"
date: 2026-01-30T10:00:00+08:00
description: "多倫多大學數學教授 Daniel Litt 在 Epoch AI Podcast 中分析 AI 解千禧年問題的可能性。他估計 AI 在 2030 年前自主產出頂級數學論文的機率只有 25%，並指出 AI 至今沒有展示過真正的數學「新想法」。"
tags: ["Daniel Litt", "Epoch AI", "FrontierMath", "千禧年問題", "AI 數學"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=jFJku8sxLWY"
source_name: "Epoch After Hours"
draft: false
---

> 本文整理自 Epoch After Hours 2026 年 1 月 29 日發布的單集。

{{< youtube jFJku8sxLWY >}}

{{< spotify "episode/1brOmv2FYLzH6TdEZacZ3S" >}}

{{< apple-podcast "tw/podcast/ai-math-capabilities-could-be-jagged-for-a-long/id1790976895?i=1000747213829" >}}

---

## 25% 的機率：一位數學家的冷靜預測

多倫多大學數學教授 Daniel Litt 在 Epoch AI 的 Podcast 中，被問到一個很多人在乎的問題：AI 什麼時候能自主完成一篇頂級數學期刊等級的研究？他的回答是：大約 25% 的機率在 2030 年前發生。

這個數字值得細想。Litt 不是隨口說的。他是代數幾何與數論的專家，在數學界最頂級的期刊 *Annals of Mathematics* 發表過論文，也為 Epoch AI 的 FrontierMath 基準測試貢獻過題目。他親眼看過 AI 解他的題目，知道模型做得到什麼、做不到什麼。25% 這個數字反映的是一種有限度的樂觀：不是不可能，但遠沒有某些人宣稱的那麼近。

更重要的是他對「自主完成」這四個字的強調。他要求的不是 AI 在人類引導下幫忙跑一些計算，也不是 AI 生成一段看似合理的證明草稿，而是從提出問題到完成證明、全程不需要人類介入的獨立研究。以目前的進展來看，這離現實還有一段距離。

## 千禧年問題的困難，遠超 AI 的射程範圍

要理解 AI 離真正的數學突破還有多遠，千禧年問題是最好的參照。這七個問題是克雷數學研究所在 2000 年提出的，每個懸賞一百萬美元，至今只有一個被解決（龐加萊猜想，由俄羅斯數學家佩雷爾曼在 2003 年證明，他還拒絕了獎金）。

Litt 在訪談中談到了其中兩個問題的現狀。黎曼猜想是最著名的那個，關於質數的分布規律。他說，幾乎每個數學家都「偶爾想一下」黎曼猜想，但想歸想，真正著手嘗試又是另一回事。你需要的不只是想到「我要解這個問題」，你需要一個真正值得嘗試的想法，而大多數時候你根本沒有那樣的想法。Hodge 猜想則是另一個極端，Litt 說大多數代數幾何學家「相信它是對的」，但也有不少人持懷疑態度，而且目前幾乎沒有被普遍認可的進攻路線。

這些問題的難度不是「再多算幾步」的問題。它們需要全新的數學概念、全新的工具、全新的理論框架。過去一百年裡解決的重大數學猜想，幾乎每一個都是在好幾代數學家的工作基礎上、在多個領域的突破同時到位之後才被解決的。費馬最後定理的證明用到了橢圓曲線、模形式、伽羅瓦表示論等多個領域幾十年的積累，不是某個天才靈光一閃的結果。

## AI 至今沒有展示過真正的「新想法」

Litt 在訪談中反覆強調一個觀察：AI 到目前為止，還沒有在數學中展示過真正的「新想法」。

什麼算是「新想法」？他舉了幾個例子來說明。數學中的「多項式方法」（polynomial method）就是一個新想法，它讓人可以用完全不同於以往的方式去攻克組合數學和數論中的問題，後來被廣泛應用到許多領域。這種想法不只是解決了一個特定問題，而是打開了一整扇門。相比之下，AI 目前做的事情，即使很厲害，也是在已知的框架內操作。

他特別提到 Google DeepMind 的 AlphaEvolve。這個系統確實做出了一些令人印象深刻的數學構造，比如找到更好的矩陣乘法算法。但 Litt 的評價是：這些構造本身的數學「趣味性有限」，它們之所以引人注意，主要是因為是一個自動化系統做到的。他可以想像未來的版本做出真正有獨立價值的構造，但目前還沒有到那個程度。

訪談中還討論了 2024 年 IMO 的第六題。AlphaProof 解出了這道被認為最難的題目，但 Litt 和 Epoch 的研究員都注意到，那個證明看起來「出奇地無聊」。它是一個暴力推導，沒有任何優雅的想法。這不代表它不是一個有效的證明，但它也不代表模型具備了創造性推理的能力。就像 Litt 說的，有時候一個問題看起來很難，解出來之後才發現解法並不怎麼有趣，人類數學家也會遇到這種情況。他和合作者 Aaron Landesman 曾經解決了一個四十年的未解問題，但因為解法本身不夠「有趣」，他們沒有把它投稿到最頂級的期刊。

## FrontierMath 的侷限：出題者的題目永遠不夠難

Litt 作為 FrontierMath 的出題者，對這個基準測試的侷限有很坦率的反思。

核心問題是：每個出題者都是忙碌的人。你被要求出一道「很難」的題目，但你不會為此花三個月做一個全新的研究項目。你會出一道自己已經知道怎麼解的題，而一道某個人已經知道怎麼解的題，「幾乎必然」可以用現有技巧完成。這就意味著，只要相關技巧出現在模型的訓練資料中，模型就有可能解出來，不需要任何真正的創造力。

他舉了一個更具體的陷阱。有些人為了出「難題」，會跑到自己不熟悉的領域去出題。結果他們覺得很難的東西，對那個領域的專家來說可能根本不算什麼。這就像一個代數幾何學家跑去出數論題，自己覺得很有深度，但數論專家看了覺得只是基本操作。

FrontierMath 有一位出題者的做法讓 Litt 覺得比較好：那個人把出題當成一個兩週的小型研究項目，不是出一道自己已經會的題，而是先去探索一個方向，看看能不能在探索過程中自然產生一道好的問題。這種方式出來的題目品質更高，因為連出題者自己事前都不知道答案，題目的難度更能反映真正的數學難度。

另一個根本性的問題是時效性。即使某道題目在出題時確實需要一個新技巧，那個技巧遲早會出現在公開論文裡，然後進入訓練資料。到那時候，模型能解出這道題就不再代表它有創造力，只代表它記住了正確的論文。

## 什麼信號會讓他真正興奮

Litt 明確列出了幾個標準，如果 AI 做到這些，他會認為是真正的突破。

第一是解決 Epoch AI 正在準備的「Open Problems Benchmark」中的問題。這個基準測試和 FrontierMath 不同，收錄的是目前沒有任何人類知道答案的數學問題。如果 AI 能在這上面取得進展，那就不只是「記住了正確答案」的問題，而是真的做了某種新的東西。他特別提到了逆伽羅瓦問題中 M23 群的情形，他認為這個問題「理論上在可及範圍內」，如果 AI 能解出來，他會和人類解出來一樣興奮。

第二是產生有「後續生產力」的想法。他提議了一個評估方式：等五年，看看有多少新的數學結果是建立在 AI 產出的想法之上的。如果一個 AI 的解法引入了一個新技巧，而這個技巧後來被人類數學家反覆使用，那就是真正有價值的貢獻。組合數學中的多項式方法就是這樣的例子：它因為後續的巨大生產力而被認定為真正重要的創新。相比之下，如果一個解法只是暴力推導，沒有人從中學到任何東西，那價值就有限。

第三也是最根本的，他想看到 AI 展示出一種能力：面對一個全新的數學對象（比如最近十年才被發明的某種新型空間），不只是「學會它的定義」，而是能夠像人類數學家一樣去「玩弄」它、探索它的性質、發展出操作它的直覺。目前的模型在面對訓練資料中沒有出現過的新概念時，表現會大幅下降，而真正的數學研究者幾乎每年都要面對全新的工具和概念。

## 數學不會是最後一塊倒下的骨牌

在訪談尾聲，主持人問了一個很多人在想的問題：數學會不會是 AI 最後才征服的領域？也就是說，有沒有可能 AI 已經取代了大部分白領工作，但數學家還在悠哉地做研究？

Litt 認為這「相當不可能」。他的理由很實際：做數學研究需要的那些能力，和做其他經濟上有價值的工作需要的能力，本質上是一樣的。你需要有創意、需要長時間專注在一個問題上、需要適應新的工具和概念、需要在不確定中做判斷。如果 AI 能解決這些問題，它就能做好數學研究；如果它不能解決這些問題，它也做不好其他需要深度思考的工作。

換個角度想：AI 目前做不好長期數學研究的原因，和它做不好六個月軟體工程專案的原因，其實是同一個。都是缺乏長期規劃、缺乏在不確定環境中的判斷力、缺乏真正從零建立理解的能力。數學只是把這些缺陷放大到了最明顯的程度，因為在數學中你無法靠「差不多正確」混過去。

## 我的觀察：基準測試的「知識 vs 推理」困境，和我們每天在做的事有什麼關係

Litt 對 FrontierMath 的分析揭示了一個更普遍的問題：當我們用基準測試評估 AI 時，我們到底在測什麼？

他指出，AI 在 FrontierMath 上的好表現，很大程度上是因為它「已經知道答案的中間步驟」。人類解同一道題時，發現那些中間步驟本身就是推理的核心。但 AI 跳過了這個環節，因為它在訓練資料中已經見過。這就好像考試前偷看了一半的答案，然後在考場上表現得很好，你很難判斷他是真的會，還是只是記性好。

這對我們日常評估 AI 工具有直接的啟示。當一個 AI 工具在某個 benchmark 上表現優異時，你該問的第一個問題不是「它的分數有多高」，而是「這個 benchmark 到底在測什麼？」它是在測這個工具遇到全新問題時的應變能力，還是在測它的訓練資料裡有沒有涵蓋類似的情境？

Litt 的洞察是：隨著 AI 模型的訓練資料越來越完整，基準測試的成績會持續提升，但這不一定代表模型的「推理能力」有同等的進步。它可能只是代表訓練資料的覆蓋面更廣了。這是一個每個使用 AI 工具的人都該放在心裡的提醒。
