---
title: "Scaling Law 的下一章：讓 AI 自己做實驗"
date: 2025-12-26T12:00:00+08:00
description: "Pre-training 的 Scaling Law 正在飽和，下一個突破在哪？Lila Sciences 技術長 Andy Beam 認為答案是讓 AI 自己做實驗。他們正在打造結合 GPU 叢集與自動化實驗平台的新系統，目標是讓 AI 能生成自己的訓練資料。這是對 Scaling Law 的全新詮釋。"
tags: ["Scaling Laws", "Lila Sciences", "Andy Beam", "AI 科學研究", "Pre-training", "Podcast"]
categories: ["AI 產業"]
source_url: "https://podcasts.apple.com/tw/podcast/can-ai-accelerate-science-dr-andy-beam-on-ais-next-frontier/id1657518313?i=1000717523879"
source_name: "NEJM AI Grand Rounds"
draft: false
---

> 本文整理自《NEJM AI Grand Rounds》2025 年 7 月播出的單集。

{{< apple-podcast "tw/podcast/can-ai-accelerate-science-dr-andy-beam-on-ais-next-frontier/id1657518313?i=1000717523879" >}}

---

大型語言模型能通過美國醫師執照考試。這件事在 2020 年還是科幻小說，到了 2024 年已經沒有人會驚訝。但這裡有一個問題：通過考試是一回事，發現新藥是另一回事。

LLM 能告訴你教科書裡寫了什麼，但它不能告訴你教科書還沒寫的東西。它能從現有文獻中找出最合理的假說，但它不能告訴你哪個假說是對的。要知道答案，你還是得做實驗。

這是 Lila Sciences 技術長 Andy Beam 正在解決的問題。他的團隊正在打造一套系統，讓 AI 不只是讀論文，而是能自己設計實驗、自己執行、自己學習。這是他所謂的「新 Scaling Law」——當 pre-training 的邊際效益越來越低，下一個突破可能來自讓模型生成自己的訓練資料。

## Pre-training 的極限

先退一步，理解現在 AI 發展的瓶頸在哪。

過去幾年 AI 的爆發式成長，靠的是一個經驗觀察：當你增加模型參數、訓練資料、和運算量，模型表現會以可預測的方式提升。這就是 Pre-training Scaling Law。OpenAI、Google、Anthropic 投入數十億美元建造超大規模運算叢集，都是基於這個定律會繼續成立的假設。

但這個定律是 Power Law，指數關係。這意味著每一代要獲得同樣的進步，你需要把算力再擴大一個數量級——從一萬張 GPU 到十萬張，從十萬張到一百萬張。Meta 預計 2025 年底要部署 130 萬張 GPU，但即使如此，進步幅度可能也不會比以前更大。

更麻煩的是，我們其實不知道這個定律為什麼會成立。Beam 用一個比喻來形容這種認知狀態：古埃及人能精確測量太陽的運行軌跡，精確到能把金字塔的東西軸對準春分點。但他們不懂軌道力學，不知道地球繞著太陽轉。我們對 Scaling Law 的理解，就處在類似的階段——精確測量，但缺乏根本性理解。

既然不知道它為什麼成立，我們也無法確定它什麼時候會失效。

## 推理模型只是過渡

業界已經開始找新的 Scaling 方向。最明顯的嘗試是推理模型——OpenAI 的 O 系列、Google 的 Gemini Thinking、Anthropic 的 Claude 3.5 with extended thinking。這些模型不只是預測下一個 token，而是在回答問題時會「想很久」，產生更多中間推理步驟。

推理模型的訓練方式不同於 pre-training。Pre-training 是預測「平均」的回應，推理模型是訓練出「正確」的回應。它需要驗證器來判斷答案對不對，然後用強化學習來優化。這就是所謂的 test-time compute：你把更多算力花在推理階段，而不只是訓練階段。

這確實有效。在數學、程式設計這類有明確正確答案的任務上，推理模型大幅超越純 pre-training 的模型。

但這裡有個關鍵限制：你需要驗證器。對數學題來說，驗證很簡單——答案對就是對，錯就是錯。對程式來說，跑過測試案例就知道。但對科學問題呢？當你問「這個分子能不能治療癌症」，沒有任何現有的驗證器能回答這個問題。

唯一的驗證器是大自然本身。你必須做實驗。

## LLM 是人類知識的索引

讓我們從更根本的層面來看 LLM 的限制。

LLM 本質上是人類知識的絕佳索引。它讀過幾乎所有公開發表的文字，能用一種模糊但有效的方式檢索這些知識。你問它任何問題，它都能從訓練資料中找出最相關的模式，組合成一個聽起來合理的答案。

但這也是它的天花板。它只能輸出訓練資料的某種組合，不能產生訓練資料中沒有的東西。

從因果推論的角度來看，LLM 學到的是觀察性資料。觀察性資料的問題在於：它只能告訴你相關性，不能告訴你因果關係。你可以從資料中看出「A 和 B 同時出現」，但你無法確定是 A 導致 B、B 導致 A、還是有個 C 同時導致了 A 和 B。

要從相關性推到因果，你只有兩條路：一是做很強的假設（這是傳統因果推論方法學在做的事），二是做實驗（隨機分組、控制變因、觀察結果）。LLM 無法自己做實驗，所以它永遠停留在「哪些假說和現有資料相容」這個層次，無法進一步分辨哪個假說是對的。

科學文獻本身也有問題。它不是事實的記錄，而是一場辯論的紀錄。研究者有動機發表對自己假說最有利的版本，有動機淡化不一致的發現。A 發了一篇論文說某個效果存在，B 發一篇說不存在，C 又發一篇說在某些條件下存在。LLM 讀完這些論文，能告訴你這場辯論的現狀，但它沒辦法告訴你誰是對的。

你不可能光靠讀論文就推導出 2050 年的科學長什麼樣子。你需要一步一步做實驗——驗證、推翻、修正、再驗證。

## Lila 的解法：實驗叢集

這就是 Lila Sciences 在做的事：打造一個讓 AI 能自己做實驗的系統。

公司有兩大部門，一半專注於可規模化的實驗平台，另一半專注於 AI 模型。Beam 把實驗平台比喻成「新型電腦」。核心是一套自動化系統：96 孔或 384 孔的實驗板透過磁懸浮，在一條軌道上高速移動。軌道旁邊是各種實驗設備——培養箱、分析儀、定序機——機器手臂負責把板子從軌道拿起來、放進設備、做完實驗再放回去。

那條軌道就像電腦裡的 PCI 匯流排。你可以在上面「插」各種實驗設備，就像在 PCI 匯流排上插顯示卡、網卡一樣。設備之間的資料傳遞，就像電腦內部的資料傳輸。

關鍵是規模。Lila 不是要建幾個這樣的工作站，而是要建整棟大樓的工作站。當你有成千上萬個自動化實驗站，它們全部連上網路、由 AI 控制，你就得到了一個「實驗叢集」——就像你有 GPU 叢集一樣。

把實驗叢集和 GPU 叢集配對，你就得到了一種全新的運算範式。GPU 叢集負責訓練模型、生成假說；實驗叢集負責驗證假說、產生新資料；新資料再回饋給 GPU 叢集，訓練出更好的模型。這是一個閉環，AI 可以自己生成自己需要的訓練資料。

這就是 Beam 說的「新 Scaling Law」。Pre-training 靠的是人類產生的資料——書、論文、網頁。這些資料有上限，總有讀完的一天。但如果 AI 能自己做實驗、自己生成資料，那上限就不存在了。

## 真實世界的困難

當然，這聽起來比做起來容易多了。

真實世界的實驗不像數位世界那樣乾淨。那些實驗板裡面有液體，液體會晃動。晃動會讓板子的位置產生偏移，機器手臂要拿的時候，它可能不在預期的地方。像這樣的邊緣案例有成千上萬個，每個都要解決。

更根本的問題是：現有的所有實驗室自動化設備，都是為人類設計的。實驗台為什麼在那個高度？因為人要站著操作。設備之間為什麼有走道？因為人要走過去。試劑瓶為什麼那個形狀？因為人的手要能握。

當你想打造一個完全由 AI 控制、沒有人類參與的實驗室，這些設計假設全部要重新思考。這是一個沒有人做過的工程問題。

Beam 坦言，這是他現在最大的挑戰。AI 的部分他有信心——大規模訓練很難，但那是已知的困難，有成熟的方法論。實驗平台的部分是未知的困難，因為從來沒有人設計過「為 AI 優化的實驗室」。

## 科學也服從 Bitter Lesson

AI 領域有一個著名的觀察叫 Bitter Lesson（苦澀的教訓），來自強化學習之父 Rich Sutton。它的意思是：長期來看，利用算力的通用方法，總是打敗利用人類知識的特定方法。

下棋就是最好的例子。早期的電腦下棋程式，靠的是讓專家把棋理寫成規則，手動編碼各種策略。這種方法有用，但進步緩慢。後來出現了純粹靠搜尋和學習的方法——給電腦夠多算力，讓它自己跟自己下，它就能發現人類從沒想過的下法。AlphaGo 和 AlphaZero 證明了這條路能走多遠。

Beam 認為科學也服從 Bitter Lesson。問題是，科學的「算力」是什麼？

對訓練模型來說，算力就是 GPU。但對科學來說，算力還包括做實驗的能力。GPU 叢集讓你能跑更多計算，實驗叢集讓你能跑更多實驗。兩者結合，才是科學版的「規模化」。

這就是 Lila 在賭的東西：科學研究的下一個 Scaling Law，來自讓 AI 能夠大規模地與真實世界互動。

## 這會改變什麼

如果 Lila 的願景成真，影響會遠超過科學研究本身。

首先是藥物研發。現在開發一種新藥平均需要 10-15 年、20-30 億美元，成功率不到 10%。大部分時間和金錢都花在試錯——測試各種分子組合，看哪個有效。如果 AI 能自己做實驗、自己學習，這個週期可能大幅縮短。

然後是材料科學。電池、半導體、太陽能板——這些領域的進步都受限於材料的發現速度。如果你有一個能 24 小時自動做實驗的系統，材料的探索空間會爆炸性擴大。

再來是基礎科學。很多科學問題之所以難，是因為需要做太多實驗，人類的時間和精力不夠。有了自動化實驗平台，你可以探索以前沒辦法探索的參數空間。

當然，這些都還是願景。Lila 成立沒幾年，還在早期階段。但願景本身就很有意思：它代表了對 AI 發展方向的一種全新思考——不只是讓模型更大、讀更多資料，而是讓模型能夠與世界互動、從互動中學習。

---

Beam 在訪談中說了一句話：「我們對 Scaling Law 的理解，就像古埃及人對太陽的理解。」

古埃及人不懂軌道力學，但這不妨礙他們建造金字塔。我們不懂 Scaling Law 為什麼有效，但這不妨礙我們繼續利用它。真正的問題是：當現有的路走到盡頭，下一步往哪走？

Lila 的賭注是：讓 AI 自己做實驗。這可能是對的，也可能是錯的。但至少它是一個清晰的方向，而且是一個需要同時解決硬體和軟體問題的方向。

在一個所有人都在比拼 GPU 數量的時代，有人開始思考「GPU 之外還需要什麼」，這本身就是一件有意思的事。
