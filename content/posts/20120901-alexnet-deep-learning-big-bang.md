---
title: "一場比賽如何引爆一場革命：AlexNet 與深度學習的 Big Bang"
date: 2012-09-01T00:00:00+08:00
description: "2012 年，多倫多大學三人團隊用兩張遊戲顯示卡訓練的深度神經網路，在 ImageNet 競賽中以 10.8 個百分點的差距碾壓所有對手。這是深度學習革命的起點，從此改變了整個 AI 領域的方向。"
tags: ["AlexNet", "Geoffrey Hinton", "ImageNet", "深度學習", "AI 經典文獻"]
categories: ["AI 技術前沿"]
source_url: "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"
source_name: "NIPS 2012"
related_companies: ["google", "nvidia"]
related_people: ["geoffrey-hinton"]
image: "/images/posts/20120901-alexnet-deep-learning-big-bang.webp"
draft: false
---

> 本文為「AI 經典文獻回顧」系列第二篇（上），介紹 2012 年由多倫多大學三位研究者發表的 AlexNet 論文，以及它如何在一場圖像辨識競賽中震驚整個 AI 領域。下篇見〈同一篇論文，三條截然不同的路：AlexNet 三位作者的命運分岔〉。

---

## 一場沒人預料到的碾壓

2012 年 10 月，歐洲電腦視覺會議（ECCV）在義大利佛羅倫斯舉行。會場上公佈了那一年 ImageNet 大規模視覺辨識挑戰賽（ILSVRC）的結果，台下的研究者們看到成績單時，集體陷入了沉默。

那年的冠軍是一支來自多倫多大學的小團隊，只有三個人。他們提交的系統在 top-5 錯誤率上跑出了 15.3%，而亞軍是 26.2%。差距不是一兩個百分點的微幅改善，而是整整 10.8 個百分點的碾壓。在 ILSVRC 的歷史上，之前每年的進步幅度大約就是一到兩個百分點，所有人都已經習慣了這種蝸牛爬行的節奏。然後這三個人突然出現，一腳把紀錄踢飛了。

更讓人震驚的不只是數字，而是方法。2010 和 2011 年的冠軍團隊用的都是學界最正統的路線：先用人工設計的特徵提取器（SIFT、HOG 這些手工打磨了十幾年的工具）把圖片轉成數學向量，再丟進支持向量機（SVM）做分類。這套流水線是幾代研究者心血的結晶，每個環節都有深厚的數學理論支撐。但多倫多那三個人完全沒用這些東西。他們訓練了一個深度卷積神經網路，讓它直接從原始像素學習，端到端，沒有任何手工設計的特徵。

在整個 2012 年的參賽隊伍裡，他們是唯一一支使用神經網路的。到了隔年，幾乎所有隊伍都改用了深度學習。

## 一個被冷落了三年的資料集

要理解 AlexNet 為什麼能產生這麼大的衝擊，得先回到三年前的另一個故事。

2009 年，史丹佛大學的李飛飛（Fei-Fei Li）發布了 ImageNet 資料集。這是一個瘋狂的計畫：她要建造一個包含超過 1,400 萬張手動標註圖片、涵蓋兩萬兩千個類別的視覺資料庫。為了完成這件事，她動用了亞馬遜 Mechanical Turk 平台上來自 167 個國家的 49,000 名標註者，篩選了超過 1.6 億張候選圖片，每張圖至少被三個人獨立標註以確保品質。整個過程從 2007 年啟動，花了將近三年。

這個計畫在當時幾乎得不到任何支持。NIH 拒絕了她的經費申請，評語說普林斯頓大學研究這種東西「令人羞恥」。2009 年她在 CVPR 頂級會議上發表 ImageNet 時，大會只給了她一個海報展示的位置，不是口頭報告。團隊只好印了 ImageNet 品牌的原子筆到會場發，靠小禮物吸引路過的人多看一眼。

但李飛飛看到了一件大多數人沒看到的事。我們在這個系列的第一篇介紹過，Google 的三位研究者在同一年發表了〈數據的不合理有效性〉，主張海量數據比精巧演算法更有效。李飛飛做的事情本質上是同一個哲學的視覺版本：她相信電腦視覺的瓶頸不是演算法不夠聰明，而是數據不夠豐富。如果你能給機器看一千四百萬張涵蓋兩萬多個類別的真實世界照片，某些事情就會發生。

2010 年，她以 ImageNet 為基礎創辦了 ILSVRC 競賽，用其中 120 萬張圖片、1,000 個類別作為挑戰題目。頭兩年，傳統方法穩坐王位。然後 AlexNet 來了。

## 用遊戲顯示卡訓練的神經網路

AlexNet 這個名字來自它的第一作者 Alex Krizhevsky。這是一個八層深的卷積神經網路，有五個卷積層和三個全連接層，總共約六千萬個參數。以今天的標準看，這個模型小得可笑，GPT-4 的參數量是它的數萬倍。但在 2012 年，六千萬個參數已經大到沒有任何 CPU 能在合理時間內訓練完。

這正是 AlexNet 最關鍵的工程突破：它用 GPU 來訓練。具體來說，是兩張 NVIDIA GeForce GTX 580 顯示卡，每張只有 3GB 的記憶體，發售價大約 500 美元。這是遊戲玩家買來打電動的硬體，不是什麼超級電腦。Krizhevsky 用 CUDA 從頭寫了 GPU 上的卷積運算核心，把模型拆成兩半分別塞進兩張顯示卡，只在特定的層讓兩張卡交換資料。整個訓練跑了大約五到六天，九十個 epoch。如果用當時的 CPU 做同樣的事，需要數週甚至數月。

有一個廣為流傳的細節：AlexNet 是在 Krizhevsky 父母家的臥室裡訓練的。兩張遊戲顯示卡插在一台桌上型電腦裡，就這樣跑出了改變歷史的結果。

## 不只是 GPU：四個讓深度學習「可行」的技術突破

光有 GPU 是不夠的。AlexNet 能成功，還因為它同時引入了幾個當時看起來不起眼、後來卻成為標準配備的技術。

第一個是 ReLU 啟動函數。在 AlexNet 之前，神經網路用的是 sigmoid 或 tanh 這類 S 形函數，它們在輸入值很大或很小的時候會趨近飽和，導致梯度幾乎消失，讓深層網路變得極難訓練。ReLU 的公式只有一行：f(x) = max(0, x)，簡單到像在開玩笑。但正是這個簡單的設計解決了梯度消失問題。論文中的實驗數據顯示，使用 ReLU 的網路達到同樣訓練準確度的速度，比使用 tanh 的版本快了六倍。到今天，ReLU 和它的變體仍然是最常用的啟動函數。

第二個是 Dropout。這個技巧是在訓練時隨機「關掉」50% 的神經元，迫使網路不能依賴任何單一神經元，必須學到更穩健的特徵。你可以把它想像成一個足球教練故意在每場練習賽隨機抽掉一半球員，讓剩下的人必須學會在任何人缺席時都能踢出像樣的配合。這個概念出自傑佛瑞．辛頓（Geoffrey Hinton）實驗室，AlexNet 是第一個大規模應用它的系統。

第三個是資料增強（data augmentation）。ImageNet 的 120 萬張圖片聽起來很多，但對六千萬個參數的網路來說，仍然存在過擬合的風險。AlexNet 的做法是在訓練時動態地對每張圖片做隨機變換：水平翻轉、隨機裁切、色彩抖動。一張 256×256 的原始圖片可以產生超過兩千個不同的 224×224 訓練樣本。這些變換在 CPU 上即時進行，不佔用 GPU 算力也不需要額外儲存空間。

第四個是重疊式最大池化（overlapping max pooling），用 3×3 的窗口搭配步幅 2 來降低特徵圖的維度。這個技巧貢獻了大約 0.3 到 0.4 個百分點的錯誤率改善，單獨看不算大，但在競賽中每一點都重要。

這四個技巧沒有任何一個是石破天驚的理論創新。ReLU 的概念早就存在，Dropout 是直覺的正則化方法，資料增強更是老招。AlexNet 的真正厲害之處在於把它們全部組合在一起，搭配 GPU 的算力和 ImageNet 的數據量，讓整個系統產生了遠超各部分之和的效果。

## 為什麼不是 1998 年：LeNet 的未竟之業

這裡有一個繞不開的問題：卷積神經網路（CNN）的概念並不是 2012 年才出現的。早在 1998 年，楊立昆（Yann LeCun）就在 AT&T 的貝爾實驗室開發了 LeNet-5，一個用 CNN 辨識手寫數字的系統，在銀行支票辨識上取得了商業成功。卷積、池化、反向傳播，這些核心概念在 1998 年就全部到位了。那為什麼要等十四年，才由另一群人引爆革命？

答案藏在規模裡。LeNet-5 只有大約六萬個參數，訓練資料是 MNIST 的六萬張 32×32 灰度手寫數字圖片。AlexNet 有六千萬個參數（一千倍），訓練資料是 120 萬張 224×224 的全彩照片（二十倍）。在 LeNet 的規模上，CNN 相對於 SVM 的優勢並不明顯，傳統方法「夠用了」。但當問題規模放大到 ImageNet 的等級，手工設計特徵的天花板就暴露出來了，而 CNN 的學習能力才真正有了施展的空間。

楊立昆自己曾半開玩笑地說：「AlexNet 只是用 GPU 把我十五年前的想法跑大了而已。」這句話既是自嘲也是洞見。科學突破往往不只需要「對的想法」，還需要「對的時機」。1998 年沒有 GPU 可用、沒有 ImageNet 那種規模的資料集、沒有 ReLU 和 Dropout 這些讓深層網路可訓練的工程技巧。CNN 的種子在 1998 年就種下了，但要等到 2012 年，數據、算力和工程能力三者同時到位，它才真正開花。

這個教訓跟我們在上一篇看到的一脈相承。Google 的三位研究者在 2009 年說「數據比演算法重要」，但 AlexNet 補上了一個重要的修正：光有數據不夠，你還需要一個對的架構（CNN 的歸納偏置對影像任務至關重要）和足夠的算力（GPU）來消化這些數據。數據、架構、算力，三者缺一不可。

## 一場地震的餘波

AlexNet 的衝擊不只停留在 ImageNet 競賽。它在學界觸發了一場方法論的全面轉向。2013 年的 ILSVRC，幾乎所有參賽隊伍都改用深度學習。電腦視覺頂級會議中深度學習論文的佔比從不到 5% 飆升到超過 50%。各大學開始重新開設或全面翻新「神經網路」課程，教授們忙著把十年前被扔進倉庫的教材翻出來。

架構演進的速度令人眩暈。2013 年的 ZFNet（AlexNet 的改良版）把錯誤率壓到 11.7%，2014 年 GoogLeNet 用 Inception 模組降到 6.7%，到了 2015 年，微軟研究院的 ResNet 用 152 層的殘差網路跑出了 3.57%，已經超越了人類在同一任務上的表現（約 5%）。從 AlexNet 到超越人類，只用了三年。

但影響最深遠的，是 AlexNet 打開的不只是電腦視覺這一扇門。它證明了「端到端深度學習」這個範式的威力：不要手工設計特徵，讓神經網路自己從原始數據中學習。這個思路從影像辨識迅速蔓延到其他領域。2013 年 word2vec 用神經網路學習詞向量，2014 年 AlexNet 的共同作者 Ilya Sutskever 發表了 Seq2Seq 模型用於機器翻譯，2015 年注意力機制被引入自然語言處理。這條線最終通向 2017 年的 Transformer，通向 GPT，通向我們今天每天使用的大型語言模型。

NVIDIA 的執行長黃仁勳（Jensen Huang）把 AlexNet 稱為「AI 的大爆炸」（Big Bang of AI）。這不是誇張。在 AlexNet 之前，深度學習是一個被主流學界邊緣化了二十年的小眾方法。在 AlexNet 之後，它成為整個 AI 產業的基礎架構。而這一切始於兩張五百美元的遊戲顯示卡和一間臥室。

下一篇，我們來看寫出這篇論文的三個人後來怎麼了。一個幾乎從地球上消失，一個成為 AI 史上最戲劇性的出走者，一個拿了諾貝爾獎。
