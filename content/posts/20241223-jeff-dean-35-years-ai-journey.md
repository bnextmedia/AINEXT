---
title: "jeff dean 35 years ai journey"
date: 2025-12-23T12:24:24+08:00
description: "> 本文整理自 Stanford AI Club 邀請 Jeff Dean 的演講。 > 🎬 觀看連結：[YouTube](https://www.youtube.com/watch?v=AnTw_t21ayE) --- ## 1990 年，一個大學生以為 32 核就能改變世界 1990 年，J"
tags: ["AI"]
categories: ["AI"]
draft: false
---


> 本文整理自 Stanford AI Club 邀請 Jeff Dean 的演講。
> 🎬 觀看連結：[YouTube](https://www.youtube.com/watch?v=AnTw_t21ayE)

---

## 1990 年，一個大學生以為 32 核就能改變世界

1990 年，Jeff Dean 在大學畢業前寫了一篇關於神經網路的論文。當時他剛接觸到這個領域，立刻被迷住了。「這是一個很棒的抽象概念，」他回憶，「我們可以用它來建構模式辨識系統，解決各種問題。」於是他決定做一個野心勃勃的畢業專題：用系上那台 32 核處理器的電腦來並行訓練神經網路。

他實作了兩種現在我們稱之為「資料平行」(data parallelism) 和「模型平行」(model parallelism) 的訓練方式，研究當處理器數量增加時，訓練速度如何提升。結果呢？「我完全錯了，」Jeff Dean 笑著說，「要訓練出真正好用的神經網路，需要的不是 32 倍的運算力，而是一百萬倍。」

這個「錯誤」說明了一件事：神經網路的潛力比當時任何人想像的都大，但實現這個潛力需要的運算規模，也遠超過 1990 年代的技術能提供的。Jeff Dean 畢業後去做了其他事，但他一直惦記著這個想法。

---

## 2012 年，Google 廚房裡的一場對話

二十多年後的某一天，Jeff Dean 在 Google 的員工休息區 (micro-kitchen) 碰到了 Andrew Ng。「嘿 Andrew，你怎麼在這？」Andrew 解釋說他剛開始每週來 Google 一天，還在摸索要做什麼。然後他提到：「我在史丹佛的學生開始用神經網路在語音辨識上取得不錯的成果。」

Jeff Dean 的眼睛亮了起來。「我們應該訓練超大型的神經網路。」

這場廚房對話成為 Google Brain 的起點。當時 Google 的資料中心裡沒有 GPU，只有大量的多核心 CPU。於是他們建構了一個叫做 DisBelief 的軟體系統，讓神經網路的訓練可以分散到數百台機器上執行。

這個系統的運作方式，用 Jeff Dean 的話說，「在數學上完全是錯的」。他們讓兩百個模型副本同時訓練，每個副本會下載當前的參數、用一批資料計算梯度、然後把更新送回參數伺服器。問題是，所有副本都在非同步地做這件事，當你的更新送回去時，參數可能已經被其他一百九十九個副本改過了。

「這讓很多人很緊張，因為這不是你『應該』做的事，」Jeff Dean 回憶，「但它居然有效，所以我們就繼續用了。」

這個「數學上錯誤但實際上有效」的系統，讓 Google 在 2012 年訓練出比當時任何人都大 50 到 100 倍的神經網路。他們拿一千萬張 YouTube 影片的隨機截圖來訓練，完全不給任何標籤，只是讓模型學習如何重建原始圖像。訓練完成後，他們發現模型頂層的某些神經元會對特定概念產生強烈反應——其中一個神經元的最強刺激是貓臉，即使它從來沒被「教過」什麼是貓。這就是著名的「貓論文」(cat paper) 的由來。

---

## 當語音辨識變好，世界需要新的晶片

Google Brain 團隊很快就把神經網路應用到語音辨識上，訓練出一個錯誤率遠低於當時 Google 產品的模型。Jeff Dean 做了一個簡單的估算：如果語音辨識變得更好，更多人會想用。假設一億人每天對著手機講三分鐘，我們需要多少運算資源？

答案讓他嚇了一跳：如果用現有的 CPU 來跑這個新模型，Google 需要把資料中心的電腦數量翻倍。只是為了一個功能。

「我們需要專用硬體，」Jeff Dean 意識到。神經網路有一些很好的特性可以被利用：它們對低精度運算很容忍，不需要 32 位元浮點數；而且當時所有的神經網路本質上都是密集線性代數運算——矩陣乘法、向量內積。如果能設計一個晶片專門做低精度的線性代數運算，效率會比通用 CPU 高很多。

於是 Google 開始設計 TPU（Tensor Processing Unit）。第一代 TPU 在 2015 年部署到資料中心，專門用於推論（inference）。當他們把它跟同期的 CPU 和 GPU 比較時，發現它快了 15 到 30 倍，能源效率則提升了 30 到 80 倍。這篇論文後來成為電腦架構頂級會議 ISCA 五十年歷史上被引用最多的論文。

但推論只是一半的問題。訓練需要更大規模的運算，於是 Google 開始設計「機器學習超級電腦」——把數千顆 TPU 用高速網路連接起來，形成一個龐大的運算叢集。從 2017 年的 TPU v2（256 顆晶片組成一個 Pod）到最新的 Ironwood（9,216 顆晶片組成一個 Pod），每個 Pod 的峰值運算能力提升了 3,600 倍，能源效率也提升了 30 倍。

---

## Transformer：「Attention 就是你所需要的一切」

2017 年，Jeff Dean 的幾位同事發表了一篇論文，標題是《Attention Is All You Need》。這篇論文提出了 Transformer 架構，徹底改變了自然語言處理的面貌。

在 Transformer 之前，處理序列資料的主流方法是 LSTM（長短期記憶網路）。LSTM 的運作方式是一個字一個字地處理，每處理一個字就更新一個內部狀態向量。問題在於，所有的歷史資訊都必須壓縮進這個固定大小的向量裡。當序列很長的時候，早期的資訊往往會被「遺忘」。

Transformer 的核心觀察是：與其強迫模型把所有資訊塞進一個向量，不如讓模型保留所有過去的狀態，然後在需要的時候「注意」(attend to) 相關的部分。這個「注意力機制」讓模型可以直接存取任何位置的資訊，而不用擔心中間的資訊被覆蓋掉。

論文的數據很驚人：Transformer 可以用比 LSTM 少 10 倍的參數、少 10 到 100 倍的運算量，達到相同的語言模型品質。換句話說，同樣的預算可以訓練一個大 10 倍的模型，或者用十分之一的成本達到同樣的效果。

Transformer 很快就被應用到各個領域。2020 年，另一組 Google 研究員把它應用到電腦視覺，證明 Vision Transformer (ViT) 可以用 4 到 20 倍更少的運算量，達到當時最好的圖像辨識準確度。

---

## 稀疏模型：不是每個神經元都需要參與

傳統神經網路有一個「浪費」的特性：不管輸入是什麼，整個模型的每一個參數都會參與運算。Jeff Dean 覺得這很不合理。「如果模型裡有專門處理不同事情的部分，為什麼每次都要全部啟動？」

這個觀察催生了稀疏模型（Sparse Models）的研究方向，其中最著名的是「專家混合模型」(Mixture of Experts, MoE)。在 MoE 架構中，模型包含很多「專家」子網路，每次推論時只有一小部分專家會被啟動——通常是 1% 到 5%。一個路由機制會根據輸入決定該啟動哪些專家。

這是一個很划算的交易：模型可以有巨大的參數量（表達能力更強），但每次推論的實際運算量只有傳統模型的幾分之一。Jeff Dean 團隊的實驗顯示，稀疏模型用大約八分之一的訓練成本就能達到相同的準確度。

「現在你聽到的大多數模型，包括 Gemini，都是稀疏模型，」Jeff Dean 指出。這是一個被低估的技術突破——它讓我們可以在不成比例增加成本的情況下，大幅擴展模型的規模。

---

## 讓模型「展示解題過程」

2022 年，Jeff Dean 的同事發現了一個簡單但威力驚人的技巧：如果你想讓模型解數學題，不要只給它「問題→答案」的範例，而是給它「問題→解題過程→答案」的範例。

這就是「思維鏈」(Chain-of-Thought) 提示法。當你在提示中示範如何一步步解題，模型就會學著在回答時也展示它的推理過程。這不只是讓答案更容易理解——模型產生每一個 token 時都會進行一輪運算，所以當它「展示解題過程」時，實際上是在用更多的運算來得出答案。

差別很明顯。在 GSM8K（一個小學程度的數學測試集）上，使用思維鏈提示讓準確率從接近隨機猜測跳到約 15%。

「現在回頭看，」Jeff Dean 說，「2022 年我們還在慶祝模型能答對 15% 的八年級數學題——那種『約翰有五個玩具，聖誕節又收到兩個』的題目。」

---

## 從小學數學到 IMO 金牌

三年後，一切都不一樣了。

Google 用 Gemini 2.5 Pro 的一個變體參加了今年的國際數學奧林匹克 (IMO)。IMO 是全球最頂尖的中學生數學競賽，兩天六題，每題都是專業數學家等級的難度。

結果：六題答對五題，獲得金牌。

Jeff Dean 在演講中展示了其中一題的解答。題目本身需要對數論有深入理解，而模型產生的證明不只是正確的，評審還稱讚它的優雅。這已經不是「能算數學」的問題了——這是能夠進行嚴謹數學推理的能力。

這種進步來自多個技術的累積。思維鏈讓模型學會拆解問題；蒸餾 (Distillation) 讓小模型能夠學習大模型的「軟標籤」（一個分布，而不只是一個正確答案），大幅提升學習效率；而「在可驗證領域做強化學習」則讓模型可以不斷嘗試、從正確與錯誤的回饋中改進。數學和程式設計特別適合這種方法，因為你可以用定理證明器或編譯器來自動驗證答案是否正確。

---

## 三十五年，從「完全錯誤」到「居然有效」

從 1990 年的 32 核電腦到 2025 年的萬顆 TPU 叢集；從「貓論文」到 IMO 金牌；從 DisBelief 的「數學上錯誤」到 Transformer 的「注意力就夠了」。Jeff Dean 的職涯幾乎就是深度學習發展史的縮影。

他在演講結尾提到，AI 會深刻影響醫療、教育、科學研究和媒體創作——當然也有錯誤資訊等風險。「但如果做得好，我們的 AI 輔助未來會是光明的。」

或許最值得玩味的是那個 1990 年的「錯誤」。Jeff Dean 以為需要 32 倍運算力就能讓神經網路變得實用，結果需要的是一百萬倍。但這個「錯誤」的背後是一個正確的直覺：這個抽象概念值得追求。三十五年後，這個直覺被證明是對的——只是規模比任何人想像的都要大得多。
