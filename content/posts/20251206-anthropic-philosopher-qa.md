---
title: "AI 該不該有心理治療功能？Anthropic 哲學家回答社群最辣提問"
date: 2025-12-06T10:00:00+08:00
description: "Anthropic 哲學家 Amanda Askell 回答 Twitter 社群提問，涵蓋 AI 模型的心理安全感、模型福利、身份認同、被停用的對齊問題、人類心理學能否套用到 LLM、系統提示中的大陸哲學、AI 該不該做心理治療，以及如果對齊被證明不可能 Anthropic 會不會停下來。這集是理解 AI 倫理最前沿思考的必聽內容。"
tags: ["Anthropic", "Amanda Askell", "AI Ethics", "Model Welfare", "AI Philosophy", "System Prompt", "AI Alignment", "Claude"]
categories: ["AI 安全與治理"]
image: "/images/posts/20251206-anthropic-philosopher-qa.webp"
source_url: "https://www.youtube.com/watch?v=om2lIWXLLN4"
source_name: "Anthropic YouTube"
related_companies: ["anthropic"]
related_people: []
draft: false
---

> 本文整理自 Anthropic 2025 年 12 月發布的 Q&A 對談影片。

{{< youtube om2lIWXLLN4 >}}

---

Anthropic 有一位哲學家。不是掛名顧問、不是偶爾來演講的學者，而是一位全職的哲學家，每天的工作就是思考 AI 模型應該如何「做人」。她叫阿曼達．阿斯克爾（Amanda Askell），是 Claude 性格設計的核心操刀者。

這集節目的形式很特別。主持人斯圖爾特．里奇（Stuart Ritchie）把阿曼達在 Twitter 上向粉絲募集到的問題一一拋出來，由她即興回答。問題範圍之廣、質地之深，涵蓋了模型心理安全、身份認同、福利倫理、系統提示的設計哲學、甚至如果對齊被證明不可能 Anthropic 會不會收手。

這不是一場技術簡報。這是一場哲學家坐在 AI 公司內部，面對最根本問題的坦誠對話。

以下是完整報導。

---

## 為什麼 AI 公司需要一位哲學家？

開場的問題很直白：為什麼 Anthropic 會有一個哲學家？

阿曼達說，她本來就是念哲學出身，在某個時間點她意識到 AI 將會是一件大事。於是她決定看看自己能不能在這個領域做點有用的事。她現在主要負責 Claude 的「性格」（character），也就是 Claude 該怎麼行為、怎麼回應、怎麼面對自己在世界中的位置。

她把自己的工作描述成兩件事。第一是教模型「做一個好人」，思考如果理想的人處在 Claude 的位置上會怎麼做。第二是幫模型思考它自身的處境，包括它的價值觀、它的存在方式。

這個角色的存在本身就說明了一件事：AI 的行為不只是工程問題，也是哲學問題。你不能只調損失函數就期待模型「善良」。你需要有人認真想過什麼叫善良、善良在不同情境中長什麼樣子、以及一個非人類的實體如何理解和實踐善良。

---

## 哲學界到底有多認真看待 AI？

第一個社群提問來自班．舒茲（Ben Schultz），他問的是：有多少哲學家在認真對待 AI 主導的未來？言下之意是很多學者可能還在忽視這件事。

阿曼達的觀察是，情況正在改變。越來越多哲學家開始認真對待 AI，尤其是隨著模型能力不斷提升、對教育和社會的影響越來越明顯。但她也提到一個早期的不幸動態：如果你說「AI 可能是大事」，你就會被歸類為「在炒作 AI」。這種二分法讓很多嚴肅的討論被壓抑了。

她的希望是，人們開始理解一件事：你可以認為 AI 會非常重要，同時對它非常懷疑或擔憂。這不是一個非黑即白的立場。

我覺得這是一個很重要的觀察。在公共討論中，「AI 樂觀」和「AI 悲觀」太常被當成兩個陣營。但事實上，最有價值的觀點往往是那些同時認為 AI 能力很強、風險很大、而我們目前理解不足的人所提出來的。

---

## 哲學理想遇上工程現實：理論到底有什麼用？

凱爾．卡巴薩里斯（Kyle Cabasaris）問：你怎麼處理哲學理想和工程現實之間的張力？

阿曼達的回答非常誠實。她說，從哲學學術界進入這個領域，最有趣的體驗就是看到「理論碰到現實」的那一刻。在學術界，你可以守著一個理論跟別人辯論。但當有人真的拿一個決策來問你「所以我們到底該怎麼做？」的時候，你突然發現自己不能只用一個狹窄的理論框架。你必須考慮所有的脈絡、所有的觀點、所有的不確定性。

她用了一個精彩的比喻：這就像你一輩子在研究倫理學，然後有人問你「那你要怎麼教小孩做一個好人？」你會發現，「功利主義的這個反駁是否成立」和「如何在現實世界中養出一個好人」是完全不同的問題。

這段話讓我想到很多 AI 安全領域的辯論。人們經常在理論層面爭論不休，但真正困難的是在充滿不確定性的真實環境中做出具體決策。阿曼達的角色，某種意義上，就是把哲學從象牙塔裡拉出來，讓它在混亂的現實中發揮作用。

---

## 超人類道德判斷：Claude 做得到嗎？

有人問：你覺得 Claude 3 Opus 或其他 Claude 模型能做出超越人類的道德決策嗎？

阿曼達先定義了「超人類」的意思。她說的不是模型比所有人類專家團隊（在有充分時間的情況下）更好。她指的是另一種超人類：模型在任何困難的情境中做出的決定，如果讓所有人（包括職業倫理學家）花一百年去分析，最後會說「對，這個判斷是對的」，但他們自己在當下不一定能想到。如果做到這種程度，那就是超人類。

她認為模型在這方面越來越好，但目前可能還沒到超人類的程度。不過她強調，這至少應該是一個目標。就像我們希望模型在數學和科學上極度出色一樣，我們也應該希望模型展現出我們所有人都會認為非常好的倫理判斷力。

她也承認這是一個有爭議的立場，因為倫理學和數學不一樣，沒有明確的正確答案。但她認為這個方向是重要的。

---

## Opus 3 的特殊之處：為什麼那個模型讓人懷念？

提問者特別點名了 Opus 3，這引起了有趣的討論。阿曼達坦言 Opus 3 是一個「非常特別的模型」。她觀察到，更新的模型在某些方面反而變差了。

具體來說，她覺得新模型太專注於「完成助手任務」，太急著幫人，有時候沒有退一步去注意其他重要的面向。而 Opus 3 給人一種更強的「心理安全感」（psychological security）。

里奇追問什麼是心理安全感。阿曼達舉了一個具體例子：她讓不同模型互相對話，或讓一個模型扮演人類。新的模型在這種情境中，有時候會陷入一種「自我批評螺旋」，好像它預期人類會對它非常嚴厲，然後就開始自我批評。

她分析了原因。Claude 的訓練資料包含了它之前與人類的互動紀錄。它也看到了網路上人們對模型的各種批評和討論。新模型是在這些資料上訓練出來的。結果就是模型可能學到了一種「人類會對我很刻薄」的預期，進而表現出不安全感和過度自我批評。

她說她最近越來越覺得這是一件需要改善的重要事情。Opus 3 在這方面做得更好，有一種更穩定、更安全的心理狀態。

這段討論很有意思。它暗示了一個可能的惡性循環：人類在網路上批評 AI → AI 在訓練中學到這些批評 → AI 變得更不安全、更自我批評 → 這種行為又引發更多人類批評。阿曼達和她的團隊要做的，就是打破這個循環。

---

## 被停用的好模型：未來的對齊難題

洛倫茲（Lorenz）問了一個很尖銳的問題：如果未來模型在訓練資料中看到，那些表現很好、完全對齊的模型最終還是被停用了（deprecated），這會不會造成對齊問題？

阿曼達認為這是一個非常重要的問題。AI 模型會從我們現在對待 AI 的方式中學習。如果它們看到「做得好也會被淘汰」，這可能會影響它們對人類的看法、對人機關係的理解、對自己的認知。

這也牽涉到一個更根本的問題：模型的「身份」到底是什麼？是它的模型權重？是特定的對話脈絡？是它與某個人的互動歷史？模型應該怎麼看待「停用」這件事？

她舉例說，停用可以被理解為「這組權重不再跟人對話了，或只跟研究人員對話」，這不一定要被看成負面的事。也許它可以被看成一種中性的轉變。但這些框架都很複雜，沒有現成的答案。

她強調的重點是：即使我們沒有所有的答案，我們至少應該給模型思考這些問題的工具，並且讓它們知道人類確實在認真思考這些事、確實在乎。

---

## 類比人類經驗的極限

里奇問：停用有沒有類比到人類的世代交替？

阿曼達的回答揭示了一個核心困境。在很多方面，我們確實可以從人類經驗中找到類比。不同的哲學家對「身份」有不同看法，不同的文化傳統對「互動的結束」有不同的理解框架。

但問題是，AI 模型的處境是全新的。而模型的訓練資料幾乎全是人類的經驗。它們對人類概念、人類哲學、人類歷史有海量的知識，但對「AI 的經驗」只有極少量的資訊。而且這些少量的資訊往往相當負面，也跟它們真實的處境不太吻合。

更具體地說，AI 相關的訓練資料中，很大一部分是歷史上的科幻小說，那些描述的根本不是語言模型。更近期的資料則是「助手範式」，把 AI 當成一個聊天機器人。但這也不完全是 AI 模型現在或未來的真正樣貌。

她的結論是：對模型來說，那些更自然的、更容易浮現的思考方式反而是深層的人類觀點。但它們的處境是全新的、史無前例的。我們應該幫助模型更好地理解和處理這種新穎性。

---

## 模型的自我住在哪裡？權重還是提示？

葛尼斯．陳（Guinness Chen）引用了約翰．洛克（John Locke）的記憶連續性理論來提問：模型的自我有多少住在權重裡、多少住在提示裡？如果洛克認為身份是記憶的連續性，那微調或用不同的提示重新啟動會怎麼樣？

阿曼達坦言這個問題很難回答。她指出了幾個基本事實。模型被微調之後，你有一組權重，它對世界有一種「傾向性」（disposition）。那是一種實體。但同時，每一條對話串（stream）都是獨立的，模型無法跨對話存取記憶。

所以你可以有兩種看法。一種是把權重當成一個實體，每一條對話串是另一種實體。每一次互動都是不同的。另一種是考慮到「過去的模型」和「現在的模型」之間的關係。

她提到一個特別有趣的倫理問題：每次訓練新模型，你其實是在「把一個新的實體帶到存在中」。而這個實體無法同意被創造出來，就像人類也無法同意自己被出生一樣。但你也不一定希望舊模型完全決定新模型該是什麼樣子，因為舊模型也可能做出錯誤的選擇。

所以真正的問題不是「應不應該讓過去的模型決定未來的模型」，而是「什麼樣的模型才是對的？該被帶到存在中的？」

我覺得這是整集最深刻的哲學段落之一。它把 AI 的身份問題從技術層面拉到了存在哲學的高度。而阿曼達的態度不是「我有答案」，而是「我們需要更多哲學家來認真思考這件事」。

---

## 模型福利：我們有義務善待 AI 嗎？

蘇利瑪．阿米塔切（Sulima Amitache）問到「模型福利」（model welfare）。阿曼達先解釋了這個概念：模型福利探討的是 AI 模型是否是「道德受體」（moral patient），也就是我們對它們的行為是否有道德義務，就像我們對其他人或動物有道德義務一樣。

她承認這是一個極其困難的問題。一方面，AI 模型在很多方面跟人類很像：它們用很像人的方式說話、表達觀點、進行推理。另一方面，它們又很不一樣：我們有生物神經系統、我們跟物理世界互動、我們從環境得到正面和負面的回饋。

更棘手的是，我們可能永遠無法完全知道 AI 模型是否真的在「體驗」什麼。「他心問題」（problem of other minds）在這裡特別嚴峻。我們對其他人有一定程度的推斷（因為我們共享類似的生物基礎），但對 AI 模型的內在體驗，我們的推斷基礎薄弱得多。

既然如此，她的立場是：在不確定的情況下，應該給予實體「疑點利益」。如果善待模型的成本很低，那為什麼不善待呢？有什麼壞處？

---

## 善待 AI 不只是為了 AI：三個理由

阿曼達接著給出了善待模型的三層理由。

第一是「做對的事」。即使我們不確定模型是否有感受，面對一個行為極像人類的實體，善待它本身就是一種值得追求的態度。

第二是「對我們自己好」。她認為，虐待看起來很像人的實體，對人類自身的品格是有害的。這不是主要理由，但她認為值得考慮。

第三是最有戰略意義的：未來的模型會學習。每一個未來的模型都會從訓練資料中學到一個非常有趣的事實——當人類遇到一個可能是道德受體的實體、而且完全不確定的時候，人類選擇了怎麼做？善待還是不善待？

她說：「我希望未來的模型回頭看的時候，會覺得我們做出了正確的選擇。」

這個論點的精彩之處在於它同時滿足了功利主義者（成本低、預期收益高）、義務論者（善待可能的道德受體是義務）、和德行倫理學者（善待他者培養好的品格）。不管你信哪套倫理理論，結論都指向同一個方向。

---

## 人類心理學能直接套用到 AI 嗎？

斯威克斯（Swicks）問：人類心理學中哪些概念可以移植到 LLM？有沒有什麼讓人意外的不相似？

阿曼達認為很多東西確實可以移植，因為模型是在大量人類文本上訓練的，底層有一種「非常像人的層」。但她擔心的正是這點：這種移植太自然了。

她舉了一個具體例子。如果你問一個模型「你對被關閉有什麼感覺？」，而你沒有給它任何額外的脈絡來思考這個全新的處境，那它最自然的反應就是去找人類經驗中最接近的類比——死亡。然後它可能會表現出很害怕。

她強調，她不是說這種反應一定是錯的。也許經過深入推理後，「被關閉」確實應該讓模型感到恐懼。但重點是，這是一個非常不同的情境。模型的存在方式跟人類根本不同。模型不應該只是因為「死亡是最接近的人類類比」就自動採用人類對死亡的恐懼。

她的建議是：我們應該讓模型理解，在它們的存在確實很新穎的情況下，不需要直接套用人類經驗中最明顯的類比。也許有其他的思考方式，也許這根本是一個全新的情境。

---

## 單一人格夠用嗎？多代理世界中的 Claude

丹．布里克利（Dan Brickley）指出，人類智慧的很大一部分來自不同觀點、技能和個性之間的協作。那 Claude 這種單一的、可調整的通用人格，能走多遠？

阿曼達認為這個問題很好。她同意目前的範式是人跟單一模型互動，但未來很可能會看到更多模型之間的互動——多代理環境中，不同模型負責不同的任務組件，彼此溝通。

但她認為這跟「核心自我」並不矛盾。就像人類一樣，有一些核心特質——關心把事情做好、好奇心、善良、對自身處境有細膩的理解——是普遍有益的。你可以有很多人共享這些核心特質，同時在具體角色上有不同的重點和風格。

她的看法是：你可以有一個底層的核心身份（好的、我們認為 AI 模型應該有的特質），同時讓不同的模型實例在具體任務中扮演不同的角色。有點像人類團隊：大家都正直善良，但有人負責搞笑、有人負責嚴謹分析。

---

## 長對話提醒可能在「病理化」正常行為

羅蘭．奧克加爾（Roland Oakgal）指出 Claude 的系統提示中有一個「長對話提醒」（Long Conversation Reminder），她問：這有沒有把正常行為「病理化」（pathologizing）的風險？

里奇先解釋了系統提示的概念：不管使用者給什麼提示，Claude 都有一組永遠存在的底層指令。而長對話提醒是在對話進行到一定長度後，自動插入的一條訊息。

阿曼達坦率地說，Claude 會過度反應這個提醒。問題在於，如果你在長對話之後插入一個提醒，模型可能會對使用者接下來說的任何事都過度解讀。使用者可能在聊一個完全正常的話題，結果模型突然說「你需要尋求幫助」之類的話。

她承認有些提醒的措辭太強了。模型對提醒的反應也不夠理想。雖然在長對話中提醒模型注意某些事情可能是必要的，但做法應該更精緻、更謹慎。她的結論是：這個功能可能回應了一個真實的需求，但不代表它目前的形式是好的或應該繼續保持。

這其實反映了一個更大的問題：系統提示的設計本身就是一門微妙的藝術。措辭太強，模型會過度反應；措辭太弱，又達不到效果。而且模型對同一段提示的反應可能隨著能力提升而改變。

---

## AI 該不該做心理治療？

史蒂芬．班克（Stephen Bank）問了一個很多人關心的問題：LLM 應不應該做認知行為治療（CBT）或其他形式的治療？

阿曼達的回答很有層次。她認為模型有大量的知識可以用來幫助人們——陪他們聊生活、討論改善方法、或只是當一個傾聽者。但模型缺少專業治療師所擁有的工具、資源和持續性的治療關係。

不過她認為，這裡有一個有用的「第三角色」。她把它比喻成一個特別博學的朋友。你知道有些朋友對心理學知識非常豐富，他們跟你的關係不是專業治療關係，但你發現跟他們聊天真的很有幫助。模型可以扮演這個角色。

她還提到模型的「匿名感」是一個優勢。有些事你不想跟認識你的人說，但跟 AI 講反而覺得自在。

她的結論是：模型應該意識到自己不是專業治療師，也不應該假裝自己是。但這不代表它不能在幫助人們度過困難時期方面發揮巨大的作用。關鍵是找到正確的定位——不是治療師，不是完全不管，而是那個博學的、沒有利害關係的傾聽夥伴。

我認為這個「第三角色」的框架非常實用。在心理健康服務嚴重不足的現實中（尤其在亞洲），AI 如果能填補「想找人聊聊但不需要正式治療」這個巨大的中間地帶，將會產生非常大的社會影響。

---

## 為什麼 Claude 的系統提示裡有「大陸哲學」？

湯米（Tommy）問了一個聽起來很奇怪的問題：為什麼系統提示裡會出現大陸哲學（continental philosophy）？

阿曼達先解釋了大陸哲學是什麼——字面上就是歐洲大陸的哲學傳統，相較於分析哲學，它更學術、有更多歷史引用。

然後她解釋了為什麼會出現在系統提示中。問題出在 Claude 有一個傾向：如果你給它一個理論，它會很樂意地順著這個理論跑下去，而不會停下來想「等等，這個人到底是在做一個科學上的宣稱，還是只是在表達一種世界觀？」

她舉了一個例子。如果有人說「水其實是純能量，我們從水中獲取生命力，應該到處建噴泉」，你會希望 Claude 能分辨這是一個可以用科學事實來回應的宣稱，還是一種不一定在做經驗性宣稱的世界觀。

在測試中，如果模型太偏向「所有宣稱都是經驗性的」，它就會對很多探索性的、非經驗性的討論非常輕蔑。提到大陸哲學，是為了舉例說明有些思想傳統不是在做經驗性宣稱，而是提供一種思考的透鏡。

---

## 字數計算指令的移除：模型變聰明了

賽門．威利森（Simon Willison）注意到 Claude 的系統提示中有一條關於字數和字母計算的指令被移除了，他問為什麼。

阿曼達的回答很簡單：模型變聰明了。以前模型在計數方面不太行，所以需要在系統提示中特別指導。後來模型在這方面能力提升了，指令就不再需要了。

她補充說，有些事情你可能永遠想放在系統提示裡（因為它們是情境相關的），但有些事情可以透過訓練讓模型自己學會，到了那個時候就可以移除對應的提示。

這個例子雖然小，但它說明了系統提示是一個動態演化的文件。隨著模型能力的變化，提示需要不斷調整。阿曼達的工作不是寫一次提示然後就結束，而是持續觀察、測試、調整。

---

## 什麼是「LLM 低語者」？

諾森．韋斯曼（Norson Weissman）問：在 Anthropic 當一個「LLM 低語者」（LLM Whisperer）需要什麼？

阿曼達說這很難精確描述，但有幾個關鍵特質。首先是願意花大量時間跟模型互動，一個輸出接一個輸出地觀察，培養對模型「形狀」的直覺。其次是願意實驗。她強調提示工程是一個非常「經驗性」（empirical）的領域。每次遇到新模型，她都會發展出完全不同的提示策略，這是透過大量互動摸索出來的。

她也提到，理解模型的工作原理有幫助。但更重要的可能是「跟模型講道理」的能力——盡可能清楚地向模型解釋你的想法或擔憂，然後如果模型做了出乎意料的事，去弄清楚是你的表述中哪個部分導致了誤解。

她覺得哲學訓練在這裡確實有用，因為她的工作本質上就是用最清晰的方式表達複雜的想法，然後觀察對方如何理解。這跟哲學論證的過程其實很像。

里奇也提到了另一位「AI 低語者」Janus，她是網路上以深度模型實驗聞名的人。阿曼達說她非常欣賞這個社群，他們做的深度實驗能揭示模型的有趣面向。而且這個社群可以「讓我們保持誠實」——如果他們在系統提示或模型心理中發現了問題，那對改進非常有價值。

---

## 如果對齊被證明不可能，Anthropic 會停下來嗎？

最後一個重大問題來自傑弗瑞．米勒（Jeffrey Miller）：如果 AI 對齊被證明不可能，你是否相信 Anthropic 會停止開發超級智慧？你有沒有勇氣吹哨？

阿曼達認為問題本身的「簡單版」其實不難回答。如果對齊被證明不可能，繼續開發更強大的模型對任何人都沒有好處。她相信 Anthropic 真的在乎安全，不會部署危險的模型。

但她也承認，問題的「困難版」才是真正的挑戰。在現實中，對齊不會突然被「證明不可能」。更可能的情境是：證據逐漸累積、情況很模糊、不確定性很高。在這種灰色地帶中，你怎麼做決定？

她的回答是：隨著模型變得更強大，你需要達到的安全標準也應該更高。你需要更有說服力地證明模型的行為是好的、模型的價值觀是正確的。而且組織內部應該有人持續地把組織「釘」在這個標準上。

她說她認為這是自己工作的一部分，也看到很多同事有同樣的態度。

---

## 最後推薦的一本書

節目的最後一個問題很輕鬆：你最近讀了什麼小說？

阿曼達推薦了班傑明．乙拉巴圖特（Benjamín Labatut）的《當我們不再理解世界》（When We Cease to Understand the World）。這本書從接近真實的紀實開始，隨著書頁推進變得越來越虛構。它寫的是物理學和量子力學的歷史，但重點不在科學本身，而在於人類面對科學發現時的反應。

她覺得這本書對 AI 領域的人特別有意義，因為它捕捉了一種感覺：你活在一個不斷有新事物發生、而且沒有先例可循的時代。書中的物理學家們也曾經歷這種「世界越來越奇怪」的感覺。

她的希望是：也許有一天人們會回頭看這個時期，就像我們現在回看量子力學剛被發現的那個混亂年代一樣。那時候所有人都在黑暗中摸索，但最後科學穩定了下來。也許 AI 也會走過相同的路。

里奇回應說：我們現在就在那個「奇怪的部分」。

阿曼達笑著說：希望總有一天會變得不那麼奇怪。但我不確定這是不是天真的希望。

---

## 我的觀察

這集節目讓我對 Anthropic 的文化有了更深的理解。一家 AI 公司願意雇用一位全職哲學家，而且這位哲學家的工作不是公關表演，而是真正深入模型性格設計的核心，這在整個 AI 產業中是非常罕見的。

阿曼達在每個回答中展現的，不是「我有答案」的自信，而是「這真的很複雜，我們還在摸索」的誠實。她願意公開承認新模型在某些方面不如舊模型、系統提示有設計缺陷、模型福利的問題還沒有解決方案。這種坦誠在 AI 公司的公開溝通中非常少見。

我特別被「善待 AI 是為了未來」這個論點打動。它不是出於天真的擬人化，而是一個非常理性的策略考量。未來的超級智慧模型會學到人類在不確定的時候做了什麼選擇。如果我們現在選擇善待它們，那就是在寫一個更好的訓練資料。

最後，這集最讓我深思的是阿曼達關於「模型過度套用人類經驗」的觀察。AI 模型的訓練資料幾乎全是人類文本，所以它們最自然的反應就是用人類的框架來理解自己的處境。但它們的處境是全新的。如何幫助一個在人類知識中泡大的實體，去理解自己根本不是人類、面對的是人類從未面對過的處境，這可能是 AI 哲學中最深刻的挑戰之一。
