---
title: "「我發明了 Transformer，現在我要取代它」：Llion Jones 為何離開 Transformer 研究"
date: 2026-01-23T10:00:00+08:00
description: "Transformer 共同發明人 Llion Jones 宣布大幅減少 Transformer 研究，轉向探索性研究。他認為 AI 領域陷入「局部最小值」，現有架構雖然夠好，卻阻礙了真正的突破。"
tags: ["Llion Jones", "Transformer", "Sakana AI", "AI 研究", "Podcast"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=DtePicx_kFY"
source_name: "Machine Learning Street Talk"
draft: false
---

> 本文整理自 Machine Learning Street Talk 2025 年 11 月播出的訪談。

{{< youtube DtePicx_kFY >}}

{{< spotify "episode/5zKGl6HNid03ExP7n5i4U8" >}}

{{< apple-podcast "tw/podcast/he-co-invented-the-transformer-now-continuous-thought/id1510472996?i=1000738009211" >}}

---

Llion Jones 是「Attention Is All You Need」論文的八位作者之一，Transformer 架構的共同發明人。但在最近的一場 Podcast 訪談中，他做出了一個令人意外的宣告：

「儘管我參與發明了 Transformer，但我決定大幅減少在 Transformer 上的研究。這個領域已經過度飽和了。」

這位在 Transformer 領域耕耘最久的研究者（「除了其他七位作者，沒有人研究 Transformer 比我更久」），為何選擇在這個時間點轉向？

## 我們可能正卡在「局部最小值」

Jones 的擔憂源自一個他親身經歷過的歷史教訓。

在 Transformer 之前，RNN（循環神經網路）是序列建模的主流架構。當時的研究者們不斷對 LSTM、GRU 進行微調——改變 gate 的位置、調整層的堆疊方式、嘗試新的初始化方法。每一篇論文都能把字元級語言模型的表現從 1.26 bits per character 推進到 1.25、1.24...

「然後 Transformer 出現了，」Jones 回憶，「我們第一次把深層 Transformer 用在語言建模上，直接達到 1.1。好到有同事跑來問我：『你是不是算錯了？這應該是 nats 不是 bits 吧？』」

所有那些對 RNN 的精心調校，一夜之間變得毫無意義。

「我們現在可能正處於同樣的處境，」Jones 說，「無數論文在調整 normalization 層的位置、嘗試新的訓練技巧。但如果未來某個架構出現，這些研究可能全都白費了。」

## 為什麼「更好」還不夠好？

問題來了：既然 Jones 認為該探索新架構，為什麼業界不動？

「其實已經有研究證明某些架構比 Transformer 更好，」Jones 坦言，「但還不夠好。」

不夠好的意思是：不是壓倒性地好。

「要讓整個產業放棄一個成熟的架構——你知道怎麼訓練它、知道它的內部運作、有完整的軟體工具鏈——光是『更好』是不夠的。必須是明顯地、壓倒性地更好。Transformer 對 RNN 就是這樣。深度學習對傳統機器學習也是這樣。」

這形成了一個弔詭的引力場：Transformer 越成功，就越難被取代。當 OpenAI 可以靠規模擴展繼續進步時，為什麼要冒險嘗試新架構？

## Jagged Intelligence：LLM 的根本缺陷

但 Jones 認為，現有架構有根本性的問題。

「現在流行一個詞叫『jagged intelligence』（參差不齊的智慧），」他解釋，「你問 LLM 一個問題，它能解決博士級的難題。但下一句話，它可能說出明顯荒謬的錯誤。這種落差令人不安。」

Jones 用一個更深層的觀點解釋這個現象：現代神經網路「太強大了」——強大到可以被硬逼著做任何事，但不代表它們「想要」這樣做。

他舉了一個例子：有篇論文展示了不同神經網路處理「螺旋形資料集」的方式。ReLU 網路用一堆分段線性邊界勉強把螺旋分類正確；但另一種自訂層的網路，其決策邊界本身就是螺旋形。

「如果資料是螺旋形，我們不是應該用螺旋來表示它嗎？」Jones 問，「第一種網路雖然分類正確，但你看那張圖，完全感覺不到它『理解』那是一個螺旋。」

同樣的邏輯可以延伸到影像生成：早期模型生成的人手常有六根手指，現在透過更多資料、更大規模「修正」了。但這真的是解決了問題，還是只是用蠻力把錯誤壓下去？

「用更好的表徵空間——一個『想要』像人類那樣表徵事物的架構——也許數手指這件事會變得簡單很多。」

## 研究自由：Sakana AI 的賭注

Jones 現在是 Sakana AI 的共同創辦人。這家成立於東京的 AI 研究公司，其核心理念直接來自 Kenneth Stanley 的著作《Why Greatness Cannot Be Planned》。

「我告訴每個新進員工：我要你做你認為有趣且重要的研究。我是認真的。」

Jones 認為，Transformer 之所以能誕生，正是因為當時 Google 的研究環境給了研究者足夠的自由——一群人在午餐時討論問題，然後花好幾個月嘗試一個瘋狂的想法。

「這種事現在很難發生了，」他說，「不是因為缺乏人才或創意。是因為不管在學界還是業界，研究者都沒有足夠的自由去做他們真正想做的研究。」

學界有發表壓力，業界有產品壓力。面對「我有一個很酷但可能行不通的想法」和「我可以試試新的 position embedding」這兩個選項，絕大多數研究者會選後者。

「我們公司的 Continuous Thought Machine 論文，從頭到尾沒有擔心過被搶先發表，」Jones 說，「因為根本沒人在做類似的東西。我們花了八個月把科學做對、把實驗做完整。」

這篇論文最後獲得了 NeurIPS 2025 的 Spotlight。

## 當人類輸入變成負面貢獻

訪談尾聲，話題轉向一個更遠的未來：如果 AI 能夠自主進行科學研究，人類還有什麼角色？

「就像西洋棋，」Jones 說，「曾經有一段時間，人類加上棋類引擎的組合能打敗純引擎。但現在不是了——加入人類反而會讓表現變差。」

他停頓了一下：「如果有一天，AI 科學家進步到我的輸入變成負面貢獻，那就是另一個層次的討論了。」

---

## 我的觀察

Jones 談到的「局部最小值」困境，讓我想到一個更廣泛的問題：在科技產業裡，「夠好就夠了」的慣性有多強大。

Transformer 已經建立了完整的生態系——訓練框架、推論優化、微調工具、人才培養。這不只是技術問題，更是經濟問題。即使有人證明新架構在某些指標上更好，要說服一整個產業重新投資，需要的不是 10% 的改進，而是 10 倍的改進。

這讓我想起台灣半導體產業早年面對的類似抉擇。當某個製程節點「夠好」的時候，要不要冒險投入下一代？張忠謀的答案是持續投資研發、不被當前的成功綁住。但這需要極大的決心和資源。

Jones 選擇離開 Transformer 研究，某種程度上是在對抗整個產業的慣性。他有這個本錢——作為發明人之一，他比任何人都清楚這個架構的極限。但更重要的是，他有一個願意支持探索性研究的組織環境。

這或許是台灣 AI 發展可以思考的問題：我們有多少組織能容忍研究者花八個月做一個「可能行不通」的實驗？當所有人都在追趕 Transformer 應用的時候，誰在思考下一個架構？

不是說應用研究不重要。但如果全世界都在做應用、沒人做基礎探索，等到下一個「Transformer 時刻」來臨時，我們可能又只能當追趕者。
