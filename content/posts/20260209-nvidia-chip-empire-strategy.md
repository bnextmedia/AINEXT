---
title: "NVIDIA 不再只靠一款 GPU 打天下：收購 Groq 背後的晶片帝國新戰略"
date: 2026-02-09T10:00:00+08:00
description: "半導體分析機構 SemiAnalysis 創辦人 Dylan Patel 深入拆解 NVIDIA 收購 Groq、推出 CPX 晶片的戰略邏輯，以及 CUDA 護城河正在從程式語言層面轉向系統軟體層面的關鍵轉變。當 vLLM 開始支援 AMD 和 TPU，NVIDIA 靠什麼守住 75% 的毛利率？"
tags: ["NVIDIA", "Groq", "AI 晶片", "Dylan Patel", "SemiAnalysis"]
categories: ["AI 產業動態"]
image: "/images/posts/20260209-nvidia-chip-empire-strategy.webp"
source_url: "https://www.youtube.com/watch?v=DqBMzuzxZog"
source_name: "MAD Podcast"
related_companies: ["nvidia"]
related_people: ["jensen-huang"]
draft: false
---

> 本文整理自 MAD Podcast 2026 年 2 月播出的單集。

{{< youtube DqBMzuzxZog >}}

---

## 一款 GPU 的時代結束了

不久前，NVIDIA 的產品策略還可以用一句話概括：一款通用 GPU 搞定所有工作負載。從訓練到推理、從語言模型到影像生成，Ampere、Hopper、Blackwell 這條主線一路往上堆規格，市場也照單全收。但半導體產業研究機構 SemiAnalysis 創辦人 Dylan Patel 在 MAD Podcast 上的一席話，點出了這個策略正在轉向。

NVIDIA 收購 Groq，不是因為 Groq 的營收多亮眼。事實上，Groq 去年的營收數字很難看，遠低於預期。但 NVIDIA 看中的是 Groq 的技術 IP 和團隊。Groq 的晶片架構把所有記憶體放在晶片上，完全不依賴外部記憶體，這讓它在特定工作負載上能達到驚人的速度，特別是單一串流的自迴歸 token 生成。用 Dylan Patel 的話來說，Groq 在通用工作負載上「不行」，不能訓練模型、不能高效推理超大模型、也無法同時服務大量使用者，但它在特定場景下可以「快到嚇人」。

同一時間，NVIDIA 還推出了 CPX 晶片，專門處理 prefill（預填充）階段，也就是建立 KV cache 的那一步。CPX 不需要昂貴的高頻寬記憶體，成本遠低於通用 GPU，但在處理大量上下文輸入時效率極高，對影片生成模型也特別有用。加上原本的通用 GPU 產品線，NVIDIA 現在手上有三種不同定位的晶片，分別對應訓練、快速解碼、和低成本預填充三個市場。

---

## Jensen 的偏執是最好的護城河

這種「晶片組合拳」策略的背後，是黃仁勳（Jensen Huang）對競爭的極度偏執。Dylan Patel 觀察，NVIDIA 之所以能在 AI 晶片市場一路領先，靠的就是在別人還沒看清方向的時候，先押下「最大表面積」的賭注。當年通用 GPU 能贏，就是因為它什麼都能做，而 AI 模型的架構恰好往那個方向演化。但現在 AI 的工作負載已經大到足以支撐專用晶片的存在，如果 NVIDIA 只守住通用 GPU 這一條線，遲早會在某些細分市場被人用更低的成本和更好的性能擊敗。

這個恐懼不是空穴來風。NVIDIA 的通用 GPU 要賣到 75% 以上的毛利率，它的性能就必須比競爭對手好上兩到四倍，因為這是它收取的溢價倍數。但當 Google 的 TPU、Amazon 的 Trainium、甚至 AMD 的 MI 系列都在快速進步，要持續維持四倍的性能差距，光靠堆更多電晶體和更快的記憶體已經不夠了。NVIDIA 需要在不同的工作負載上都有最佳方案，而不是用一款通用晶片去「將就」所有場景。

更關鍵的是，AI 模型的架構本身還在快速演化。Dylan Patel 指出，目前大家唯一確定的是模型是自迴歸的，下一個 token 的生成是核心。但除此之外，注意力機制怎麼變、是否走向稀疏化、推理時是單一思維鏈還是多路並行，這些都還沒有定論。Google 和 OpenAI 的 Pro 模型已經開始用多路並行推理，而不是只走一條思維鏈。如果這個方向成為主流，需要的硬體架構會和目前的「極速單串流解碼」完全不同。NVIDIA 用收購 Groq 加上自研 CPX，就是為了在不確定的未來裡，確保自己不管模型怎麼演化，都有對應的晶片方案。

---

## CUDA 的護城河正在質變

在 AI 晶片的討論裡，CUDA 生態系一直被視為 NVIDIA 最深的護城河。但 Dylan Patel 提出了一個不太一樣的觀察：CUDA 作為「程式語言」的護城河正在弱化，但 NVIDIA 正在把護城河從語言層面轉移到系統軟體層面。

真正的轉變藏在一個聽起來很無聊的事實裡：大多數人使用 AI 晶片的方式，根本不是寫 CUDA 程式碼。他的觀察是，絕大多數企業、新創、甚至科技公司使用 GPU 的方式，就是下載一個開源推理引擎（比如 vLLM 或 SGLang），下載一個開源模型，然後按下「開始」。沒有人真的在寫 CUDA kernel。就連做研究的人，多數也是用 PyTorch 寫程式，然後讓 torch.compile 把它編譯到 GPU 上執行。從手寫 CUDA kernel，到用 PyTorch 編譯，再到直接下載 vLLM 一鍵部署，每往上一層抽象，能操作的人就多一個數量級，但對底層硬體的依賴也就弱了一層。

這正是 NVIDIA 感到緊張的地方。vLLM 和 SGLang 這兩個主流開源推理引擎，現在已經把 AMD GPU 當作「幾乎是一等公民」來支援，對 TPU 和 Trainium 的支援也正在快速推進。Dylan Patel 估計，到今年年中，所有主要晶片平台在 vLLM 上都會有不錯的使用體驗。如果你只是要下載一個模型、跑在推理引擎上，底層是 NVIDIA 還是 AMD 的差異會越來越小。

那 NVIDIA 的新護城河在哪裡？答案是比「語言」更深、比「框架」更廣的系統軟體層。Dylan Patel 舉了一個具體的例子：KV cache 管理。當 AI 代理（agent）在處理程式碼重構這類長時間任務時，它會不斷在不同的程式碼段落之間切換上下文，每次切換都要重新生成 KV cache，這是預填充階段的主要成本。像 Cursor 和 Claude Code 這類編碼工具的營運商，多數成本其實花在預填充而非解碼上，因為上下文太大、切換太頻繁。

如果能把 KV cache 存到 SSD 上，需要的時候再拉出來，就不用每次都重新計算，成本可以大幅下降。但這牽涉到記憶體管理、儲存管理、網路壅塞處理、分散式儲存節點的協調，是一整套系統軟體的工程。Anthropic、Google、OpenAI 這些大公司已經自己解決了這個問題，但廣大的開源生態和中小企業還沒有。NVIDIA 正在投入大量資源開發這類開源系統軟體，包括 KV cache 管理器、Triton Inference Server、Dynamo 等工具。這些軟體不是 CUDA，但它們是「CUDA 護城河的新形態」，用生態系的廣度和深度來鎖住使用者，而不是靠底層語言的獨佔性。

---

## AI 晶片新創的機會窗口只有一條縫

對於 Etched、MADDX、Positron、Cerebras 這些 AI 晶片新創，Dylan Patel 的態度是「興奮但極度懷疑」。他直言，這些公司成功的機率每一家都低於 1%。不是因為它們的技術不好，而是因為它們面臨一個結構性的困境：你今天優化的目標工作負載，在兩年後晶片真正量產時，可能已經不存在了。

第一波 AI 晶片新創（包括早期的 Groq、Cerebras、SambaNova）都做了類似的賭注：把大量記憶體放在晶片上。這個方向在很長一段時間裡都不太行，直到最近推理工作負載爆發，才終於找到用武之地。但等到 Groq 的商業模式剛開始有起色，它就被 NVIDIA 直接收購了。這對其他新創來說是一個警示：即使你賭對了方向，NVIDIA 也可能直接把你買下來，讓你變成它的晶片組合拳的一部分。

新一波的新創公司確實有更清晰的願景。它們不是在做一款「萬能晶片」，而是針對特定的模型架構和工作負載做深度優化。比如有的公司專門針對解碼、有的針對稀疏注意力、有的針對特定的矩陣運算規格。但問題在於，模型的架構和注意力機制還在快速演化，稀疏注意力技術（如 N-gram）是否會成為主流、模型是否會從稠密走向稀疏，這些都會徹底改變硬體需求。如果你花兩年開發的晶片，正好對上了被淘汰的架構，那就是數億美元的沉沒成本。

Dylan Patel 認為，這些新創真正的成功場景，是一個「多晶片並存」的世界，每個客戶會根據不同的工作負載使用不同的加速器。Anthropic 不在乎影像和影片生成，它需要的是長時間解碼的效率；Midjourney 不在乎語言模型推理，它需要的是運算密集型的影像生成能力。如果一款專門為影像生成優化的晶片存在，Midjourney 絕對會在推理階段全面採用。但訓練呢？訓練還是會用通用 GPU，因為靈活性太重要了。

---

## 我的觀察

Dylan Patel 的分析讓我印象最深的一點，是他對 CUDA 護城河的重新定義。過去我們談 NVIDIA 的競爭優勢，往往聚焦在「幾千個 CUDA 工程師」和「龐大的 CUDA 生態系」上，好像只要這兩件事存在，NVIDIA 就固若金湯。但 Dylan Patel 指出了一個正在發生的結構性轉變：當使用者從「寫 CUDA kernel」進化到「用 PyTorch 編譯」再到「下載 vLLM 按下開始」，底層硬體的可替換性就在一層一層地增加。

NVIDIA 顯然看到了這個趨勢，而且反應得很快。收購 Groq 不只是買一款晶片，更是買一個「萬一 AI 模型往極速單串流解碼方向演化」的保險。推出 CPX 也不只是增加一個產品線，而是在推理成本這個最敏感的戰場上先卡位。最聰明的是投入開源系統軟體，因為誰控制了推理堆疊的「中間層」，誰就控制了未來 AI 基礎設施的生態。這不是一個防守策略，而是一個重新定義戰場的進攻策略。

問題是，NVIDIA 能跑多快？它的對手不只是 AMD 和晶片新創，還有 Google、Amazon、Meta 這些自研晶片的超大規模企業。這些公司有自己的工作負載資料、有自己的模型團隊、有足夠的規模去開發專用晶片。當 AI 的市場大到足以支撐五六條不同的晶片產品線時，「通用 GPU 之王」這個頭銜能帶來的溢價，恐怕會比過去十年少很多。
