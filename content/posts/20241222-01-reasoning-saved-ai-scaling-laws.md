---
title: "01 reasoning saved ai scaling laws"
date: 2025-12-22T20:58:54+08:00
description: "如果不是 Reasoning 模型的出現，AI 的進展可能在 2024 年就停滯了。這個說法聽起來驚人，但背後有著嚴謹的技術邏輯——而且這個故事鮮為人知。 Gavin Baker 是 Atreides 投資公司創辦人，曾任 Fidelity 科技基金經理人，長期追蹤 AI 與半導體產業。2024"
tags: ["AI"]
categories: ["AI"]
draft: false
---


如果不是 Reasoning 模型的出現，AI 的進展可能在 2024 年就停滯了。這個說法聽起來驚人，但背後有著嚴謹的技術邏輯——而且這個故事鮮為人知。

Gavin Baker 是 Atreides 投資公司創辦人，曾任 Fidelity 科技基金經理人，長期追蹤 AI 與半導體產業。2024 年 12 月，他接受知名投資 Podcast《Invest Like the Best》主持人 Patrick O'Shaughnessy 專訪，深入剖析了 AI 產業的競爭格局、技術演進與投資邏輯。在這場近兩小時的對談中，他揭露了一個公開市場投資人普遍忽略的事實：基於預訓練規模定律的邏輯，2024 和 2025 年的 AI 進展「理論上不應該發生」。這個看似矛盾的陳述，需要從 Scaling Laws 的本質說起。

---

## Scaling Laws：我們精確測量卻不理解的神秘法則

要理解 Reasoning 為什麼「拯救」了 AI，首先要理解什麼是 Scaling Laws（規模定律）。這個概念是當前 AI 發展的核心驅動力，但它的本質卻帶有一種令人不安的神秘性。

Pre-training Scaling Laws（預訓練規模定律）並非像牛頓力學那樣的物理定律，而是一個「經驗觀察」。研究人員發現，當你增加模型的參數量、訓練資料量、以及運算量時，模型的表現會以一種可預測的方式提升。這個觀察已經被精確測量並持續驗證了很長時間，成為各大 AI 實驗室投入數十億美元建設超大規模運算叢集的理論基礎。然而，沒有人真正知道這個定律為什麼會成立。

Baker 用了一個精妙的比喻來描述這種認知落差。古埃及人可以精確測量太陽的運行軌跡，精確到能夠把金字塔的東西軸完美對準春分點，巨石陣的建造者同樣展現了對太陽週期的精確掌握。但他們完全不懂軌道力學，不知道地球繞著太陽轉，不知道為什麼太陽會東升西落、為什麼會有四季變化。他們的神話中，太陽是由神駕著戰車拉過天空。當代 AI 研究者對 Scaling Laws 的理解，與古人對太陽的理解處於類似的階段：精確測量，但缺乏根本性的理解。

這種認知狀態帶來了一個實際問題：既然我們不知道 Scaling Laws 為什麼會成立，我們也無法確定它什麼時候會停止成立。每一次新模型的訓練，都是對這個經驗定律的又一次驗證。這就是為什麼 2024 年底 Google 發布 Gemini 3 時，業界如此關注——它證明了預訓練規模定律在又一個數量級上依然有效。這個確認對於整個產業的信心至關重要，因為目前所有的大規模資本支出，都是基於這個定律會繼續成立的假設。

---

## Blackwell 延遲：科技史上最複雜的產品轉換

2024 年，Nvidia 的新一代晶片 Blackwell 遭遇了嚴重的產品延遲。這不是普通的供應鏈問題或良率挑戰，而是科技史上「最複雜的產品轉換」之一。理解這次延遲的嚴重性，需要先理解從 Hopper 到 Blackwell 究竟改變了什麼。

從 Hopper 世代升級到 Blackwell，改變的不只是晶片本身，而是整個資料中心的基礎架構。冷卻系統從空氣冷卻改為液體冷卻，這意味著需要全新的管路、泵浦、冷卻液分配單元。機架重量從約 1,000 磅暴增到 3,000 磅，許多現有資料中心的地板根本承受不了這個重量，需要結構補強甚至重建。功耗從約 30 千瓦（相當於 30 個美國家庭用電量）躍升到 130 千瓦，這不只是電費問題，而是需要全新的供電基礎設施、變壓器、配電系統。

Baker 用一個生動的比喻來形容這種轉換的複雜度：想像你要換一支新 iPhone，但你必須先把家裡所有插座換成 220 伏特、裝一個 Tesla Powerwall 儲能系統、裝一台備用發電機、裝太陽能板來應付暴增的用電需求，然後還要加固地板，因為地板承受不了這支新手機的重量。這聽起來荒謬，但這正是資料中心營運商在 2024 年面對的現實。他們不是在升級伺服器，而是在重建整個基礎設施。

更棘手的是，這些極其複雜的機架必須穩定運作才能發揮作用。Blackwell 的設計密度極高，散熱成為巨大挑戰。即使所有基礎設施都到位，要讓這些機架持續穩定運作、把熱量有效排出，也需要大量的工程調校。這就是為什麼 Blackwell 在 2024 年底才開始真正大規模部署，比原本預期晚了將近一年。

---

## 一個被忽略的危機：AI 進展理論上應該停滯

這場延遲帶來了一個很少被公開討論的危機。根據預訓練規模定律的邏輯，AI 能力的提升需要更大規模的運算。當 XAI 在 2024 年初成功讓 20 萬張 Hopper GPU 協同運作（也就是業界所說的「連貫」運作，每個 GPU 都知道其他 GPU 在「想」什麼，透過高速網路共享記憶體）之後，Hopper 世代的算力天花板基本上已經觸及。物理定律限制了你能讓多少張 GPU 保持連貫，大約就是 20 萬到 30 萬張的量級。

這意味著，要繼續推動預訓練規模定律、訓練出更強大的基礎模型，就必須等待下一代晶片。Blackwell 的運算效能遠超 Hopper，理論上可以在相同數量的 GPU 上實現數倍的等效算力。但 Blackwell 延遲了。與此同時，Google 的下一代 TPU 也還沒準備好。整個產業陷入了一個尷尬的真空期。

如果你嚴格按照預訓練規模定律的邏輯推演，2024 年到 2025 年這段期間，AI 能力應該會停滯。已經沒有更多的 Hopper 算力可以投入，Blackwell 又還沒到位。大眾投資人對這個問題並不敏感，因為他們看到的是持續發布的新模型、持續改善的基準測試分數、持續擴張的應用場景。但這些進展的來源，與過去幾年截然不同。

---

## Reasoning 的出現：兩個新的規模定律

就在 AI 可能停滯的關鍵時刻，Reasoning（推理）模型出現了。2024 年 10 月，OpenAI 發布了 o1 模型，這是第一個真正意義上的 Reasoning 模型。它的出現，為 AI 開闢了一條不依賴硬體升級的進步路徑。

傳統的預訓練規模定律之外，現在有了兩個新的規模定律。第一個是 Post-training Scaling Laws（後訓練規模定律），核心技術是「帶有可驗證獎勵的強化學習」（Reinforcement Learning with Verified Rewards, RLVR）。「可驗證」是這裡的關鍵概念。Andrej Karpathy（前 Tesla AI 總監、OpenAI 共同創辦人）有一句名言：「在軟體領域，任何你能規格化的東西，你就能自動化。在 AI 領域，任何你能驗證的東西，你就能自動化。」這是一個重要的區別。如果一個任務的結果有明確的對錯判斷——數學證明是否正確、程式碼是否能執行、答案是否符合事實——那麼你就可以用強化學習來大幅提升模型在這類任務上的表現。

第二個新的規模定律是 Test-time Compute（推論時運算）。傳統模型在回答問題時，運算量基本上是固定的，不管問題多難，模型都是「想」差不多長的時間就給出答案。但 Reasoning 模型可以在回答問題時進行更長時間的「思考」，在內部進行多步推理、自我檢查、嘗試不同的解題路徑。你投入越多的推論時運算，模型的表現就越好。這創造了一個新的規模維度：即使基礎模型不變，你也可以透過增加推論時運算來提升表現。

這兩個新的規模定律帶來了驚人的進展速度。ARC-AGI 是一個專門測試 AI 泛化能力的基準測試，被認為是衡量「真正智慧」的重要指標。在過去四年間，最好的 AI 模型在這個測試上的表現從 0% 緩慢爬升到 8%。但在 o1 發布後的短短三個月內，這個數字從 8% 躍升到了 95%。這種跳躍式的進步，完全來自後訓練和推論時運算的改善，而非更大規模的預訓練。

---

## 拯救與彌補：Reasoning 如何讓 AI 穿越硬體真空期

從產業發展的角度來看，Reasoning 模型的出現具有雙重意義。表面上，它是一項技術突破，開闢了新的能力提升路徑。但更深層的意義在於，它彌補了 Blackwell 延遲造成的 18 個月空白期，讓 AI 在硬體升級停滯的情況下繼續展現進步。

這個時機的巧合（或者說必然）值得深思。如果 Reasoning 沒有在 2024 年下半年出現，整個 AI 產業會面臨什麼局面？從 2024 年中到 2025 年底的 Gemini 3 發布，整整 18 個月的時間，預訓練規模定律無法推動任何實質進展。市場會看到什麼？數百億美元的資本支出，但模型能力停滯不前。投資人會如何反應？那些押注在 AI 革命上的科技股會經歷什麼？整個敘事可能會從「AI 正在改變世界」轉變為「AI 遇到了天花板」。

Reasoning 的出現讓這個假設情境沒有發生。公眾看到的是持續的進步：更強的程式碼生成能力、更好的數學推理、更可靠的事實性回答。這些進步來自後訓練和推論時運算的改善，而非更大的基礎模型。但對於不深入追蹤技術細節的觀察者來說，進步就是進步，來源並不重要。AI 的敘事得以延續，資本繼續流入，產業繼續擴張。

從技術演進的角度，這段時期也有其積極意義。它證明了 AI 能力的提升不只有一條路徑。預訓練規模定律仍然重要，但它不再是唯一的遊戲。後訓練和推論時運算提供了互補的提升維度。更重要的是，這三個規模定律是「相乘」而非「相加」的關係。當 Blackwell 終於大規模部署，新的基礎模型開始訓練時，這些模型將同時受益於更強大的預訓練、更成熟的後訓練技術、以及更充裕的推論時運算資源。三重加乘效應，意味著未來幾年的模型進步可能遠超過去任何時期。

---

## Gemini 3 的驗證：預訓練規模定律依然有效

2024 年底，Google 發布了 Gemini 3。這個發布之所以重要，不只是因為它是一個強大的新模型，更因為它提供了一個關鍵的驗證：預訓練規模定律在新的數量級上依然有效。

這個驗證為什麼重要？因為 Gemini 3 是自 Hopper 世代以來，第一個真正測試預訓練規模定律的重大模型。在 Reasoning 崛起的這段期間，產業的注意力大多集中在後訓練和推論時運算上。預訓練規模定律雖然是整個大型語言模型革命的基礎，但它已經有一段時間沒有被新的大規模實驗所驗證了。理論上，它可能已經開始失效而我們還不知道。

Google 在發布 Gemini 3 時明確表示，模型的表現符合預訓練規模定律的預期。這是一個極其重要的數據點。它意味著，當 Blackwell 和後續的 Rubin 晶片大規模部署後，投入更多算力訓練更大的模型，依然會帶來可預期的能力提升。整個產業數千億美元的資本支出計畫，其底層假設依然成立。

Gemini 3 也展現了實際的能力躍進。它是第一個成功幫 Baker 完成餐廳訂位的 AI 模型——不是推薦餐廳，而是實際完成訂位這個動作。這聽起來是個小事，但它標誌著一個重要的轉折：AI 開始能夠「做事」，而不只是「說話」。如果 AI 能訂餐廳，那離訂飯店、訂機票、叫車就不遠了。一個真正的個人助理開始成形。

---

## 2026 年展望：三重規模定律的加乘效應

展望未來，Baker 預測第一個在 Blackwell 上訓練的模型將在 2026 年初問世，最可能來自 XAI。這個預測基於一個簡單的事實：沒有人建資料中心比 Elon Musk 更快。Nvidia 執行長黃仁勳已經公開證實了這一點。XAI 部署新 GPU 的速度最快，因此能最先與 Nvidia 合作解決新晶片的各種問題，也最先完成大規模訓練。

但取得 Blackwell 只是第一步。即使你擁有了這些晶片，也需要六到九個月的時間才能讓它們的表現達到前一代 Hopper 的水準。這不是硬體問題，而是軟體和工程調校的問題。Hopper 已經被各團隊用了兩年多，每個人都知道怎麼最佳化它，軟體堆疊已經針對它的特性精細調校，工程師熟悉它的所有怪癖和最佳實踐。Blackwell 是全新的，所有這些知識都需要重新累積。歷史經驗顯示，當 Hopper 剛推出時，也花了六到十二個月才真正超越前一代的 Ampere。

當 Blackwell 模型終於問世，它們將展現三重規模定律加乘的威力。更大規模的預訓練帶來更強的基礎能力，更成熟的後訓練技術帶來更好的指令遵循和推理能力，更充裕的推論時運算帶來更深入的思考和更可靠的答案。GB300（Blackwell 的改進版）和 AMD 的 MI450 帶來的每 Token 成本大幅下降，意味著這些模型可以被允許「思考」更長時間，執行更複雜的任務。

這種能力提升會如何轉化為實際應用？一些領先的科技公司已經讓 AI 處理超過 50% 的客戶支援工作。這是一個價值 4,000 億美元的產業。AI 在說服和溝通方面的能力，直接對應到銷售和客戶支援這兩個核心商業功能。如果把企業的功能簡化為三件事——製造產品、銷售產品、支援客戶——到 2026 年底，AI 可能已經能夠很好地處理其中兩項。機器人技術也終於開始進入實用階段，各種新創公司正在湧現，儘管大規模量產的競爭最終可能集中在 Tesla 的 Optimus 和中國廠商之間。

---

## 投資視角：這場技術演進的資本意涵

從投資的角度來看，這段技術演進史揭示了幾個重要的洞察。預訓練規模定律依然有效，這意味著「更大就是更好」的資本競賽還會持續。那些能夠取得最大規模算力的玩家，在基礎模型競爭中仍具有結構性優勢。這解釋了為什麼 Meta、Microsoft、Google、Amazon 都在競相擴張資料中心，為什麼 OpenAI 提出了 1.4 兆美元的 Stargate 計畫。

Reasoning 模型的出現創造了新的競爭維度，但也加高了進入門檻。後訓練需要大量的人類反饋數據和精細的工程調校，推論時運算需要龐大的即時算力支撐。這些都是資本密集且知識密集的能力。更重要的是，Reasoning 創造了一個飛輪效應：用戶使用產生數據，數據被用來改進模型，改進的模型吸引更多用戶。這個飛輪過去在 Netflix、Amazon、Google 運轉了十幾年，現在終於開始在 AI 領域轉動。領先者的優勢將越來越難以追趕。

Blackwell 的延遲給了 Google 一個暫時的喘息空間。在 Blackwell 部署困難的這段期間，Google 的 TPU v6 和 v7 成為相對先進的選擇，讓 Google 得以維持「Token 低成本生產者」的地位，對競爭對手施加價格壓力。但這個優勢窗口正在關閉。當 Blackwell 和後續的 Rubin、GB300 大規模部署，成本結構將重新洗牌。那些擁有最新 Nvidia 硬體的玩家將奪回成本優勢，而 Google 的戰略計算將需要調整。

綜觀這一切，一個更宏觀的模式浮現：無論 AI 發展遇到什麼瓶頸，似乎總能找到突破口。硬體升級停滯？Reasoning 填補空白。電力供應受限？太空資料中心的可能性正在浮現。稀土供應被中國掌控？替代供應鏈和新提煉技術正在加速發展。這種「AI 需要什麼就得到什麼」的模式，究竟是巧合、是市場機制的自然調節、還是某種更深層的技術演進必然性？這個問題本身，或許就是理解當前 AI 革命最重要的線索。

---

🎧 **收聽本集完整訪談**：
- [Invest Like the Best EP.451 - Gavin Baker](https://podwise.ai/dashboard/episodes/6327964)

---

*本文為 AINEXT 系列報導「Gavin Baker 談 AI 產業」第一篇。下一篇將探討 Baker 對太空資料中心的驚人分析。*
