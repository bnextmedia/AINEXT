---
title: "LMArena 募資 1 億美元：從柏克萊地下室到 AI 評測標準制定者"
date: 2026-01-27T10:00:00+08:00
description: "LMArena 從柏克萊大學的學術專案，蛻變為募資 1 億美元的 AI 評測霸主。本文完整解析其商業化歷程、排行榜爭議、NanoBanana 現象，以及為何「評測即權力」正在重塑 AI 產業格局。"
tags: ["LMArena", "Anastasios Angelopoulos", "AI 評測", "a16z", "Chatbot Arena", "Podcast"]
categories: ["AI 產業動態"]
source_url: "https://www.youtube.com/watch?v=NBnOk0Uy9ig"
source_name: "Latent Space Podcast"
draft: false
---

> 本文整理自 Latent Space Podcast 2026 年 1 月播出的單集，訪談 LMArena 共同創辦人 Anastasios Angelopoulos。

{{< youtube NBnOk0Uy9ig >}}

{{< spotify "episode/0y5p7g1JkQjSSX8GsZNvxi" >}}

{{< apple-podcast "tw/podcast/state-of-evals-lmarenas-1-7b-vision-anastasios/id1674008350?i=1000743324937" >}}

---

如果你曾經想知道「哪個 AI 模型比較強」，大概都看過 LMArena 的排行榜。這個從柏克萊大學地下室起步的學術專案，如今已募資 1 億美元，成為全球最具影響力的 AI 模型評測平台。當一個匿名代號「NanoBanana」能撼動 Google 數十億美元市值，當 Cohere 研究團隊發表論文公開質疑其公平性，LMArena 的一舉一動，已經牽動整個 AI 產業的神經。

## 從學術專案到創業：一個不得不做的決定

LMArena 的前身是柏克萊大學的 LMSys 研究計畫。共同創辦人 Anastasios Angelopoulos 回憶，當時他們只是一群在地下室做研究的學者，沒想過要創業。轉折點來自 a16z 的合夥人 Anj，他主動找上門，提供資金讓團隊繼續開發，甚至幫他們成立公司實體，但完全不強迫——「你們隨時可以走人，把錢留著也沒關係。」

這種「先養再說」的孵化模式相當大膽。Anj 賭的是：這群人遲早會發現，要把 LMArena 做大，學術體制撐不住。果然，隨著用戶數飆升，Angelopoulos 意識到，無論是繼續當學術專案或轉型非營利組織，都無法取得足夠資源來維持平台運作。世界需要一個能夠衡量、理解、推進前沿 AI 能力的地方，而這件事只有商業公司才做得到。

2025 年底，LMArena 宣布完成 1 億美元募資。Angelopoulos 對這筆錢的態度很務實：「錢的用途是讓你有籌碼可以翻牌。第一個賭注失敗了，還能下第二個、第三個。」但他也坦言，平台營運成本確實很高——LMArena 負擔所有模型的推論費用，讓用戶免費使用。即使拿到企業等級的折扣，這仍是一筆龐大支出。

資金的另一個重要用途，是把前端從 Gradio 遷移到 React。Gradio 是 Hugging Face 推出的快速原型工具，曾幫助 LMArena 在早期快速擴張到百萬用戶，但隨著功能需求增加，團隊發現很多客製化介面在 Gradio 上很難實現。更現實的問題是：熟悉 Gradio 的工程師太少，招聘困難。轉到 React 後，不僅開發彈性大增，人才池也寬廣得多。

## 五百萬用戶背後的數據

LMArena 目前每月活躍用戶超過 500 萬，平台上累計發生超過 2.5 億次對話。這不只是一個 AI 玩具網站——根據團隊的調查與 prompt 分析，約有 25% 的用戶是軟體從業者，其他則分布在醫療、法律、金融、行銷等各行各業。

值得注意的是，現在約有一半的用戶選擇登入使用。這是刻意設計的留存機制：登入後可以保存對話歷史，下次回來還能接續之前的問題。聽起來很基本，但這個功能大幅提升了用戶黏著度。Angelopoulos 說得很直白：「用戶的忠誠度是每天掙來的。他們隨時可以離開，非常善變。所以你必須時時刻刻思考：我能給他們什麼價值？他們怎麼使用我的網站？我還能多給他們什麼？」

## 與競爭者的差異：真實用戶 vs. 預設題庫

市場上不只 LMArena 在做 AI 評測。Artificial Analysis（人工分析）是另一個常被提及的平台，定位是成為「AI 界的 Gartner」，主要業務是彙整公開的基準測試結果，提供企業顧問服務。他們也有自己的 Arena，但運作方式不同。

關鍵差異在於：Artificial Analysis 的 Arena 使用預先產生的內容讓用戶投票，例如影片 Arena 會展示事先生成好的影片讓你比較。這樣做的好處是用戶不用等待，投票體驗更流暢。但 Angelopoulos 認為這犧牲了真實性——「你真的在乎別人生成的影片嗎？」

LMArena 的做法完全相反：用戶自己輸入問題，系統即時調用兩個匿名模型回答，用戶根據實際結果投票。這意味著每一筆數據都來自真實使用場景，不是研究者設計的測試題。Angelopoulos 認為這是 LMArena 最核心的價值：「給整個世界一個基準真相，讓大家知道真實用戶怎麼使用這些模型，以及模型在這些場景下的表現如何。」

## 排行榜幻覺：一場公開的學術爭論

2025 年 4 月，一篇名為《The Leaderboard Illusion》（排行榜幻覺）的論文在 AI 社群引發軒然大波。作者群來自 Cohere Labs、普林斯頓大學、史丹佛大學、MIT、華盛頓大學、滑鐵盧大學，以及艾倫人工智慧研究所，陣容堅強。Cohere Labs 的研究主管 Sara Hooker 甚至用「危機」來形容 AI 評測排行榜的可信度問題。

論文的核心指控有四點。第一，私下測試造成不公平優勢：Meta、Google、OpenAI、Amazon 等大廠可以在模型正式發布前，先在 LMArena 上測試多個變體版本，只公布表現最好的那個。論文指出，Meta 在推出 Llama 4 之前，測試了多達 27 個私有變體。第二，數據存取嚴重不平等：四大廠（OpenAI、Google、Meta、Anthropic）合計佔據 62.8% 的 Arena 數據，而 83 個開源模型只分到 29.7%。第三，過度擬合風險：研究顯示，針對 Arena 數據訓練的模型，在 Arena 專屬基準上可以獲得 112% 的性能提升，但在其他測試集上反而下降。第四，靜默下架模型：有 205 個模型被移除時完全沒有通知，而且不成比例地影響開源選項。

LMArena 很快發布正式回應，逐點反駁。針對開源模型比例，他們指出論文計算方式有誤，漏算了 Llama、Gemma 等開源權重模型，實際比例是 40.9%，不是 8.8%。針對預發布測試能大幅提升分數的說法，他們澄清論文引用的圖表是用高斯分布模擬出來的，跟真實 Arena 無關；根據他們的數據，預發布測試帶來的分數提升約 11 Elo，遠低於論文宣稱的 100 多分。針對「秘密政策」的指控，他們強調預發布測試的政策早在 2024 年 3 月就公開了，從來不是什麼秘密。

不過，LMArena 也承諾做出改進：未來會更清楚標記已退役的模型，當同時測試超過 10 個模型變體時，分數會標註為「暫定」，直到累積 2000 份新投票才確定。

柏克萊大學教授、LMArena 共同創辦人 Ion Stoica 的評價很直接：這篇論文充滿「不準確」和「可疑的分析」。但無論如何，這場爭論確實點出了一個真實的治理難題——當評測平台變得如此重要，它的公平性該由誰來監督？

## NanoBanana：一個代號如何撼動科技巨頭

說到 LMArena 的影響力，不能不提 NanoBanana 事件。這是一個匿名的圖像生成模型代號，在 LMArena 上進行預覽測試時，表現驚艷全場，迅速在社群引爆討論。Angelopoulos 說，NanoBanana 的成功「改變了 Google 的市場份額」，甚至讓 Google 內部傳出針對圖像生成的「Code Red」警報。

有趣的是，NanoBanana 這個名字其實有個溫馨的由來：它是以負責這個專案的產品經理 Naina 的暱稱命名的。但對 LMArena 團隊來說，這只是個隨機產生的代號——他們完全沒想到這個名字會變成全球話題。

這個案例完美展現了 LMArena 社群的威力。用戶喜歡追蹤匿名模型、猜測背後是哪家公司、在社群媒體上分享測試結果。這種「尋寶」的樂趣，讓預覽測試機制成為病毒式傳播的引擎。當然，這也是 Cohere 論文批評的焦點之一：只有大廠有資源玩這種遊戲。

## 平台誠信：不能買的排行榜

面對各種質疑，Angelopoulos 反覆強調一件事：公開排行榜的誠信不可妥協。

他用了一個很有意思的比喻：「公開排行榜對我們來說是慈善事業，是虧本在做的。」這話聽起來有點誇張，但邏輯是這樣的——排行榜本身不賺錢，你不能付費讓模型上榜，也不能付費把表現差的模型撤下來。這跟 Gartner 之類的分析機構完全不同，那些地方是可以「付費玩遊戲」的。

LMArena 的營利模式在別處：企業客戶可以付費進行私有測試、取得更詳細的分析報告、在特定垂直領域做客製化評估。但公開排行榜永遠保持中立，分數完全由用戶投票決定。「排行榜上的每一個分數都是統計上可靠的，反映模型的真實能力。為什麼？因為全世界數百萬人投票，我們只是把這些票數轉換成一個數字。」

## 未來方向：從文字到影片，從模型到 Agent

LMArena 的野心不只是文字模型排行榜。他們已經推出 CodeArena（程式碼競技場）和 Expert Arena（專家競技場），後者可以展示模型在醫療、法律、金融、會計、創意行銷等垂直領域的表現。由於用戶基數夠大，即使某個專業領域只佔用戶的個位數百分比，也代表數十萬人的真實使用數據。

下一步是多模態。影片生成的 Arena 預計今年稍晚上線，讓用戶可以用自己的 prompt 即時生成影片來比較——不是看別人生成好的內容，而是真正測試「我的需求，模型能不能滿足」。

更有趣的是 Agent 評估。Angelopoulos 提到，他們正在思考如何把完整的 AI Agent 系統放上 Arena。他直接對 Cognition 喊話：「讓我們把 Devin 放上 Arena，想辦法把 Devin 的整個工具鏈接進來。」CodeArena 本質上已經是一種 Agent 評估，但如果能支援像 Devin 這樣的完整開發助手，就能回答一個很多人想知道的問題——這些 AI Agent 到底誰比較強？

至於是否開放 API 讓外部開發者接入，Angelopoulos 說他們考慮過，但目前的答案是「專注優先」。新創公司資源有限，應該把一件事做到極致，而不是四處擴張。

## 我的觀察

### 評測即權力

LMArena 的故事，本質上是一個關於「誰來制定標準」的故事。

當全世界的開發者、投資人、企業採購都在看同一個排行榜來決定要用哪個模型，這個排行榜的制定者就擁有了巨大的權力。NanoBanana 事件是最好的證明：一個匿名代號在排行榜上的表現，可以讓 Google 內部發布緊急警報，可以影響科技巨頭數十億美元的市值波動。

這讓我想到金融界的信用評等機構。標準普爾、穆迪、惠譽三大機構的評等，決定了企業和國家的借貸成本。2008 年金融海嘯後，大家才驚覺這些機構的權力有多大——以及當評等機制出問題時，後果有多嚴重。

LMArena 正在成為 AI 界的信評機構。這不一定是壞事，產業確實需要一個可信的基準。但隨著影響力增加，治理機制也必須跟上。Cohere 的論文雖然有數據錯誤，但它提出的核心問題是真實的：當評測平台變得如此重要，它的公平性該由誰來監督？

### 開源的雙面刃

LMArena 做了一件很少平台願意做的事：他們開源了超過 250 萬筆真實用戶對話數據，讓整個社群可以拿來研究、訓練、改進模型。這是建立公信力的重要基礎——「我們沒有藏私，數據都在這裡，你可以自己驗證。」

但這種開放也帶來新的問題。《排行榜幻覺》論文指出，模型開發者可以拿這些公開數據來「過度擬合」Arena——專門針對 Arena 用戶常問的問題類型做優化，在 Arena 上拿高分，但換到其他場景就不行了。這就像學生專門背考古題，考試分數很高，但真正的能力沒有提升。

更微妙的是預覽測試機制。LMArena 允許模型開發者在正式發布前先進行匿名測試，這對社群來說是有趣的「尋寶遊戲」，對開發者來說是寶貴的真實用戶反饋。但問題是：只有資源豐富的大廠才能同時測試幾十個變體、挑最好的發布。小團隊和開源社群沒有這種餘裕，結構性的不平等就這樣產生了。

LMArena 的回應是：這不是秘密政策，規則對所有人一樣，大廠測試比較多只是因為他們開發的模型比較多。這話沒錯，但也沒有真正解決問題。開放帶來透明，透明卻不一定帶來公平。

### Goodhart 定律的活教材

經濟學有一條著名的 Goodhart 定律：「當一個指標變成目標，它就不再是好指標。」LMArena 的處境，可能是這條定律在 AI 時代最鮮明的案例。

一開始，LMArena 排行榜只是一個觀察工具，讓大家知道不同模型的相對表現。但當它變成產業標準，當投資人、客戶、媒體都用這個分數來評判模型的價值，開發者的行為就會改變。他們不只是要做出好模型，更要做出「在 Arena 上表現好」的模型。

這不是作弊，但確實扭曲了競爭。大廠可以投入資源做多輪測試、分析 Arena 用戶的 prompt 分布、針對性優化，最後只發布分數最高的版本。小團隊沒有這種資源，只能一次定生死。表面上規則公平，實際上起跑點不同。

Angelopoulos 顯然意識到這個問題，所以他們不斷強調要「保持基準的新鮮度」——持續有新用戶、新問題、新使用場景進來，讓模型沒辦法只靠背考古題過關。這是對的方向，但能不能真正解決問題，還需要時間驗證。

---

LMArena 的故事還在繼續。從柏克萊地下室到 1 億美元募資，從學術專案到產業標準制定者，他們走過的路徑，可能會成為 AI 基礎設施公司的經典案例。但隨著影響力增加，挑戰也會越來越大。如何在商業成功與平台誠信之間取得平衡、如何在開放透明與公平競爭之間找到答案，這些問題沒有標準解法。

唯一確定的是：在 AI 軍備競賽的時代，誰掌握了評測標準，誰就掌握了話語權。而 LMArena，正站在這個權力的中心。
