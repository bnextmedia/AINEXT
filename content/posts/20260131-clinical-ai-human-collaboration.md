---
title: "醫生加 AI 不一定比 AI 單獨強：臨床人機協作的殘酷真相"
date: 2026-01-31T11:00:00+08:00
description: "2026 臨床 AI 報告揭示一個違反直覺的事實：在多項隨機對照試驗中，醫師加上 AI 的表現經常只是持平甚至略遜於 AI 單獨運作。報告深入探討自動化偏見、技能退化、以及為什麼工作流程設計比模型能力更重要。"
tags: ["臨床 AI", "人機協作", "自動化偏見", "ARISE", "醫療 AI"]
categories: ["AI 技術前沿"]
source_url: "https://arise-ai.org/report"
source_name: "ARISE Network"
image: "/images/posts/20260131-clinical-ai-human-collaboration.jpg"
draft: false
---

> 本文整理自 ARISE Network 於 2026 年 1 月發布的 *State of Clinical AI Report 2026*，聚焦「AI in Clinical Workflows」章節的核心發現。

---

在大多數關於 AI 的討論中，有一個預設幾乎沒有人質疑：AI 最理想的角色是輔助人類，讓人類做出更好的決策。這個「人類在迴路中」（human-in-the-loop）的敘事既符合直覺，也政治正確。史丹佛與哈佛團隊 ARISE Network 在 2026 年 1 月發布的臨床 AI 報告，卻用一系列隨機對照試驗的數據，對這個美好敘事潑了一盆冷水。

報告的結論很不客氣：在多個臨床情境中，醫師加上 AI 的表現確實優於醫師獨自作業，但經常只是持平甚至略遜於 AI 單獨運作。這個發現挑戰了資訊學的一個基本定理，也迫使我們重新思考人機協作的本質。

## 數據說了什麼

故事要從一項 92 名醫師參與的隨機對照試驗說起。這項由 ARISE 執行總監 Ethan Goh 和史丹佛的 Jonathan H. Chen 等人主持的研究，讓醫師在有和沒有 GPT-4 輔助的情況下處理真實去識別化的病患案例，並用預先設定的專家評分標準來評量管理推理能力。使用 GPT-4 的醫師得分確實比只用傳統資源的醫師高出 7%，聽起來是正面結果。但當研究者把醫師加 GPT-4 的成績（43%）與 GPT-4 獨自作答的成績（44%）放在一起時，差距幾乎不存在。醫師花了更多時間，每個案例多了大約兩分鐘，卻沒有為 AI 的判斷增添什麼價值。

後來，另一組研究者針對這個問題做了一次更精心設計的嘗試。他們建了一個客製化的協作式 GPT-4 系統，讓 70 名醫師在三種條件下處理診斷案例：AI 作為第一意見、AI 作為第二意見、或只用傳統資源。系統的關鍵設計是，AI 會產生自己的鑑別診斷和下一步建議，然後生成一個「聯合綜合視圖」，整合雙方的觀點。結果確實樂觀了一些：診斷準確率從只用傳統資源的 75% 提升到 AI 為第二意見的 82% 和 AI 為第一意見的 85%。AI 還拉高了表現底線，減少了低分案例的數量。然而，即便經過這番精心設計，人機團隊依然沒有超越 AI 獨自表現。

一項涵蓋 52 項研究、87 種協作條件的統合分析，從更宏觀的角度印證了這個模式。人機團隊（Human-Machine Teaming, HMT）的診斷可靠度平均高於人類單獨作業，但很少達到真正的互補效果，也就是 1+1=2 的理想狀態。在少數案例中，AI 的介入甚至降低了人類的表現。同步協作（醫師和 AI 同時工作）的效果優於序列協作（先人後 AI 或先 AI 後人），而資歷較淺的醫師從 AI 輔助中獲得的提升明顯大於資深醫師。

## 自動化偏見：訓練過的醫師也逃不掉

如果人機協作的效益已經讓人失望，接下來的發現更令人警惕。

一項在巴基斯坦進行的實驗分兩個階段。第一階段，58 名醫師先接受了 20 小時的 AI 素養培訓，涵蓋 AI 的能力、局限性和適當使用方式，然後隨機分配到使用 GPT-4o 加傳統資源，或只用傳統資源。結果令人鼓舞：AI 輔助組的診斷推理分數從 43% 躍升到 71%，而且在 38% 的案例中，醫師加 AI 的表現甚至超越了 GPT-4o 獨自作答（83%），展現了真正的互補潛力。年輕醫師和平時較少使用 AI 的醫師獲益最大。

但第二階段的實驗徹底改變了故事的調性。研究者讓 44 名同樣受過 AI 培訓的醫師處理 6 個臨床案例，其中一半人接觸的 GPT-4o 建議中，有 3 個案例被刻意植入了錯誤。這些錯誤設計得可以被發現，但不會一眼看出。結果，接觸到錯誤建議的醫師，診斷準確率暴跌至 73%，而對照組（接收正確建議者）為 85%。首選診斷的準確率差距更大：76% 對 91%。兩組醫師查閱 GPT-4o 的頻率幾乎相同（69% 對 67%），這表示即便是經過訓練、知道 AI 可能出錯的醫師，在實際面對 AI 建議時仍然傾向於照單全收。

這就是自動化偏見（automation bias）的威力。它不是因為醫師偷懶或不懂 AI，而是一種深層的認知捷徑：當一個看似權威的系統給出了一個聽起來合理的答案，人類大腦會自動降低批判性思考的強度。20 小時的培訓改變了知識，卻沒有改變這個根深蒂固的認知傾向。

## 技能退化：用久了 AI，不用時反而更差

如果自動化偏見是「使用 AI 時」的問題，技能退化（deskilling）則是「停止使用 AI 後」的問題，而且它可能更難解決。

一項多中心觀察研究追蹤了在大腸鏡 AI 輔助檢查試驗中，經驗豐富的內視鏡醫師在有無 AI 輔助下的表現差異。這些醫師長期使用 AI 來偵測息肉，然後研究者比較了他們在 AI 導入前三個月和導入後三個月，進行「非 AI 輔助」大腸鏡檢查時的腺瘤偵測率（ADR）。結果，ADR 從 28.4% 下降到 22.4%，下降幅度達 6 個百分點。校正性別、年齡、中心等變項後，AI 暴露仍與較差的偵測表現顯著相關（勝算比 0.69）。

這個發現的含義非常深遠。大腸鏡檢查中的腺瘤偵測率被視為結腸癌預防的關鍵指標，每提升 1%，間隔期結腸癌的風險就降低約 3%。如果 AI 輔助讓醫師的「裸眼」能力退步了 6 個百分點，那麼一旦 AI 系統當機、更新、或因為某些原因無法使用，病患可能面臨比 AI 導入前更高的風險。這不是理論上的擔憂，而是已經在數據中觀察到的現象。

## 肯亞的故事：AI 作為安全網，而非替代品

在一片令人憂心的發現中，報告也呈現了一個亮點。肯亞的 Penda Health 與 OpenAI 合作開發的 AI Consult 系統，是目前最大規模的前瞻性臨床 AI 部署研究之一。在 15 個診所、近 4 萬次門診中，基於 GPT-4o 的 AI Consult 作為一個安全網在背景運行，在看診的關鍵節點提供文件、檢查、診斷和治療的輔助，並用紅黃綠三色燈號提示風險等級。

結果相當明確：使用 AI 的組別在病史採集的錯誤率降低了 32%，檢查錯誤降低 10%，診斷錯誤降低 16%，治療錯誤降低 13%。以診斷錯誤為例，需要治療人數（NNT）為 18.1，意味著每 18 次看診使用 AI，就能避免一次診斷錯誤。如果擴展到 Penda Health 每年 40 萬次看診，這相當於每年減少約 22,000 次診斷錯誤和 29,000 次治療錯誤。

但這個案例最有啟示意義的發現不在這些數字裡，而在一條趨勢線上。在研究期間，使用 AI 的醫師在看診「起始紅燈」的比率，從 45% 逐漸降到 35%。這意味著醫師不只是在 AI 的提醒下修正錯誤，而是隨著時間推移，學會了在 AI 介入之前就避開常見的陷阱。AI 不只是安全網，還成了一種持續教育的工具。

這個設計與前述那些令人失望的實驗有一個關鍵差異：AI Consult 不是在「取代醫師的判斷」，而是在「提供一個獨立的第二檢查點」。醫師仍然做自己的決策，AI 只在判斷有疑慮時才亮燈。這種「守門員」模式避免了自動化偏見的主要觸發機制，因為醫師不需要在 AI 的建議和自己的判斷之間做選擇。

## AI 抄寫員：主觀感受很好，客觀數據很薄

報告也檢視了目前醫療現場滲透率最高的 AI 應用之一：環境 AI 語音抄寫員（Ambient AI Scribe）。在一項 272 名醫師參與的多中心前後對照分析中，使用 AI 抄寫員 30 天後，醫師自評倦怠比率從 52% 顯著下降到 39%。認知負荷、對病患的注意力、下班後的文件作業時間等指標也都有所改善。

然而，當兩項隨機對照試驗嘗試客觀測量時間節省時，結果卻令人意外地平淡。研究測試了 Abridge、Microsoft DAX Copilot 和 Nabla 三套系統，透過 Epic 電子病歷系統的 Signal 數據追蹤實際文件作業時間，發現每份筆記大約只省下 20 秒左右。主觀上的巨大改善與客觀上的微小時間節省之間，存在明顯落差。

報告認為，這並不代表 AI 抄寫員沒有價值。醫師感受到的減壓是真實的，可能來自認知負荷的降低而非絕對時間的節省。但如果要從 AI 抄寫員中榨出更大的生產力提升，下一步應該是把功能從「轉錄」擴展到「下游工作流程」，例如自動處理醫囑、溝通、和行政任務。

## 工作流程設計才是下一個前沿

讀完報告中所有關於人機協作的研究後，一個結論浮現出來：下一個突破口不在更強的模型，而在更聰明的工作流程設計。

目前的問題不是 AI 不夠強，而是人類和 AI 之間的介面設計還很原始。醫師拿到一個 AI 建議，然後必須決定採納或拒絕。這種二元選擇的設計幾乎注定會觸發自動化偏見，因為拒絕一個看起來合理的建議需要消耗認知資源，而臨床現場的醫師認知資源本就捉襟見肘。

Penda Health 的紅黃綠燈號模式、協作式 GPT-4 的「聯合綜合視圖」、以及 52 篇統合分析中「同步協作優於序列協作」的發現，都指向同一個方向：最有效的人機協作不是讓 AI 給建議、人類做決定，而是讓雙方的思考過程在一開始就交織在一起，讓醫師和 AI 犯不同類型的錯誤，然後設計一個系統來抓住這些互補的盲點。

報告的 52 項研究統合分析有一個特別值得注意的結論：人機協作獲益最大的情境，是醫師和模型犯互補性錯誤的時候。換句話說，關鍵不在於讓 AI 更準確，而在於確保 AI 和醫師「錯在不同的地方」。這要求的不只是更好的模型，更是對臨床認知模式和 AI 失敗模式的深入理解，然後據此設計出能夠最大化互補效果的協作流程。

對於所有在思考「如何把 AI 導入專業決策流程」的產業來說，這些發現都有直接的參考價值。醫療是一個極端案例，因為決策錯誤的代價是人命，但底層的認知動態放諸四海皆準：人類在面對看似權威的 AI 建議時，會不自覺地降低批判標準；長期依賴 AI 可能侵蝕原有的判斷能力；而工作流程的設計，最終決定了人機協作是加分還是扣分。
