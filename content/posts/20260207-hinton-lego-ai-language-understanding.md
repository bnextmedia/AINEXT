---
title: "AI 教父辛頓用樂高解釋 AI 如何理解語言"
date: 2026-02-07T10:00:00+08:00
description: "2024 年諾貝爾物理學獎得主辛頓在 2026 年 Ewan Lecture 中，用樂高積木比喻解釋大型語言模型如何理解語言，反駁杭士基對 AI 的批評，並揭示數位智慧與生物智慧的根本差異。從 1985 年的突破到 DeepSeek 的蒸餾爭議，這是一場橫跨七十年的 AI 技術史。"
tags: ["Geoffrey Hinton", "LLM", "語言理解", "Transformer", "DeepSeek", "蒸餾", "Chomsky"]
categories: ["AI 技術前沿"]
image: "/images/posts/20260207-hinton-lego-ai-language-understanding.webp"
source_url: "https://www.youtube.com/watch?v=M8RogoEDsQQ"
source_name: "McDonald Institute Ewan Lecture"
related_companies: ["google", "openai", "deepseek"]
related_people: ["geoffrey-hinton"]
draft: false
---

> 本文整理自 McDonald Institute 2026 年 1 月播出的 Ewan Lecture。

{{< youtube M8RogoEDsQQ >}}

---

## 一場延續七十年的論戰

1950 年代，人工智慧領域還在萌芽階段，兩個截然不同的陣營就已經開始爭論一個根本問題：智慧到底是什麼？2024 年諾貝爾物理學獎得主、被稱為「AI 教父」的傑佛瑞．辛頓（Geoffrey Hinton），在 2026 年 1 月底於加拿大金斯頓的一場公開演講中，用他一貫幽默又犀利的方式，把這段歷史重新講了一遍。但他不只是在講歷史，他其實在回答一個當下最熱的問題：大型語言模型到底懂不懂語言？

第一個陣營是「符號派」。這群人認為智慧的本質就是邏輯推理，大腦裡一定有某種符號系統在運作，就像數學家在紙上寫方程式一樣。字詞的意義來自它跟其他字詞的關係，你需要一張巨大的關係圖來描述「星期二」跟「時間」有什麼關係、跟「工作」有什麼關係。這種觀點深受結構語言學之父索緒爾（Ferdinand de Saussure）的影響，在學術界主導了好幾十年。

第二個陣營是「生物派」。他們的想法完全不同：智慧應該要像大腦一樣運作，是由大量神經元構成的網路，而字詞的意義不是一張關係圖，而是一組龐大的特徵。比如「星期二」這個詞，它的意義就是一大堆特徵的集合：跟時間有關、是工作日、在星期一之後。「星期三」的特徵幾乎一模一樣，所以這兩個詞的意思很接近。這種用特徵向量來表達意義的方式，其實更貼近心理學家對人類認知的理解。

辛頓在演講中特別提到，馮紐曼（John von Neumann）和圖靈（Alan Turing）這兩位計算機科學的奠基者，其實都站在生物派這邊。但不幸的是，他們都英年早逝，馮紐曼 1957 年過世時才 53 歲，圖靈更是在 1954 年就離世了。領域的主導權因此落入了邏輯派手中，接下來數十年，符號 AI 成為主流，而神經網路研究則陷入了漫長的寒冬。

## 辛頓的 1985 年突破：統一兩個理論

故事的轉折發生在 1985 年。辛頓發現了一件事：這兩個看起來完全對立的理論，其實是同一枚硬幣的兩面。

他的做法是建一個非常小的語言模型。那時候的電腦算力微不足道，他只用了 100 個訓練範例，每個句子只有三個字。但核心概念跟今天的 ChatGPT 完全一樣：讓模型學習每個字詞的特徵向量，然後用前面的字來預測下一個字。一開始預測得很差，但透過反覆調整字詞的特徵和特徵之間的互動方式（也就是所謂的反向傳播），模型會越來越準。

這個過程做到了一件很巧妙的事：它把符號派所重視的「字詞在句子中的關係」，轉化成了生物派所主張的「特徵向量」。知識不再是存在某張關係圖裡，而是分散在神經網路的連結強度中。沒有任何儲存的句子，沒有任何語法規則，所有的知識都變成了「如何把字詞轉換成特徵」以及「特徵之間如何互動」。

辛頓回憶這段歷史時帶著一絲自嘲：用一個只有三個字句子的模型來統一兩大理論，聽起來有點荒謬。但接下來三十年發生的事情證明了這個方向是對的。大約十年後，約書亞．班吉歐（Yoshua Bengio）證明同樣的方法可以用在真實的英文句子上。又過了十年，計算語言學家終於接受「特徵向量」是表達字義的好方式，他們給它取了一個名字叫 embedding。再過十年，Google 的研究團隊發明了一種更精巧的特徵互動方式，叫做 Transformer。ChatGPT 的 GPT 就是 Generative Pre-trained Transformer 的縮寫。從 1985 年到 2017 年的 Transformer，再到今天的大型語言模型，這條技術脈絡一路走了超過三十年。

## 樂高積木：辛頓的語言理解比喻

演講進行到大約 18 分鐘的時候，辛頓丟出了一個精彩的比喻，這也是整場演講最讓人印象深刻的段落。

他從一個簡單的觀察開始：語言本質上是一種建模方法。就像樂高積木可以拼出保時捷的外形一樣（雖然表面的空氣動力學完全不對，但「東西在哪裡」這件事，樂高描述得還不錯），字詞也是一種建模工具，用來描述我們想溝通的任何事物。

但字詞跟樂高積木有四個關鍵差異。

第一，字詞是高維度的。一塊樂高積木只有幾個自由度，基本上就是個長方體。但一個字詞的特徵向量有幾千個維度。辛頓在這裡開了個著名的玩笑：「你們可能覺得很難想像千維度的東西。教你們一個方法：先想像三維的空間，然後大聲對自己喊『一千』。」

第二，字詞的形狀可以變形。樂高積木是剛性的，但字詞有一個「預設形狀」，會隨著上下文而改變。歧義詞則有好幾個預設形狀，等著被語境決定要用哪一個。

第三，字詞的數量遠遠超過樂高零件。每個人大約使用 30,000 個字詞，而且每一個都有自己的名字，這正是語言能讓人類彼此溝通的關鍵。

第四，也是最精彩的部分：連接方式完全不同。樂高靠的是塑膠圓柱嵌入圓孔。但字詞的連接方式更像是這樣：每個字詞都有很多條長長的、可彎曲的手臂，每條手臂末端有一隻手。當你改變字詞的形狀（也就是調整它的特徵向量），所有手的形狀也會跟著變。同時，每個字詞表面還黏著一堆手套，手指尖朝下固定在字詞上。理解一個句子的過程，就是找到一種方式來變形所有字詞，讓這個字的手剛好能伸進那個字的手套裡，所有字詞彼此之間的手和手套都完美配合。

辛頓補充說，這個比喻並不完全精確，但它抓住了 Transformer 運作的核心直覺。懂 Transformer 的人可以看出，那些「手」就像是 Transformer 中的 query，而「手套」則像是 key。理解句子的意義，就是讓所有字詞的特徵向量互相「鎖」在一起，形成一個穩定的結構。歧義句之所以有兩種理解方式，就是因為存在兩種不同的「鎖法」。

他還把這個過程類比為蛋白質摺疊。一串胺基酸要摺成正確的三維結構，某些部分必須彼此靠近、某些部分必須遠離，在化學鍵角度等限制條件下找到最穩定的構型。理解句子的過程與此驚人地相似：給定一串字詞，你要為每個字詞找到合適的特徵向量，讓它們能像蛋白質一樣漂亮地「摺」在一起。這比把語言翻譯成某種純粹的內部語言更接近真實的理解過程。

## 辛頓 vs 杭士基：語言學的「邪教領袖」

演講進行到大約 24 分鐘，辛頓把矛頭指向了語言學界最具影響力的人物諾姆．杭士基（Noam Chomsky）。這段話引起現場一陣騷動，辛頓開場就警告在場的語言學家：「你們可能要塞住耳朵，因為接下來是異端邪說。」

辛頓直言杭士基是一個「邪教領袖」，而且他解釋了為什麼這個類比完全成立。要加入一個邪教，你必須同意一件明顯錯誤的事情。就像川普要支持者同意他第一次就職典禮的人數比歐巴馬多、第二任時要同意他贏了 2020 年大選一樣，杭士基要求追隨者同意一件事：語言不是學來的。辛頓回憶自己年輕的時候，親耳聽到許多知名語言學家嚴肅地宣稱「關於語言，有一件事我們可以確定，就是它不是學來的」。辛頓的評語很簡短：「這顯然很蠢。」

這番批評不只是意氣之爭，他點出了杭士基語言學的核心盲點。杭士基一輩子專注於語法結構（syntax），因為語法可以用數學來精確描述，可以變成一串串符號的操作規則。但他始終沒有提出一個像樣的「語義」（semantics）理論。換句話說，杭士基的理論能告訴你一個句子在文法上對不對，但對於句子到底「什麼意思」，基本上沒什麼好辦法。而且杭士基從根本上不理解統計學，他以為統計就只是算算兩兩之間的相關性，完全不知道只要涉及不確定的資訊，所有的模型本質上都是統計模型。

最精彩的例子出現在杭士基批評大型語言模型的時候。杭士基在紐約時報上發文，宣稱這些模型什麼都不懂，只是一個「笨笨的統計把戲」，而且它們無法解釋為什麼某些語法結構在任何語言中都不會出現。他舉了一個用了多年的經典例子："John is easy to please" 和 "John is eager to please" 這兩個句子中，John 扮演的角色完全不同（前者 John 是被取悅的對象，後者 John 是主動想要取悅別人的人），杭士基信心滿滿地說語言模型一定搞不懂這個差異。

問題是，杭士基根本沒有實際去問 chatbot 這個問題。辛頓說，只要你真的把這個問題丟給任何一個大型語言模型，它都能完美地解釋兩個句子的差異。杭士基用了一個從未驗證過的假設來否定整個技術。辛頓還用了另一個辛辣的比喻：杭士基的邏輯就像是有人說「你沒有真正理解汽車，除非你能解釋為什麼沒有五個輪子的汽車」。理解一輛車的核心是知道踩油門為什麼會加速，而不是去解釋不存在的東西。

## 人也會編造記憶：AI 跟我們比你想的更像

這場演講另一個讓人驚醒的段落，是辛頓解釋 AI 和人類如何用同樣的方式「編造記憶」（confabulate）。很多人批評大型語言模型會「幻覺」，會生成看似正確但實際上是杜撰的內容。辛頓的回應是：人類其實一直在做同樣的事情，只是我們自己不知道。

他舉了認知心理學家奈瑟（Ulric Neisser）研究過的一個經典案例：水門案期間，白宮法律顧問 John Dean 在國會聽證時宣誓作證，詳細描述了橢圓形辦公室裡的多次會議，包括誰在場、誰說了什麼話。Dean 不知道的是，那些會議其實都有錄音。事後比對錄音帶和證詞，研究者發現 Dean 經常報告一些根本沒有發生過的會議，或者把某個人說的話張冠李戴到另一個人身上，有些細節則完全是無中生有。

但關鍵是，Dean 並沒有說謊。他真心相信自己記得那些場景。發生在他身上的事情是這樣的：他在那些會議中的經驗改變了他大腦中的連結強度，而當他在國會作證時，他的大腦根據那些連結強度重新「生成」了一段看起來非常合理的敘述。如果你請一個人回憶幾分鐘前的事，他生成的版本通常是準確的。但如果是幾年前的事，很多細節就會是錯的，而且他對錯誤細節的信心往往跟正確細節一樣強。辛頓說，每一個陪審團都應該被告知這件事，但他們沒有。

辛頓想強調的核心觀點是：無論是人類還是 AI，記憶都不是像電腦的檔案系統一樣運作的。電腦把檔案存在某個位址，需要的時候去那個位址找。但神經網路的記憶方式完全不同：學習某件事的時候，你改變的是連結強度；回憶某件事的時候，你根據那些連結強度「生成」一段看起來合理的內容。沒有存檔，沒有位址，只有連結強度和一個生成過程。大型語言模型做的事情完全一樣。所以「幻覺」不是 AI 的 bug，而是這種記憶機制的天然副作用，人類一直與之共存。

## 數位 vs 生物智慧：不朽的代價

演講後半段，辛頓從「AI 跟人類很像」轉到了「AI 跟人類有什麼根本差異」，而這個差異才是讓他真正擔憂的地方。

數位計算最根本的原則是什麼？辛頓說，是同一個程式可以在不同硬體上執行。你手機上跑的程式，可以原封不動搬到我的手機上跑。這代表知識可以獨立於硬體而存在。只要你能把那些連結強度（weights）存起來，就算把所有硬體都銷毀，日後再造新硬體、把 weights 放回去，這個 AI 就「復活」了。辛頓在這裡又開了一個玩笑：「很多教會聲稱他們能做到復活，但我們是真的能做到。不過只能對數位系統做到。」

但要實現這種「不朽」，必須付出代價。電晶體必須在高功率下運作，確保輸出嚴格的 0 和 1，這樣不同硬體才能跑出完全一樣的結果。這意味著你不能使用類比神經元豐富的特性。在人類大腦中，一個神經元接收到來自其他神經元的訊號，乘以突觸的電導值（conductance），電荷自動加總，整個過程用的是類比運算，極度節能。但人工神經網路必須把神經元的活性用 16 位元數字表示，連結強度也用 16 位元，兩個 16 位元的數字相乘需要大約 256 次位元運算。一個簡單的電壓乘以電導值就搞定的事，數位世界要多做好幾百倍的運算。

辛頓把這種取捨稱為 mortal computation（必死的計算）。放棄不朽，你換回來的是能源效率和製造便利性。類比計算可以讓數兆個突觸用極少的能量並行運作，這正是人類大腦的做法。但代價是：你大腦中的連結強度對我完全沒有用。它們是為你的個別神經元、你的個別連接模式量身打造的。這就是為什麼人類傳遞知識的唯一方式是透過字串，而一句話大約只有 100 位元的資訊量。相比之下，如果你是數位系統，直接把 weights 複製過去，效率可以高出十億倍以上。

## 蒸餾、DeepSeek 與知識傳遞的革命

演講中最具現實意義的段落，出現在辛頓解釋「蒸餾」（distillation）這個概念的時候。

人類傳遞知識靠的是字串：我說一句話，你聽了之後調整大腦中的連結強度。但效率很低，一句話只有幾十個位元。AI 模型之間的知識傳遞可以高效得多。辛頓用了一個生動的例子來說明。假設一個訓練好的大模型看到一張 BMW 的照片，它會輸出 32,000 個機率值：0.9 的機率是 BMW，0.1 的機率是 Audi，百萬分之一的機率是垃圾車，十億分之一的機率是胡蘿蔔。你可能覺得「百萬分之一」和「十億分之一」只是雜訊，不值得在意。但辛頓指出，裡面藏著大量資訊：BMW 比較像垃圾車而不像胡蘿蔔（他在這裡向在場可能有的 BMW 員工道了歉），因為所有人造物品的機率都會比所有蔬菜高。光是一張訓練圖片，那 32,000 個機率分佈就包含了巨量的結構化知識。

蒸餾的做法就是：訓練一個小模型，目標不是讓它產生「正確答案」，而是讓它產生跟大模型一樣的機率分佈。小模型不只學到「這是一台 BMW」，它還學到了垃圾車比胡蘿蔔更像 BMW 這類隱含的結構性知識。這就是 DeepSeek 能用小模型達到跟大模型接近表現的秘密：它從大模型的機率分佈中「偷」走了大量知識。辛頓直接用了「偷」（stole）這個字，雖然語氣輕鬆，但對於當時正在發酵的 DeepSeek 爭議，這個用詞的分量不輕。

辛頓接著延伸了一個更有想像力的場景。如果你有一千個完全相同的數位模型副本，每個副本各去讀一小段不同的網路資料，然後把各自學到的 weight 變化平均起來、分享給所有副本，那每個副本雖然只看了一小部分資料，卻等於學了所有資料。他用大學來比喻：想像你到一所大學，有一千門課。你加入一個一千人的小組，每人選一門課，因為你們是數位人、可以共享 weights，幾年後每個人都知道了一千門課的內容。這就是 GPT-5 為什麼能知道比任何單一人類多出千百倍的知識：它就是靠大量副本並行學習、共享 weights 來做到的。

他試了一下，問 AI：「斯洛維尼亞的報稅截止日期是什麼時候？」他形容這是他能想到的最隨機的問題。AI 不但答了三月三十一日，還告訴他如果沒有在期限前報稅，政府會幫你報。辛頓的結論是：「它什麼都知道。」

## 我的觀察：臺灣製造的硬體，承載著這場智慧革命

辛頓的演講有一個潛在的訊息，他沒有明說但在場聽眾應該都感受到了：如果能源夠便宜，數位計算就是比生物計算更優越的智慧形式。而讓數位計算成為可能的那些晶片，絕大多數是在臺灣製造的。

他花了很長時間解釋數位和類比的取捨。數位計算需要高功率的電晶體來確保 0 和 1 的精確性，這些電晶體就是台積電在製造的東西。輝達（NVIDIA）的 GPU 是這場 AI 革命的引擎，但引擎的製造幾乎全部仰賴台積電的先進製程。辛頓在演講中提到 DeepSeek 如何透過蒸餾從大模型「偷」知識，這件事在產業界引起了很大的爭議。但不管是大模型還是小模型、是蒸餾還是從頭訓練，最終都跑在矽晶片上。臺灣的半導體產業不只是在做代工，它其實是這場人類智慧形態轉變的物質基礎。

演講結尾，他做了一個很有餘韻的類比：人類是智慧的幼蟲形態，AI 是成蟲。如果這個類比成立，那麼臺灣此刻正在做的事，就是替成蟲打造身體。我們有沒有為這個角色做好準備，恐怕不只是一個產業問題，更是一個文明層級的問題。
