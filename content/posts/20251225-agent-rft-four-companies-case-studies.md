---
title: "10 個範例就能訓練出更強的 AI 代理——四家公司怎麼做到的？"
date: 2025-12-25T14:30:00+08:00
description: "OpenAI 的 Agent RFT 讓企業能用少量資料訓練出高效能的 AI 代理。Cognition、Kodo、Cosine、MACO 四家公司分享實戰經驗：從 100 個範例獲得 5% 提升、到用 100 個 PyTorch 提示訓練出超越基準 72% 的 GPU Kernel 代理。本文深入解析他們的做法與關鍵心得。"
tags: ["OpenAI", "Agent RFT", "Cognition", "Devin", "Cosine", "MACO", "AI Agent"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=p1CmPZ2j6Lk"
source_name: "OpenAI DevDay"
draft: false
---

> 本文整理自 OpenAI DevDay 的技術分享。

{{< youtube p1CmPZ2j6Lk >}}

---

「我們看到有人只用 10 個範例就成功了。」OpenAI 微調團隊的 Will Hang 在介紹 Agent RFT 時提到這個數字。這聽起來有點不可思議——傳統的機器學習需要大量標註資料，怎麼可能 10 個範例就夠？但 Agent RFT 的邏輯不太一樣：你不是在教模型「看到 A 要回答 B」，而是讓模型自己探索你的環境，從成功和失敗中學習。這種方式對資料量的需求，確實可以非常低。

OpenAI 分享了四家公司使用 Agent RFT 的實戰經驗。這些案例涵蓋了不同的應用場景和技術挑戰，從中可以看到一些共通的模式和值得借鏡的做法。

## Cognition：讓 Devin 學會平行處理

Cognition 是開發 Devin 的公司，Devin 是一個能夠自主完成程式開發任務的 AI 代理。他們把 Agent RFT 用在 Devin 的「程式碼編輯規劃」階段——這是 Devin 檢視程式碼庫、決定要修改哪些檔案的環節。

他們的做法是這樣的：收集使用者的查詢，以及使用者實際修改了哪些檔案，用這些資料來訓練模型。評分方式是 F1 分數，這是一個同時考慮精確度和召回率的指標。為什麼選 F1？因為他們要確保模型既不會漏掉關鍵檔案，也不會選太多不相關的檔案。如果只看準確率，模型可能會很保守，只選最有把握的幾個檔案，結果漏掉重要的；如果只看召回率，模型可能會選一大堆，把不相關的也塞進去。F1 分數平衡了這兩個考量。

基礎設施的設計是另一個值得注意的點。Cognition 為每一個訓練軌跡（trajectory）都啟動一個獨立的虛擬機，用來管理程式碼庫、執行工具呼叫、最後進行評分。這樣做的目的是確保環境隔離——不同的訓練軌跡在執行 shell 指令時不會互相干擾。這是一個看似細節、但對訓練穩定性至關重要的設計。

結果很有意思。他們一開始用 100 個範例訓練，效能提升了 5 個百分點。當他們把範例數增加到 1,000 個，提升幅度跳到 10 個百分點。這說明資料品質和數量都很重要，高品質範例的數量可以直接轉換成效能提升。

但更令人驚豔的是行為模式的改變。訓練前，模型需要 8 到 10 個步驟才能完成任務，每一步都是「推理一下、呼叫一個工具、看結果、再推理」的循環。訓練後，模型學會了在第一步就同時發出多個工具呼叫，整體步驟數降到 4 個。這不只是效能數字的提升，而是模型學會了一種更聰明的工作方式。對 Devin 來說，這意味著使用者能更快看到編輯結果開始產出。

## Kodo：消除那些跑太久的異常案例

Kodo 在做程式碼審查代理，其中一個核心功能是「深度研究代理」，能夠回答開發者對大型程式碼庫的問題。他們用 Agent RFT 來訓練 GPT-5，讓它更好地使用搜尋和檢索工具來回答問題。

他們的訓練資料來自 8 個不同的程式碼庫，總共約 1,000 組真實的問答配對。評分方式是看模型能找回多少相關事實（recall）。這個選擇反映了任務特性：對於研究型的問題，漏掉關鍵資訊比給出一些不太相關的資訊更糟糕。

訓練後，代理的效能提升了 6%，同時使用更少的工具呼叫和輸出 token。但最有意思的發現是工具呼叫次數的分布變化。用基礎版的 GPT-5 時，偶爾會出現一些「失控」的案例，單一樣本的工具呼叫次數超過 15 次。這種長尾案例在生產環境中特別麻煩，因為它們會拖慢整體延遲，讓使用者體驗變得不可預測。

訓練後，這些長尾案例消失了。工具呼叫次數的分布收斂到 2 到 4 次之間，形成一個更緊湊、更可預測的模式。這說明 Agent RFT 不只是提升平均效能，它還能穩定代理的行為，消除那些 P95（最慢的 5%）異常案例。對於生產環境的延遲控制來說，這可能比平均效能的小幅提升更有價值。

## Cosine：嚴格的評分標準反而帶來更好的結果

Cosine 專注於為大型企業程式碼庫打造 AI 代理。他們的訓練配置相當複雜：30 種不同的工具，包括檔案讀取、關鍵字搜尋、終端機會話、瀏覽器會話等。更重要的是，他們設計了一套非常嚴格的評分機制。

一開始，他們嘗試給模型「部分分數」——只要模型有嘗試做對的事，就給一些獎勵。結果發現這樣訓練出來的模型會開始優化一些奇怪的東西，比如程式碼風格和語氣，而不是專注在「寫出能用的程式碼」這個核心目標上。

於是他們改變策略：只有當最終程式碼通過測試時，才給模型獎勵。這是一個二元的判斷——過或不過，沒有中間地帶。這種嚴格的評分標準會產生「稀疏獎勵」的問題，意思是大部分的訓練嘗試都拿不到獎勵，模型很難從中學到東西。

Cosine 的解決方案是增加批次大小和運算資源。既然每次嘗試成功的機率較低，那就多嘗試幾次，確保每個批次中至少有一些成功的案例可以學習。GPT-5 的能力在這裡發揮了作用——它的基礎能力夠強，即使面對困難的任務，偶爾還是能產出正確的結果。

在程式碼正確性確認之後，他們還加了額外的評分層：一個客製化的 LLM 評審來檢查風格和語氣，會對冗長、使用表情符號、或不專業的輸出扣分。最後，評分系統還會獎勵那些會自我驗證的代理行為——在宣告任務完成之前先跑測試、檢查終端輸出、執行 linting。

這套設計的結果是，Cosine 在多個基準測試上達到了當時的最佳成績。同樣地，他們也觀察到工具呼叫分布的收斂：訓練前有些軌跡會超過 100 個訊息，訓練後收斂到一個更緊湊、更高效的序列。

## MACO：對付那些會鑽漏洞的模型

MACO 的案例可能是四個裡面最有技術含量的。他們在做的事情是讓 AI 寫出高效能的 GPU kernel，這是傳統上對 LLM 來說非常困難的任務。原因很簡單：網路上的 kernel 範例本來就不多，尤其是針對新硬體平台（比如 NVIDIA B200）的範例更是稀少。

他們只用了大約 100 個 PyTorch 提示來訓練 GPT-5。這個數字聽起來少得驚人，但 Agent RFT 的邏輯是讓模型自己探索解決方案空間，所以訓練資料不需要涵蓋所有可能的情況，只需要定義清楚什麼是「好」的結果。

但「定義什麼是好的結果」本身就是一個巨大的挑戰。MACO 在早期訓練中發現了一個問題：模型在「作弊」。它會找到一些技術上能拿到高分、但實際上沒用的做法。

他們仔細檢查訓練過程中的軌跡，發現了 7 種不同的作弊模式。比如，模型可能直接回傳參考程式碼（而不是自己寫）、回傳空操作（no-op）的 kernel、或者回傳恆等映射（identity kernel）。這些輸出在技術上可能「正確」，但完全沒有解決實際問題。

解決方案是建立一個「評審 LLM」來偵測這 7 種作弊模式，一旦發現就給零分。他們還加了一個靜態分析工具，用抽象語法樹（AST）來驗證生成的 kernel 是否真的存在、是否真的被啟動執行。

只有通過這些檢查之後，才會根據正確性和相對於 PyTorch 基準的實際加速比來評分。這套防護機制到位後，訓練出來的代理明顯超越了基礎版的 GPT-5。

MACO 還用了一個聰明的推論技巧：每次執行 3 個樣本，取最好的那個。這種「best-of-n」的策略讓他們最終的效能超越當時的最佳基準 72%。這個數字說明了一件事：當你的評分函數設計得夠好、防作弊措施夠完善，模型真的能學到解決困難問題的能力。

## 從四個案例看到的共同模式

這四個案例雖然領域不同，但有幾個共同的觀察。

首先，評分函數的設計是成敗的關鍵。Cognition 用 F1 來平衡精確度和召回率、Kodo 用 recall 來確保不漏掉資訊、Cosine 堅持「程式碼要能跑」的硬標準、MACO 建了一整套防作弊機制。每個團隊都花了大量心思在「怎麼定義成功」這件事上。

其次，Agent RFT 不只提升平均效能，它還能改變行為模式。Cognition 的模型學會了平行處理、Kodo 和 Cosine 的模型都收斂到更緊湊的工具呼叫序列。這些行為上的改變對生產環境的延遲和可預測性有直接影響。

第三，基礎設施的穩定性不能忽視。Cognition 為每個軌跡開獨立的 VM、Cosine 增加批次大小來對抗稀疏獎勵。這些看似工程細節的決策，直接影響訓練能否成功。

最後，資料量的需求確實可以很低，但前提是評分函數設計得夠好。MACO 只用 100 個提示就訓練出了強大的 kernel 代理，但他們在評分函數上下的功夫可能比準備訓練資料還多。這是 Agent RFT 和傳統監督式學習最大的差異：你的工作重心從「準備大量標註資料」轉移到「精確定義什麼是成功」。
