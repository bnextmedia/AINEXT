---
title: "OpenAI 元老談 AGI 時程：工程師預測要乘以 2-3 倍，但 AI 自我加速可能打破規律"
date: 2026-01-22T13:00:00+08:00
description: "John Schulman 同意工程師習慣性低估專案時程，AGI 預測可能也要乘以 2-3 倍。但他也指出 AI 加速自身發展的正向回饋可能打破直覺。自駕車是最好的類比——比預期晚很多，但終究還是來了。"
tags: ["John Schulman", "AGI", "AI 預測", "自駕車", "OpenAI", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=29BYxvvF1iM"
source_name: "Cursor Podcast"
draft: false
---

> 本文整理自 Cursor Podcast 對 John Schulman 的訪談。

{{< youtube 29BYxvvF1iM >}}

---

AGI 什麼時候會來？這是 AI 領域最熱門也最沒有共識的問題。

John Schulman——OpenAI 共同創辦人、PPO 演算法發明者——在這場訪談中分享了他的思考框架。不是給出一個具體年份，而是分析預測本身的偏誤。

---

## 工程師預測的系統性偏誤

主持人提出一個觀察：工程師和研究者對專案時程的預測，通常都太樂觀。他的經驗法則是乘以 3 倍。

Schulman 同意這個偏誤存在：

> **「是的，我同意有一個持續的低估時程偏誤。在好的情況下，可能是 2 到 3 倍。」**

這個偏誤不是 AI 特有的，而是人性。我們傾向於：
- 低估複雜系統的整合難度
- 忽略「最後 10%」需要的時間
- 對未知的未知缺乏想像
- 被樂觀情緒影響判斷

把這個經驗法則套用到 AGI 預測，意味著那些說「2-3 年內達成 AGI」的預測，實際上可能是 6-9 年。

---

## 自駕車：最接近的類比

Schulman 認為自駕車是最好的類比案例：

> **「最類似的問題可能是自駕車。我們看到它比人們預期的花了更長時間，才達到全自動駕駛和 robotaxi。」**

回顧自駕車的預測歷史：
- 2015 年左右，許多人預測 2020 年會有大規模 robotaxi
- 實際上，2025 年才開始真正規模化部署
- 中間經歷了「這很快就會解決」到「這可能永遠解不了」的情緒擺盪

自駕車的教訓：
1. 長尾問題比預期難解
2. 99% 和 99.99% 的差距巨大
3. 真實世界的複雜度超乎想像
4. 但最終還是會達成

---

## 但有一個變數：AI 自我加速

Schulman 沒有完全採信「乘以 2-3 倍」的預測，因為有一個獨特變數：

> **「另一方面，有一個 AI 加速自身發展的正向回饋迴路，這也可能打破直覺。那些把這個效應納入考量的人，得出非常短的時程預測，我也認為那條推理線是有說服力的。」**

這是 AGI 預測與其他技術預測的關鍵差異。

自駕車不會讓下一代自駕車更容易開發。但 AI 可能會：
- AI 輔助寫程式，加速 AI 研究
- AI 輔助論文閱讀和想法發展
- AI 輔助實驗設計和分析
- 更好的 AI 產生更好的訓練資料

如果這個正向回饋夠強，時程可能不是線性縮短，而是指數加速。

---

## 他的結論：不做自信預測

被問到具體時程，Schulman 選擇謙遜：

> **「關於人們從 AI 獲得多少加速、以及是否有關於人類理解正在發生什麼的瓶頸，有很多不確定性。所以我不會做出非常有信心的預測。」**

他點出兩個關鍵不確定性：
1. **AI 加速效應有多強**：是 10% 還是 10 倍？
2. **人類理解是否成為瓶頸**：如果 AI 發展太快，人類能跟上嗎？

這不是迴避問題，而是承認問題的複雜度。

---

## 我的觀察：預測的價值不在準確

每次看到 AGI 時程預測，我都會想：這個預測的目的是什麼？

如果目的是「猜對年份」，那幾乎所有預測都會失敗。因為：
1. AGI 的定義本身就有爭議
2. 影響因素太多、太複雜
3. 黑天鵝事件無法預測

但預測還有另一個價值：**讓我們思考關鍵變數**。

Schulman 的分析框架就很有用：
- 承認人類預測的系統性偏誤（乘以 2-3 倍）
- 找出最接近的歷史類比（自駕車）
- 辨識這次可能不同的變數（AI 自我加速）
- 承認不確定性

### 對個人的啟示

與其猜「AGI 什麼時候來」，不如問：

1. **如果 AGI 比預期早來**，我現在該做什麼準備？
2. **如果 AGI 比預期晚來**，我是否押太多在「AI 會解決一切」上？
3. **在不確定性中**，什麼是 robust 的選擇？

Schulman 自己的選擇是：離開 OpenAI，創辦 Thinking Machines，做底層訓練基礎設施。這個選擇無論 AGI 早來晚來，都有價值——因為訓練模型的需求不會消失。

**好的預測不是猜對時間點，而是做出在各種情境下都合理的決策。**

這或許是 OpenAI 元老給我們最實用的建議。
