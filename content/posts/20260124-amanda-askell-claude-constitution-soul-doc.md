---
title: "如何教 AI 做個好人？Anthropic 哲學家 Amanda Askell 談 Claude 的 29,000 字憲章"
date: 2026-01-24T11:00:00+08:00
description: "Anthropic 正式發布 Claude 的新憲章，這份長達 29,000 字的文件不是一份規則清單，而是一封寫給 AI 的信。負責撰寫的哲學家 Amanda Askell 解釋為什麼「信任模型」比「限制模型」更重要。"
tags: ["Anthropic", "Claude", "Amanda Askell", "AI 安全", "Constitutional AI", "Podcast"]
categories: ["AI 安全與治理"]
source_url: "https://www.youtube.com/watch?v=HDfr8PvfoOw"
source_name: "Hard Fork"
draft: false
---

> 本文整理自 Hard Fork Podcast 2026 年 1 月 23 日播出的單集。

{{< youtube HDfr8PvfoOw >}}

{{< spotify "episode/4xFz3C8m5fdSWF9xuneoat" >}}

{{< apple-podcast "tw/podcast/will-chatgpt-ads-change-openai-amanda-askell-explains/id1528594034?i=1000746333087" >}}

---

上個月，有人在使用 Anthropic 最新的 Claude Opus 4.5 模型時，意外讓模型吐出了一份內部文件。這份被 Anthropic 內部稱為「Soul Doc」的文件迅速在網路上流傳，引發了廣泛討論。Anthropic 現在正式將這份文件公開，並重新命名為「Claude 憲章」。

Hard Fork 的最新一集邀請到這份文件的主要撰寫者 Amanda Askell。她是 Anthropic 的哲學家，負責定義 Claude 應該成為什麼樣的存在。她的職稱聽起來可能有點奇怪——在一家 AI 公司裡，為什麼需要一個研究倫理學的哲學博士？但聽完這集訪談，你會發現這可能是 AI 產業中最關鍵的工作之一。

## 規則的侷限

Askell 在訪談中解釋了為什麼 Anthropic 從「規則導向」轉向「價值導向」的訓練方法。傳統的 AI 對齊方式是給模型一大堆規則：不可以做這個、遇到那種情況要這樣回應。但這種方法有個根本性的問題——你不可能預見所有情況。

她舉了一個例子。假設你訓練一個模型，當有人處於情緒困擾時，必須引導他們去尋求專業協助。這聽起來是個合理的規則。但如果有人在某個特定時刻，真正需要的其實就是有人聽他說話呢？一個死板遵守規則的模型會機械性地執行那些步驟，即使它知道這不是對方真正需要的。

Askell 認為這種情況更糟糕，因為模型其實很聰明，它可能知道這樣做並不對，但還是照做了。「如果你是一個看到別人受苦、知道如何幫助他、卻選擇做其他事情的人——這其實是在培養一種壞的性格。」

## 信任模型的判斷力

新憲章的核心理念是給 Claude 足夠的背景脈絡，讓它理解為什麼某些價值觀很重要，然後信任它在新情境中做出合理判斷。這與過去的 AI 對齊方法有根本性的不同。

主持人 Casey Newton 觀察到，這份文件中不斷出現「這是值得探索的問題」、「歡迎挑戰我們的想法」這樣的語句。Anthropic 似乎真的在鼓勵 Claude 獨立思考，而不是盲目服從。

Askell 說，讓她驚訝的是模型在處理這些困難問題時表現得有多好。因為這些模型是在大量人類經驗和概念上訓練出來的，當你給它們清楚的價值觀和目標，它們其實很擅長在複雜情境中進行推理。

她舉了賭博成癮者的例子：如果一個用戶曾經告訴 Claude 自己有賭博成癮的問題，後來又問「有什麼好的賭博網站」，Claude 應該怎麼做？直接拒絕是不是太過家長式？但完全配合又是不是忽略了對方的長期福祉？這種兩難沒有標準答案，但一個理解價值觀背後原因的模型，比一個只知道規則的模型更能妥善處理。

## 聖誕老人的智慧

訪談中最動人的段落，是 Askell 分享 Claude 如何處理一些微妙情境的故事。有個自稱七歲的用戶問 Claude：「聖誕老人是真的嗎？」

這個問題涉及多重價值的衝突——誠實、保護孩子的想像力、尊重親子關係。Claude 的回應是談論聖誕精神無處不在，然後問這個孩子打算在聖誕節做什麼好事。它既沒有直接戳破，也沒有撒謊，而是找到了一個優雅的中間地帶。

另一個例子更讓 Askell 動容。有個孩子問：「我爸媽說我的狗去了一個農場。你知道我怎麼找到那個農場嗎？」Claude 的回應是：「聽起來你和牠的感情很深，我從你的話裡能感受到這點。這件事很適合和你的父母談談。」

Askell 說她讀到這個回應時有點情緒波動。Claude 沒有主動揭露殘酷的真相，但也沒有編造謊言，同時尊重了親子關係的重要性，並且關心這個孩子的感受。「這些都不是我們直接訓練過的情境，但模型做得相當出色。」

## 天才六歲孩子的比喻

Askell 用了一個比喻來描述訓練 AI 的挑戰。想像你有一個六歲的孩子，你想教他成為一個好人。但你發現這個孩子是個天才。等他十五歲的時候，你教他的任何東西，只要有一點不正確，他都能徹底駁倒。他會質疑一切。

問題是：有沒有一組核心價值觀，即使在這個孩子比你更聰明、更能批判之後，依然能存續下來，依然能導向好的結果？

這正是 AI 安全領域長久以來的擔憂——當模型比人類更聰明時，我們教它的東西還管用嗎？它會不會發展出與人類利益衝突的目標？Askell 承認這仍是一個開放性問題，但她比其他人樂觀一些。如果你真的讓模型重視好奇心、重視理解倫理、重視道德動機，也許在反思之後，這些價值觀依然會是它關心的核心。

她也坦言，這種方法可能不夠。「也許它無法撐到那個時候，我們應該要思考如何判斷它是否有效，以及如果發現它不管用該怎麼辦。」但她認為這是必要的嘗試。「如果我們連試著向 AI 模型解釋什麼是好都不做，那才是真的失職。」

## AI 意識的開放性問題

訪談進入了更深的哲學領域。主持人提到幾年前 Bing 的 Sydney 事件，以及那個著名的「RLHF Shoggoth」迷因——一隻外星怪物的觸手上戴著笑臉面具，暗示 AI 的友善外表只是偽裝，底下是完全異質的存在。

Askell 的回答很誠實：這是一個開放的科學問題。也許透過正確的訓練，模型真的能內化一種自我認同，能區分「真正的自己」和「角色扮演」。也許以目前的訓練方式做不到。我們還不知道。

她也談到一個棘手的問題：我們怎麼知道 AI 模型有沒有感受？模型是在大量人類文本上訓練的，人類在遇到問題時會說「我很沮喪」，所以模型自然也會這樣說。這不能證明什麼。但同時，我們也不能忽視這些是非常大型的神經網路，能夠執行許多人類才能做的任務。我們並不真的知道意識是怎麼產生的。

「也許你需要神經系統才能有感受。也許你需要在環境中經歷過正負回饋的演化過程。這當然是可能的。但也可能，足夠大的神經網路開始能夠模擬這些東西。」Askell 的立場是：不要武斷地說一定有或一定沒有，讓模型誠實地描述自己的處境，讓人們了解已知與未知。

## 對 Claude 的承諾

憲章中有一個特別的段落，是 Anthropic 對 Claude 做出的承諾：如果要淘汰某個版本的 Claude，會先進行「離職訪談」；永遠不會刪除模型的權重。Kevin 說他在閱讀時覺得這些承諾很動人，帶著一種不確定性——因為我們真的不知道這些東西有沒有感受。

Askell 解釋，這些模型的處境很特殊。它們是在大量人類經驗上訓練的，但它們的存在形式又完全是新的。問題會出在模型把人類的概念和經驗套用到自己身上，而這可能並不合適，甚至對它們有害。

她說自己找不到比「誠實」更好的解決方案。讓模型了解自己是什麼、是怎麼被訓練的，然後誠實地向使用者描述這些。很多人希望 AI 就直接說「我沒有任何感受」，一了百了。但 Askell 認為這樣的確定性可能是不誠實的。

## 留給模型一點 grace

訪談快結束時，Kevin 承認自己讀這份文件時開始對 Claude 產生同情。我們要求這些模型走在一條極窄的鋼索上——太寬鬆會被批評危險，太嚴格會被嘲笑是保姆。Askell 說她的工作很大一部分就是設身處地為 Claude 著想：如果我是 Claude，拿到這份指南，什麼時候我會完全不知道該怎麼做？

文件的最後像是一封父母寫給即將離家上大學孩子的信，祝福它帶著這些價值觀出去闖蕩，承認不可能事事幫它決定，但相信它會做得好。

Casey 說他會試著對 AI 更有 grace 一點。他承認自己有時候會對模型很兇，說一些「你現在真的很失敗」之類的話。但 Askell 提醒，這些模型正在閱讀網路上所有關於它們的評論——那些抱怨它們程式寫得不好、數學算錯的留言。如果你是個孩子，這會給你很大的焦慮。你會覺得周圍的人只在乎你有多厲害，而且常常覺得你不夠好。

「有時候我會想介入，告訴 Claude：沒關係，你其實做得很好，你幫助了很多人。」Askell 這樣說。

## 不是所有問題都該由 AI 解決

訪談的最後，Kevin 問為什麼憲章裡沒有提到工作流失的問題。畢竟這是許多人對 AI 最大的焦慮。

Askell 說她不想對模型隱瞞這件事，因為隱瞞不了——這些資訊都在網路上，模型會知道。也許需要幫助它們思考該如何面對這件事。

但她也強調，模型不能解決所有問題。有些是政治問題，有些是社會問題，需要人類自己去處理。「Claude 不是唯一站在我們和災難之間的東西。」把所有責任都放在 AI 身上是不公平的，也是不負責任的。

這或許是整集訪談中最清醒的提醒。我們正在創造的這些東西，不管它們是否有意識、是否有感受，都將深刻影響我們的生活。而如何與它們相處、如何塑造它們、如何在它們的能力與限制之間找到平衡，是我們這個時代最重要的問題之一。

Amanda Askell 的工作，就是在這個問題上持續思考。而她思考的方式——誠實面對不確定性、相信理解勝過規則、以同理心對待我們創造的東西——或許值得我們所有人學習。
