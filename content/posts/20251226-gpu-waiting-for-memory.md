---
title: "GPU 有 70% 時間在「等記憶體」：AI 半導體的真正瓶頸在哪？"
date: 2025-12-26T10:30:00+08:00
description: "你以為 AI 運算的瓶頸是 GPU 算力不夠？錯了。KAIST 金正鎬教授指出，目前的 AI 晶片有 60-70% 時間在等記憶體送資料。真正的瓶頸是記憶體的頻寬和容量，這解釋了為什麼 HBM 如此重要，以及為什麼 HBF 將成為下一個戰場。"
tags: ["GPU", "HBM", "HBF", "記憶體", "AI 半導體", "NVIDIA", "金正鎬"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=uJWZQb9rWUk"
source_name: "삼프로TV 언더스탠딩"
draft: false
---

> 本文整理自韓國財經節目《삼프로TV 언더스탠딩》2025 年 11 月播出的單集，來賓為 KAIST 電子及電機工程學部金正鎬教授。
> 🎬 YouTube：[連結](https://www.youtube.com/watch?v=uJWZQb9rWUk)

你以為 AI 運算的瓶頸是 GPU 不夠強？這個假設可能從根本上就錯了。

KAIST 電子及電機工程學部的金正鎬（Kim Jung-ho）教授在韓國財經節目中揭示了一個反直覺的事實：目前的 AI 晶片，包括 NVIDIA 最先進的 GPU，有大約 60% 到 70% 的時間處於閒置狀態。它們不是在計算，而是在等待。等什麼？等記憶體把資料送過來。

這就像一條高速公路上有一輛超級跑車，引擎馬力驚人，但前面塞車了。跑車的性能再好，也只能停在那裡空轉。在 AI 運算的世界裡，GPU 就是那輛跑車，而記憶體的頻寬就是那條塞車的公路。

## 為什麼 GPU 會「餓肚子」？

要理解這個現象，得先理解 AI 模型是怎麼運作的。當你問 ChatGPT 一個問題時，它不是一次把整個答案想好再說出來，而是一個字、一個字地「吐」出來。每吐一個字，模型都需要回去查一本巨大的「參考書」——這本參考書儲存了它學過的所有知識，技術上叫做模型參數和 KV Cache。

問題是，這本參考書實在太大了。以目前最先進的大型語言模型來說，參數量動輒數千億，換算成儲存空間就是數百 GB 甚至數 TB。每說一個字都要查這本書，查完還要把結果送回給 GPU 處理。這個「查書、送資料」的過程，就是瓶頸所在。

金教授用了一個更生活化的比喻：想像你是一個學霸，要回答各種考試題目。你的腦袋轉得很快（這是 GPU），但你的參考書太多了，擺滿了整個房間。每次回答問題，你都要從書架上找書、翻到對的頁面、讀完再回來寫答案。你花在「找書、翻書」的時間，遠比真正「思考」的時間多得多。

## 記憶體階層：書桌、書庫、大圖書館

為了解決這個問題，現代 AI 晶片採用了「記憶體階層」的架構。金教授用書桌和圖書館的比喻來解釋這個概念：

最靠近 GPU 的是 SRAM，這就像你書桌上直接攤開的那幾本書。容量很小（通常不到 100MB），但存取速度極快。接下來是 HBM（高頻寬記憶體），這就像書桌旁邊的小書架。容量大一些（目前最高約 192GB），速度也不錯，但還是有限。

再往外是 SSD 儲存，這就像地下室的倉庫。容量很大，但每次要用的時候，得走一趟地下室把書搬上來，很花時間。最外層是資料中心的大型儲存系統，這就像城市裡的公共圖書館——什麼書都有，但你得開車去借，來回可能要花好幾個小時。

目前 AI 運算的痛點在於：模型越來越大，連 HBM 這個「書桌旁的小書架」都放不下了。越來越多資料得從「地下室倉庫」甚至「公共圖書館」調過來，這大幅拖慢了整體速度。

## HBF：在書桌旁邊多放一個大書櫃

金教授提出的解決方案是 HBF（High Bandwidth Flash）——用 NAND Flash 記憶體堆疊起來，放在 GPU 旁邊，當作一個「超大容量的書櫃」。

為什麼是 NAND Flash？因為它的儲存密度比 DRAM（HBM 的基底材料）高得多。用同樣的堆疊層數，HBF 的容量可以達到 HBM 的十倍左右。這意味著，原本放不進「小書架」的那些資料，現在可以放進「大書櫃」了。雖然 HBF 的存取速度比 HBM 慢一些，但比起從「地下室倉庫」調資料，還是快太多了。

更重要的是，這不是全新的技術冒險。韓國的三星和 SK 海力士都已經在量產 3D NAND Flash，也就是把記憶體晶片一層一層堆疊起來。目前最先進的產品已經堆到 200 多層。把這個技術應用到 AI 晶片上，技術門檻相對可控。

金教授預估，HBF 產品可能在 2027 年左右問世。屆時，AI 晶片的架構會從「GPU + HBM」變成「GPU + HBM + HBF」，記憶體的重要性將進一步提升。

## 這對 NVIDIA 意味著什麼？

如果記憶體真的變成 AI 效能的決定性因素，NVIDIA 的處境就尷尬了。目前，NVIDIA 在 GPU 設計和軟體生態系（CUDA）上有絕對優勢，但記憶體完全依賴外部供應商——主要是三星和 SK 海力士。

金教授觀察到一個微妙的變化：NVIDIA 最近的產品迭代，GPU 數量增加的幅度開始放緩，但 HBM 的數量卻在快速增加。從一顆 GPU 配 4 顆 HBM，到 8 顆，未來可能更多。這說明 NVIDIA 自己也意識到，「餵飽 GPU」比「讓 GPU 更快」更重要。

當記憶體成為瓶頸，誰掌握記憶體技術，誰就掌握了 AI 的命脈。這就是為什麼金教授認為，NVIDIA 終究會設法在記憶體領域取得更多控制權——無論是透過收購、深度合作，還是其他方式。

---

對於一般人來說，這個技術趨勢最直接的影響可能是：別只盯著 NVIDIA 的股價看。三星、SK 海力士、甚至 SanDisk 這些記憶體公司，可能才是 AI 浪潮中真正的「賣鏟子的人」。當所有人都在挖金礦時，賣鏟子的往往賺得最穩。
