---
title: "Ilya Sutskever：為什麼 AI 模型在測驗上很強，實際使用卻讓人抓狂？"
date: 2025-12-25T10:30:00+08:00
description: "AI 模型在各種評測上表現驚人，但實際經濟影響卻遠遠落後。Ilya Sutskever 提出兩個解釋：RL 訓練讓模型過度專精，以及研究者不自覺地追著 eval 跑。這解釋了為什麼你的 AI 助手有時候聰明得嚇人，有時候又蠢得讓人崩潰。"
tags: ["Ilya Sutskever", "SSI", "AI 評測", "RL 訓練", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=aR20FWCCjAs"
source_name: "Dwarkesh Podcast"
draft: false
---

> 本文整理自 Dwarkesh Podcast 2024 年 12 月播出的單集。
> 🎧 收聽連結：[YouTube](https://www.youtube.com/watch?v=aR20FWCCjAs)

你有沒有遇過這種情況？

你用 AI 寫程式，它很快生出一版。跑起來有 bug，你告訴它。它說：「天哪你說得對！讓我修一下。」然後製造了第二個 bug。你再告訴它，它又說：「天哪我怎麼會犯這種錯！」然後把第一個 bug 帶回來。你可以在這兩個 bug 之間無限循環。

Ilya Sutskever 在訪談中描述了這個場景，然後問了一個尖銳的問題：「這怎麼可能？」

一個模型能在各種困難的評測上拿高分，能解競程題目、能寫複雜程式，怎麼會在這種基本情境中卡住？

## 這個矛盾有多嚴重？

Ilya 說，這是目前 AI 最讓人困惑的現象之一。

「模型在評測上表現太好了。你看那些評測，會覺得『這真的很難耶』。但經濟影響卻遠遠落後。」

他指的是，如果按照評測分數來看，這些模型應該已經能大幅提升生產力、改變工作方式。但實際上呢？大家確實在用，但還沒有看到那種「整個產業被翻轉」的效果。

這個落差怎麼解釋？

## 第一個解釋：RL 訓練讓模型變成考試機器

Ilya 提出的第一個解釋有點諷刺。

在 pre-training 時代，資料的問題很簡單：用所有的資料。你不需要選擇，就是把能找到的東西全部餵進去。

但 RL（強化學習）訓練不一樣。你必須選擇要讓模型在什麼環境中學習，用什麼方式給獎勵。這就產生了選擇的問題。

而人的本能是什麼？追著評測跑。

「我聽說所有公司都有團隊專門產生新的 RL 訓練環境，然後加到訓練裡。問題是，這些環境怎麼設計？很多時候，人們會從評測中找靈感。你會說：『我希望模型發布時評測分數很漂亮，那什麼樣的 RL 訓練能幫助這個任務？』」

這造成了一個問題：模型變得非常擅長「像評測的任務」，但這些能力不見得能遷移到真實世界。

## 第二個解釋：模型的泛化能力根本不夠

Ilya 用了一個比喻。

想像兩個學生都在學競程。第一個花了一萬小時練習，記住所有題型、所有演算法，成為頂尖競程選手。第二個只花了一百小時，但也表現得很好。

誰的職業生涯會更成功？

答案是第二個。因為第一個學生的能力高度專精，不一定能遷移到其他領域。第二個學生用更少的練習達到類似的水準，代表他有某種更根本的能力。

Ilya 說，現在的模型更像第一個學生——甚至更極端。「我們會說：『模型應該要很會競程，所以把所有競程題目都收集起來，再做資料增強產生更多題目。』然後你就得到一個超強的競程選手。但用這種方式訓練出來的能力，不見得能泛化到其他事情上。」

這解釋了為什麼模型能在評測上拿高分，卻在實際使用中犯低級錯誤。那些評測分數反映的是「經過大量專項訓練後的表現」，不是「真正理解問題後的通用能力」。

## 誰才是真正在做 reward hacking？

訪談中有一句話讓我印象深刻：「真正的 reward hacking，是那些太專注於 eval 的研究員。」

這話聽起來刺耳，但可能是對的。

當整個產業都在追求評測分數，當公司發布模型時都要秀各種 benchmark 的進步，研究者自然會傾向優化這些指標。這不是壞意，只是人的本能。但結果是，模型變得越來越會考試，卻不見得變得更有用。

## 這對我們使用 AI 有什麼啟示？

幾個實際的建議：

第一，不要只看 benchmark。當你選擇用哪個模型時，評測分數只是參考。更重要的是在你實際的使用場景中試一試。

第二，預期會遇到奇怪的錯誤。那種「聰明得嚇人」和「蠢得讓人崩潰」交替出現的體驗是正常的，不是你用錯了。模型確實有這種不一致性。

第三，理解「能做到」和「穩定做到」的差別。模型可能在某個任務上展現驚人能力，但這不代表它每次都能做到。在重要的事情上，還是需要人類檢查。

Ilya 的觀察揭示了一個更深的問題：我們可能還不知道怎麼訓練出真正「理解」任務的模型，而不只是「會考試」的模型。這是接下來需要解決的核心問題之一。
