---
title: "OpenAI 讓 AI 在訓練時操作真實世界——Agent RFT 是什麼？"
date: 2025-12-25T14:00:00+08:00
description: "OpenAI 推出 Agent RFT，首次讓模型在訓練過程中與外部世界互動。這項技術允許 AI 代理在訓練時呼叫真實的工具端點，並透過自訂獎勵函數學習最佳行為模式。對於打造企業級 AI 代理的開發者來說，這是一個重要的里程碑。"
tags: ["OpenAI", "Agent RFT", "AI Agent", "Fine-tuning", "強化學習"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=p1CmPZ2j6Lk"
source_name: "OpenAI DevDay"
draft: false
---

> 本文整理自 OpenAI DevDay 的技術分享。
> 🎧 收聽連結：[YouTube](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)

「這是我們第一次讓模型在訓練過程中與外部世界互動。」OpenAI 微調團隊的 Will Hang 在介紹 Agent RFT 時這樣說。這句話聽起來或許平淡，但它標誌著 AI 訓練範式的一個重要轉變：模型不再只是被動地學習靜態資料，而是在訓練過程中主動操作真實環境、接收真實反饋。

## Agent 與一般模型有什麼不同

要理解 Agent RFT 為什麼重要，首先得搞清楚什麼是 AI Agent。一般的語言模型像是一個知識淵博的顧問，你問它問題，它給你答案，但它無法替你「做事」。Agent 則不同，它能夠與外部世界互動，使用各種工具來完成任務，而且整個過程不需要你一步步指導。

具體來說，一個 Agent 需要具備兩種能力的交織：工具呼叫和推理。想像你在使用一個程式碼 Agent，它不只是告訴你「應該這樣寫」，而是直接打開終端機、執行指令、讀取程式碼庫、甚至提交修改。在這個過程中，它的工具呼叫和思考過程是交錯進行的——呼叫一個工具、看到結果、思考下一步、再呼叫另一個工具。OpenAI 內部的 Codex 就是按照這個範式打造的，它能端對端地完成程式碼任務，從寫單元測試到提交大規模的程式碼變更。

這種設計帶來了一個核心挑戰：如何讓 Agent 正確地使用你的工具？模型在 OpenAI 內部訓練時，接觸的是特定的環境和工具集。但你的業務環境可能完全不同，你的工具有不同的輸入格式、不同的回傳結構、不同的使用情境。這種「領域偏移」（domain shift）會導致 Agent 表現失常——它可能呼叫太多次工具、傳入錯誤的參數、或者完全搞錯工具的用途。

## Agent RFT 的運作原理

Agent RFT（Agent Reinforcement Fine-Tuning）的核心想法是：讓模型在訓練過程中直接操作你的真實環境，透過強化學習來調整它的行為。這聽起來簡單，實際上是一個重大的系統工程突破。

傳統的微調方式是給模型一堆「輸入-輸出」配對，告訴它「看到這個問題，應該這樣回答」。但 Agent 的任務本質上是多步驟的，你很難事先定義每一步該怎麼做。Agent RFT 的做法是：讓模型自己去探索各種可能的工具呼叫序列，然後根據你定義的獎勵函數來判斷哪些行為是好的、哪些是壞的。

具體來說，OpenAI 新增了兩個關鍵功能。第一，模型現在可以在訓練過程中呼叫你架設在公開網路上的工具端點。這意味著模型真的會打 API 給你的系統、執行真實的操作、拿到真實的回應。第二，每次模型完成一個任務（稱為一次「rollout」），系統會呼叫你自訂的評分端點，讓你告訴模型這次表現如何。

這套機制帶來了幾個直接的好處。模型會學會更好地使用你的工具，不只是知道「該用哪個工具」，還會學到更細緻的技巧，比如什麼時候該平行呼叫多個工具、什麼時候該等待上一個結果再繼續。更有意思的是，你可以透過獎勵函數來控制模型的行為模式。如果你想讓模型在固定的工具呼叫次數內完成任務，可以對超出預算的行為施加懲罰，模型會自己學會如何在限制內高效完成工作。

## 使用 Agent RFT 的正確時機

OpenAI 的建議是，不要一開始就跳進 Agent RFT。這聽起來有點反直覺——既然這是最強的工具，為什麼不直接用？原因在於，微調是一個需要投入資源的過程，而且它的效果取決於你的基礎設定是否正確。

正確的流程應該是這樣：首先，確保你的訓練資料集和評估資料集真的反映了生產環境的流量分布。如果訓練資料和實際使用情境有落差，微調出來的模型在生產環境可能反而表現更差。接著，用基礎模型跑一遍你的評估資料集，建立效能基準線，知道「不微調的話能到什麼程度」。

然後，先嘗試不需要微調的最佳化手段：調整提示詞、簡化任務設計、增減工具、改善工具的輸入輸出格式。這些方法成本低、迭代快，往往能帶來可觀的改善。只有當你把這些方法都試過了，還是覺得「差那麼一點」，才是動用 Agent RFT 的時機。

這個建議背後有一個務實的考量：Agent RFT 需要你建置一套穩定的基礎設施——能夠處理大量訓練請求的工具端點、可靠的評分系統、以及足夠多的高品質訓練樣本。如果你連基礎的提示詞都還沒調好，花時間建這套系統的投資報酬率不會太高。

## 這對開發者意味著什麼

Agent RFT 的推出，對於認真想打造 AI Agent 產品的團隊來說，是一個值得關注的訊號。它說明了一件事：打造好用的 Agent，光靠提示詞工程和任務設計是有天花板的，最終還是需要讓模型真正適應你的環境。

從實務角度來看，這意味著你需要開始思考幾件事。你的工具設計是否足夠乾淨，能夠支援大量的訓練請求？你有沒有辦法定義出一個好的評分函數，準確反映「任務成功」的標準？你的訓練資料集是否真的代表了使用者會遇到的情境？

這些問題在傳統的軟體開發中不太會被問到，但在 Agent 開發的脈絡下，它們變成了核心的工程挑戰。Agent RFT 提供了一個強大的工具，但它也對開發者的系統設計能力提出了更高的要求。

從更宏觀的角度來看，Agent RFT 代表的是一種新的人機協作模式：人類定義目標和約束（透過獎勵函數），機器自己探索達成目標的方法。這種模式讓開發者可以把精力放在「定義什麼是好的結果」，而不是「規定每一步該怎麼做」。對於那些任務太複雜、無法事先規劃每個步驟的場景，這可能是唯一可行的開發方式。
