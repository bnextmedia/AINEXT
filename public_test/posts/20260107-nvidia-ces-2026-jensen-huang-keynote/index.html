<!DOCTYPE html>
<html lang="en" >
  <head>
  <title>Nvidia 的全棧帝國： ces 2026 揭示黃仁勳的終極野心 | AINEXT</title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


<meta name="keywords" content="AINEXT">
<meta property="og:locale" content='en_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="NVIDIA 的全棧帝國：CES 2026 揭示黃仁勳的終極野心">
<meta property="og:description" content="
本文整理自 NVIDIA 於 CES 2026（2026 年 1 月 5 日）發表的完整活動，包含開場 Panel 討論與黃仁勳主題演講。


  
    ">
<meta property="og:url" content="https://bnextmedia.github.io/AINEXT/posts/20260107-nvidia-ces-2026-jensen-huang-keynote/">
<meta property="og:image" content="https://bnextmedia.github.io/AINEXT/images/images/logo.png">
<link rel="canonical" href="https://bnextmedia.github.io/AINEXT/posts/20260107-nvidia-ces-2026-jensen-huang-keynote/">

<link rel="apple-touch-icon" sizes="180x180" href='https://bnextmedia.github.io/AINEXT/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='https://bnextmedia.github.io/AINEXT/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='https://bnextmedia.github.io/AINEXT/favicon-16x16.png'>
<link rel="manifest" href='https://bnextmedia.github.io/AINEXT/site.webmanifest'>

<link rel="stylesheet" href="https://bnextmedia.github.io/AINEXT/css/styles.648e60c6d86809f863ae1346848574b9c685732794e7851c7d3e557de9ddd293bc1c209f963d5041785c2fd4268470bdfde453a99f550b655d1b4f825eed4682.css" integrity="sha512-ZI5gxthoCfhjrhNGhIV0ucaFcyeU54UcfT5Vfend0pO8HCCflj1QQXhcL9QmhHC9/eRTqZ9VC2VdG0&#43;CXu1Ggg==">
</head>

  <body>
    <div class="nav-drop">
  <div class="nav-body">
      <a href="https://bnextmedia.github.io/AINEXT/" class="nav_item">首頁</a>
      <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav_item">文章列表</a>
    <div class="nav-close"></div><div class="color_mode">
  <label for="mode">Toggle Dark Mode</label>
  <input type="checkbox" class="color_choice" id="mode">
</div>

  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href=https://bnextmedia.github.io/AINEXT/ class="nav-brand nav_item">
        <img src="https://bnextmedia.github.io/AINEXT/images/logo.png" alt="AINEXT " class="logo-light" width="130px" />
        <img src="https://bnextmedia.github.io/AINEXT/images/logo-dark.png" alt="AINEXT " class="logo-dark" width="130px" /></a>
    
    
    <div class="nav-links">
        <a href="https://bnextmedia.github.io/AINEXT/" class="nav-link">首頁</a>
        <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav-link">文章列表</a>
    </div>
    
    <div class="nav_bar-wrap">
      <div class="nav_bar"></div>
    </div>
  </nav>
</header>

<style>
 
.nav-links {
  display: flex;
  flex-direction: row;
  gap: 1.5rem;
  align-items: center;
  white-space: nowrap;
}

.nav-link {
  color: var(--text);
  text-decoration: none;
  font-size: 0.95rem;
  padding: 0.5rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover {
  color: var(--theme);
}

 
@media (min-width: 768px) {
  .nav_bar-wrap {
    display: none;
  }
}

 
@media (max-width: 767px) {
  .nav-links {
    display: none;
  }
  
  .nav_bar-wrap {
    display: grid;
  }
}

 
.logo-dark {
  display: none;
}

html[data-mode="dark"] .logo-light {
  display: none;
}

html[data-mode="dark"] .logo-dark {
  display: block;
}

 
@media (prefers-color-scheme: dark) {
  html:not([data-mode="light"]) .logo-light {
    display: none;
  }
  
  html:not([data-mode="light"]) .logo-dark {
    display: block;
  }
}

 
.nav {
  position: relative !important;
  background: var(--bg);
  padding: 0.5rem 0;
  border-bottom: 1px solid var(--border);
}

.mt {
  margin-top: 2rem !important;
}

 
.post {
  padding-top: 1rem;
}

.post_date {
  margin-top: 0;
}

 
.archive {
  padding-top: 1rem;
}

.archive_title {
  margin-top: 0;
}
</style>


    <main>
      
  <div class="wrap mt post">
    <div><p class=post_date>07. January 2026</p>
      <h1 class="post_title">NVIDIA 的全棧帝國：CES 2026 揭示黃仁勳的終極野心</h1>
      <div class="post_body">
        <div class="post_inner">
        
        
          <blockquote>
<p>本文整理自 NVIDIA 於 CES 2026（2026 年 1 月 5 日）發表的完整活動，包含開場 Panel 討論與黃仁勳主題演講。</p>
</blockquote>
<div class="media-embed">
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/0NBILspM4c4" 
      allowfullscreen 
      title="YouTube Video"
      loading="lazy"
    ></iframe>
  </div>
</div>

<style>
.media-embed {
  max-width: 100%;
  margin: 1rem 0;
}

.video-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
}

.video-container iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border: 0;
  border-radius: 12px;
}
</style>

<hr>
<h2 id="開場白這不只是產品發布">開場白：這不只是產品發布</h2>
<p>3,000 人擠滿拉斯維加斯 Fountain Blue 劇院，另外 3,000 人在場外透過螢幕觀看，全球數百萬人同步收看直播。這是 CES 2026 最受矚目的一場活動，但黃仁勳要展示的，遠不只是幾顆新晶片。</p>
<p>當你看完這整場活動，你會發現一件事：<strong>NVIDIA 已經不是一家賣 GPU 的公司了。</strong> 它正在系統性地吃下 AI 產業鏈的每一層——從最底層的晶片，到最上層的應用程式，中間經過基礎設施、模型、開發工具，全部都要。</p>
<p>更令人不安的是，它的策略不是封閉壟斷，而是「開放壟斷」。所有模型都開源，所有工具都免費，所有架構都公開——但每一層都跑在 NVIDIA 的硬體上。這是一種比微軟 Windows 更聰明的生態鎖定。</p>
<p>讓我從頭說起。</p>
<hr>
<h2 id="第一部分panel-討論nvidia-生態圈的集體現身">第一部分：Panel 討論——NVIDIA 生態圈的集體現身</h2>
<p>主題演講前，NVIDIA 安排了一場精心策劃的 Panel 討論。表面上是邀請產業專家聊 AI 趨勢，實際上是讓整個 NVIDIA 生態圈集體現身，展示這家公司的觸角已經伸到多遠。</p>
<h3 id="11-ai-基礎設施為什麼這次不是泡沫">1.1 AI 基礎設施：為什麼這次不是泡沫</h3>
<p>Bank of America 半導體分析師 Vivek Arya 開場就點出一個關鍵問題：過去三年，超過 8000 億美元砸進 AI 基礎設施，光是 2026 年預估就有 6000 億美元。每個人都在問：這是泡沫嗎？</p>
<p>Arya 給出三個理由，說明為什麼這次不同：</p>
<p><strong>第一，無縫採用（Seamless Adoption）</strong>。ChatGPT 上線那天，全球 50 億手機和電腦用戶立刻可以使用。不像過去的光纖建設，基礎設施蓋好了，終端用戶還沒準備好。這次是基礎設施追著需求跑。</p>
<p><strong>第二，高利用率</strong>。還記得「暗光纖」（Dark Fiber）這個詞嗎？當年光纖鋪滿地底，卻沒人在用。但現在，Arya 說：「你聽不到『暗運算』（Dark Compute）這個詞。所有部署的運算基礎設施都被充分利用，連六、七年前的老舊 GPU 都跑滿了。」</p>
<p><strong>第三，資金來源不同</strong>。上一波是資金不足的新創在燒錢，這一波是現金流充沛的大公司在投資。他們的資本支出只占營運現金流的三分之二，自由現金流依然為正。</p>
<p>這段話表面上在分析產業，實際上在說：<strong>NVIDIA 的需求是真實的，而且會持續。</strong></p>
<h3 id="12-snowflake資料平台如何成為-nvidia-附庸">1.2 Snowflake：資料平台如何成為 NVIDIA 附庸</h3>
<p>Snowflake CEO Sridhar Ramaswamy 上台，談的是他們與 NVIDIA 的合作。但聽完之後，你會發現 Snowflake 在 AI 領域的每一步都離不開 NVIDIA：</p>
<ul>
<li>Snowflake Intelligence（他們的 AI 產品）跑在 NVIDIA 晶片上</li>
<li>過去訓練的大型基礎模型用 NVIDIA 晶片</li>
<li>正在開發的嵌入模型與 NVIDIA 深度合作</li>
<li>未來 GPU 加速資料運算也要靠 NVIDIA</li>
</ul>
<p>Ramaswamy 還透露了一個細節：<strong>他自己的手機上就跑著一個資料 Agent</strong>，隨時可以問「這個客戶的狀況如何？關係是上升還是下降？」這種即時存取企業資料的能力，正是 AI Agent 的殺手級應用。</p>
<p>但更有趣的是他談到的企業採用障礙：</p>
<blockquote>
<p>「實務問題永遠存在。資料所有權、資料主權、特定地區的 GPU 微短缺（micro shortages）&hellip;&hellip;我們經常遇到德國客戶沒有足夠的德國境內算力，必須協商能不能把流量送到瑞典。」</p>
</blockquote>
<p>這段話揭示了一個現實：<strong>NVIDIA GPU 的短缺已經細化到區域層級</strong>。不是全球缺貨，是德國缺、瑞典不缺。這種「微短缺」反而強化了 NVIDIA 的議價地位。</p>
<h3 id="13-開放模型-vs-封閉模型開發者心智的爭奪戰">1.3 開放模型 vs 封閉模型：開發者心智的爭奪戰</h3>
<p>Ramaswamy 對開放模型有一段精闘的分析：</p>
<blockquote>
<p>「前沿模型在關鍵應用上——像 Agent 工具呼叫、程式碼 Agent——還是遠遠領先。但當事情成熟、人們想要大規模運行時，開放模型就變得越來越重要。」</p>
<p>「我們都需要記住：開放模型對開發者心智有巨大影響。只有 OpenAI 那幾千名工程師能接觸到最新最強的模型，這是一個很小的環境。但我們反覆看到，開放平台能吸引成千上萬、甚至數百萬開發者投入，這可以形成正向循環。」</p>
<p>「這就是為什麼連 OpenAI 這種封閉公司也會釋出開放模型——因為他們想成為開發者生態圈的一部分。」</p>
</blockquote>
<p>這段話解釋了為什麼 NVIDIA 要大力投入開放模型。不是因為善心，是因為<strong>開發者在哪裡，生態圈就在哪裡，硬體需求就在哪裡</strong>。</p>
<h3 id="14-醫療-aiabridge-的臨床革命">1.4 醫療 AI：Abridge 的臨床革命</h3>
<p>Abridge 創辦人 Shiv 帶來了醫療 AI 的第一線觀察。他引用了一個驚人的數據：</p>
<blockquote>
<p>「幾年前《一般內科醫學期刊》有篇文章指出，醫生需要一天 30 小時才能完成所有工作。」</p>
</blockquote>
<p>目前醫生的工作分配是 80% 文書作業、20% 面對面臨床推理。Abridge 的目標是用 AI 翻轉這個比例。</p>
<p>Shiv 強調，醫療 AI 的成功不在技術，在於<strong>融入工作流程</strong>：</p>
<blockquote>
<p>「我們必須把自己塞進工作流程，而不是讓工作流程來配合我們。這不只是勾選隱私和安全的方框，還要考慮延遲、要考慮醫療紀錄的所有產出物。」</p>
</blockquote>
<p>他們的做法是蒸餾（distillation）、微調（fine-tuning）、後訓練（post-training）。現在 Abridge 每年將觸及 8000 萬到 1 億條生命，每天從編輯中學習，持續變得更好。</p>
<p>關於信任，Shiv 說得很直接：</p>
<blockquote>
<p>「信任是基本門檻。它是透明度、可靠性、可信度的組合。透明度包括可審計性、與醫療系統分享我們的指標。可信度則是發表研究、與研究者合作進行隨機對照試驗來評估實際影響。」</p>
</blockquote>
<p>他們的信條是：<strong>省時間、讓人更專注彼此、省錢</strong>。最終目標是第三階段——救命，幫助臨床醫生做出更好的決策。</p>
<h3 id="15-程式碼審查-aicoderabbit-的信任層">1.5 程式碼審查 AI：CodeRabbit 的信任層</h3>
<p>CodeRabbit 創辦人 Harjot 代表的是 AI 程式碼領域。他的定位很清楚：</p>
<blockquote>
<p>「程式碼生成的量大幅增加，CodeRabbit 用生成式 AI 來審查這些程式碼。我們提供一個關鍵的信任層，介於你的程式碼 Agent 和生產環境之間，讓組織能對進入生產環境的東西設定護欄。」</p>
</blockquote>
<p>這家公司已經被數萬個組織、數十萬開發者使用。Harjot 觀察到一個有趣的現象：</p>
<blockquote>
<p>「生成式 AI 降低了進入軟體開發的門檻。以前的瓶頸是『我不會 TypeScript、不會 Rust』，現在這個門檻大幅降低了。同時，生成式 AI 並不會讓 10 倍工程師變成 100 倍。它把開源中的優秀範例帶給普通開發者或初學者，讓他們的程式碼『夠好』。」</p>
</blockquote>
<p>換句話說：<strong>AI 縮小了高手和新手的差距，但沒有把天花板提高。</strong> 這解釋了為什麼審查變得更重要——產出更多了，但品質參差不齊。</p>
<h3 id="16-agent-可靠性human-in-the-loop-的必要性">1.6 Agent 可靠性：Human-in-the-Loop 的必要性</h3>
<p>當被問到「Agent 可靠性是否已經解決」時，Shiv 和 Harjot 都給出了務實的答案。</p>
<p>Shiv 區分了兩種情境：</p>
<blockquote>
<p>「在醫療領域，有高頻率、低風險的工作流程，Agent 可以在適當的護欄下在背景執行。但也有需要人類參與的任務，特別是涉及化療、重要診斷或治療決策時，必須有人類做最後一哩。」</p>
</blockquote>
<p>Harjot 則指出程式碼領域的現狀：</p>
<blockquote>
<p>「我們看到強烈產品市場契合的使用案例，都是非常互動式的——人類在迴圈中，持續與 Claude Code 或其他 Agent 對話。挑戰在於，一旦進入背景 Agent 可以長時間運行的模式，可靠性就非常低。」</p>
<p>「你會發現，十個開發者裡，可能只有兩三個真的把 Cursor 或 Claude Code 用得很好，其他人還在摸索正確的提示技巧。」</p>
</blockquote>
<p>這是一個重要的現實檢查：<strong>Agent 的炒作與實際可靠性之間還有很大落差。</strong></p>
<h3 id="17-開源模型的成本考量">1.7 開源模型的成本考量</h3>
<p>關於開源模型是否已經追上閉源模型，Harjot 的觀察是：</p>
<blockquote>
<p>「12 個月前，開源模型還很早期。但現在我們看到開源模型開始取代我們部分工作負載，像是摘要和程式碼庫搜尋。這些可以用更小、更划算的開源模型很好地完成。」</p>
<p>「『你只能用最好的程式碼模型，不管多貴多慢』的敘事已經改變了。開源確實在追上。但頂級模型還是頂級模型——像是程式碼審查這種推理密集的使用案例，我們還是會用 Opus 或 GPT-5.2 這類模型的最高設定。」</p>
</blockquote>
<p>Shiv 補充了垂直領域的觀點：</p>
<blockquote>
<p>「在醫療這種垂直產業的應用層，擁有更多控制權會帶來很大的差異。LLM 對垂直任務來說還沒有商品化——你需要在上面做很多工作才能創造差異化。」</p>
</blockquote>
<h3 id="18-mercedes-benzlevel-3-自駕的真實挑戰">1.8 Mercedes-Benz：Level 3 自駕的真實挑戰</h3>
<p>Mercedes-Benz CEO Ola Källenius 帶來了自駕車的第一手經驗。Mercedes 是最早取得美國 Level 3 自駕認證的車廠之一。</p>
<p>Källenius 強調，Level 3 的突破不只是工程問題，更是法律問題：</p>
<blockquote>
<p>「Level 3 的重大突破不只是工程任務，也是法律任務——因為電腦真的接管了。這是 Level 2（人類負責）和 Level 3 及以上（電腦負責）的差別。」</p>
</blockquote>
<p>他也坦承「長尾問題」的挑戰：</p>
<blockquote>
<p>「讓工程團隊做到 99% 的展示相對容易。但其他所有可能發生的事情的長尾，那才是更大的挑戰。這就是為什麼安全是關鍵。你可以快速但馬虎，但那樣你承擔的風險可能超出你的預期。」</p>
</blockquote>
<p>Mercedes 的策略是：不急著搶第一，但要確保推出的東西夠穩健。他們剛在舊金山和矽谷測試了與 NVIDIA 合作的 Level 2++ 系統：</p>
<blockquote>
<p>「我昨天開了一個多小時，從舊金山穿過相當繁忙的週日交通，上高速公路、下高速公路、回到市區，全程無中斷。感覺就像車子在軌道上行駛。」</p>
</blockquote>
<p>這套系統將在今年稍晚於美國上市，使用 NVIDIA 技術。</p>
<h3 id="19-skilled-ai通用機器人大腦">1.9 Skilled AI：通用機器人大腦</h3>
<p>Skilled AI 創辦人兼卡內基美隆大學機器人學教授 Deepak Pathak，帶來了機器人領域最前沿的思考。</p>
<p>他的一句話定義：<strong>「任何機器人、任何任務、一個大腦。」</strong></p>
<p>為什麼要追求這種通用性？Pathak 解釋：</p>
<blockquote>
<p>「想到語言模型，你會想到大腦。想到機器人，你會想到硬體。但過去 70 年，儘管有那麼多令人印象深刻的機器人展示，每年都感覺機器人要搶走工作、要進入家庭了，卻從未發生。原因是缺少一個通用大腦。」</p>
<p>「硬體有那麼多種——工廠用的、家用的、狗型的、人形的。如果能跨機器人身體通用，就能利用任何來源的資料，創造持續推動進步的資料飛輪。」</p>
</blockquote>
<p>關於機器人領域的資料稀缺問題，Pathak 的解法是：</p>
<blockquote>
<p>「沒有魔法子彈。不像數位 AI，網路上有資料。機器人沒有。所以我們必須從某處啟動。我們從人類影片啟動——觀看人類做事，可以是第一人稱視角，也可以是 YouTube 或 Flickr 上的第三人稱視角。」</p>
<p>「但光看不夠。如果夠的話，我看了那麼多費德勒的比賽，應該打得跟他一樣好了。你必須練習、嘗試、失敗。但在真實世界失敗幾百萬次是不可能的，這就是模擬的用處。」</p>
<p>「真實人類影片加模擬，這是我們啟動的方式。然後創建基礎層，再針對各種任務微調。比如在 Mercedes 工廠的某個場景，我們可以收集遙控資料來微調。」</p>
</blockquote>
<p>他也呼應了語言模型的經驗：<strong>先通用，再專精。</strong> 在 LLM 出現之前，每個應用都有專門的系統，但都很爛。LLM 的發現是：先做通用模型，再微調。機器人也應該走同樣的路。</p>
<h3 id="110-工廠機器人百年來最大的生產力躍進">1.10 工廠機器人：百年來最大的生產力躍進</h3>
<p>Källenius 對工廠機器人的前景非常興奮：</p>
<blockquote>
<p>「聽 Deepak 講，我很興奮，因為我知道機器人領域正在發生的事，將會是我們工廠幾十年甚至一百年來最大的生產力躍進。」</p>
<p>「你可以想像，一個製造業工人在做某些任務，旁邊有他的機器人夥伴。這個機器人夥伴是會思考的機器，你可以跟它說話、給它指令，它可以幫你完成任務。」</p>
</blockquote>
<p>他們已經在工廠進行試驗，預計三到五年內，走進 Mercedes 工廠會看到完全不同的景象。</p>
<p>同時，透過 NVIDIA Omniverse，他們在虛擬世界建造工廠，在灌注任何混凝土之前，就能在虛擬世界完整生產汽車，用 AI 工具除錯。這讓建造製造設施更快、更便宜。</p>
<p>Pathak 補充了一個重要觀點：</p>
<blockquote>
<p>「在工廠裡，如果我要選一個任務垂直深入，有幾百個任務可選。每個任務可能只有三四個人在做，但每個任務都跟其他任務有點不同。這是機器人本質上是通用問題的另一個論點。」</p>
</blockquote>
<hr>
<h2 id="第二部分黃仁勳主題演講全棧帝國的藍圖">第二部分：黃仁勳主題演講——全棧帝國的藍圖</h2>
<p>Panel 討論結束，黃仁勳登場。接下來兩小時，他系統性地展示了 NVIDIA 的全棧布局。</p>
<h3 id="21-雙重平台轉移ai-正在重塑一切">2.1 雙重平台轉移：AI 正在重塑一切</h3>
<p>黃仁勳開場就定調：</p>
<blockquote>
<p>「每 10 到 15 年，電腦產業就會重置。新的平台轉移發生。從大型主機到 PC、PC 到網路、網路到雲端、雲端到行動裝置。每一次，應用程式都會以新平台為目標。」</p>
<p>「但這一次，有兩個平台轉移同時發生。第一，應用程式現在建構在 AI 之上。人們一開始以為 AI 是應用程式，但其實 AI 是應用程式，而且你還要在 AI 上面建構應用程式。」</p>
<p>「第二，軟體的運行和開發方式徹底改變了。整個電腦產業的軟體堆疊正在被重新發明。你不再『程式設計』軟體，而是『訓練』軟體。不再跑在 CPU 上，而是跑在 GPU 上。」</p>
</blockquote>
<p>這意味著過去十年約 10 兆美元的運算投資，現在都要「現代化」到新架構。這解釋了為什麼有數千億美元的創投資金湧入，為什麼各產業的研發預算正在從傳統方法轉向 AI 方法。</p>
<h3 id="22-2025-年四大突破">2.2 2025 年四大突破</h3>
<p>黃仁勳回顧了過去一年的關鍵進展：</p>
<p><strong>Scaling Laws 持續有效</strong>：從 2015 年的 BERT、2017 年的 Transformer、2022 年的 ChatGPT，到 2023 年 O1 推理模型引入的「測試時擴展」（test-time scaling），每個階段都需要更多算力。預訓練、後訓練（強化學習）、推理時思考，三個階段的運算需求都在爆炸。</p>
<p><strong>Agentic Systems 興起</strong>：具備推理、研究、使用工具、規劃未來、模擬結果能力的 AI Agent。黃仁勳特別點名 Cursor：「徹底改變了我們在 NVIDIA 做軟體開發的方式。」</p>
<p><strong>Physical AI 成為焦點</strong>：理解物理世界常識的 AI——物體恆存、因果關係、摩擦力、重力、慣性。這些對幼兒是常識，但對 AI 完全未知。同時還有「AI 物理學」——理解物理定律的 AI。</p>
<p><strong>開放模型達到前沿</strong>：DeepSeek R1 的出現激活了整個開放模型運動。雖然仍落後封閉模型約六個月，但每六個月就有新模型出現。下載量爆炸，因為新創、大公司、研究者、學生、每個國家都想參與 AI 革命。</p>
<h3 id="23-nvidia-的開放模型帝國">2.3 NVIDIA 的開放模型帝國</h3>
<p>這可能是最令人意外的部分。NVIDIA 投入數十億美元建造自己的 DGX 超級電腦，不是為了賣雲端服務，而是為了開發開放模型。</p>
<p>黃仁勳展示了 NVIDIA 在各領域的開放模型：</p>
<table>
  <thead>
      <tr>
          <th>領域</th>
          <th>模型</th>
          <th>功能</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>數位生物學</td>
          <td>La Proteina</td>
          <td>蛋白質合成與生成</td>
      </tr>
      <tr>
          <td></td>
          <td>OpenFold3</td>
          <td>蛋白質結構理解</td>
      </tr>
      <tr>
          <td></td>
          <td>Evo 2</td>
          <td>多蛋白質理解與生成，細胞表示的開端</td>
      </tr>
      <tr>
          <td>物理模擬</td>
          <td>Earth 2</td>
          <td>理解物理定律的 AI</td>
      </tr>
      <tr>
          <td></td>
          <td>ForecastNet</td>
          <td>天氣預測</td>
      </tr>
      <tr>
          <td></td>
          <td>CoreDiv</td>
          <td>物理模擬</td>
      </tr>
      <tr>
          <td>語言模型</td>
          <td>Nemotron</td>
          <td>混合 Transformer-SSM 架構，可快速思考或深度推理</td>
      </tr>
      <tr>
          <td>世界模型</td>
          <td>Cosmos</td>
          <td>理解世界運作方式的開放基礎模型，對齊語言</td>
      </tr>
      <tr>
          <td>機器人</td>
          <td>Groot</td>
          <td>人形機器人系統，整合關節、移動、行走</td>
      </tr>
      <tr>
          <td>自駕車</td>
          <td>Alpamayo</td>
          <td>會思考、會推理的自駕 AI</td>
      </tr>
  </tbody>
</table>
<p>關鍵是：<strong>不只開源模型，還開源訓練資料。</strong> 黃仁勳說：「只有這樣，你才能真正信任模型是怎麼來的。」</p>
<p>他們還提供完整的生命週期管理工具（NEMO 系列），從資料處理、生成、訓練、評估、護欄到部署，全部開源。</p>
<p>這些模型不只是開源，還登上各種排行榜榜首——智慧、PDF 解析、語音辨識、語意搜尋。NVIDIA 現在是 AI 模型領域貢獻最多的公司之一。</p>
<h3 id="24-agentic-ai-架構未來應用程式的藍圖">2.4 Agentic AI 架構：未來應用程式的藍圖</h3>
<p>黃仁勳詳細解釋了 AI Agent 的架構革命：</p>
<blockquote>
<p>「ChatGPT 剛出來時，人們說它會幻覺。當然會幻覺——它能記住過去的一切，但無法記住未來和現在。所以它需要做研究、查資料。」</p>
<p>「推理的能力——判斷是否需要做研究、是否需要使用工具、如何把問題拆解成步驟——每個步驟都是 AI 知道怎麼做的事，組合起來就能完成從未被訓練過的任務。這是推理的美妙之處。」</p>
</blockquote>
<p>他特別提到 Perplexity 的創新：<strong>同時使用多個模型。</strong> 一個 AI 可以呼叫世界上所有優秀的 AI 來解決問題的不同部分。這讓 AI 天生就是多模態（理解語音、圖像、文字、影片、3D、蛋白質）、多模型（使用最適合任務的模型）、多雲（模型分散在各處）、混合雲（有時在邊緣、有時在企業、有時在醫院）。</p>
<p>現場展示了一個簡單的個人助理 Demo：用 DGX Spark（家用 AI 電腦）搭配 Hugging Face 的 Ricci 機器人，建立一個能管理日曆、電子郵件、待辦事項的助理。幾分鐘內就能組合前沿模型 API、本地開放模型、意圖路由器、機器人控制、語音合成。</p>
<p>黃仁勳說：「這在幾年前是完全不可想像的，現在是完全微不足道的。」</p>
<p>這個架構已經整合到企業 SaaS 平台：</p>
<ul>
<li><strong>Palantir</strong>：整個 AI 和資料處理平台由 NVIDIA 加速</li>
<li><strong>ServiceNow</strong>：全球領先的客服和員工服務平台</li>
<li><strong>Snowflake</strong>：頂級雲端資料平台</li>
<li><strong>CodeRabbit</strong>：NVIDIA 內部大量使用</li>
<li><strong>CrowdStrike</strong>：用 AI 偵測 AI 威脅</li>
<li><strong>NetApp</strong>：資料平台加上語意 AI 和 Agent 系統</li>
</ul>
<p>重點是：<strong>Agentic 系統將成為這些平台的使用者介面。</strong> 不再是 Excel 表格或命令列，而是像跟人對話一樣與平台互動。</p>
<h3 id="25-physical-ai三台電腦的架構">2.5 Physical AI：三台電腦的架構</h3>
<p>Physical AI 是黃仁勳談了好幾年的主題，NVIDIA 已經投入八年。核心問題是：如何讓 AI 從螢幕走進真實世界？</p>
<p>這需要教 AI 物理世界的常識：物體恆存（看開再看回來，東西還在）、因果關係（推一下會倒）、摩擦力、重力、慣性（重卡車需要更長時間煞車）、滾動的球會繼續滾。這些對小孩是常識，但 AI 必須學習。</p>
<p>更重要的是，AI 必須能<strong>模擬其行動的後果</strong>。它怎麼知道自己的動作是否正確？必須在環境中模擬物理世界對其行動的反應。</p>
<p>這需要三台電腦：</p>
<ol>
<li><strong>訓練電腦</strong>：訓練 AI 模型</li>
<li><strong>推理電腦</strong>：在車輛、機器人、工廠等邊緣設備運行</li>
<li><strong>模擬電腦</strong>：這是 NVIDIA 最舒適的領域，模擬是 Physical AI 一切的基礎</li>
</ol>
<p>三台電腦上運行多個軟體堆疊：</p>
<ul>
<li><strong>Omniverse</strong>：數位孿生、物理模擬世界</li>
<li><strong>Cosmos</strong>：世界基礎模型（不是語言的基礎模型，是世界的基礎模型），對齊語言</li>
<li><strong>Groot</strong>：人形機器人模型</li>
<li><strong>Alpamayo</strong>：自駕車模型</li>
</ul>
<h3 id="26-cosmos把運算變成資料">2.6 Cosmos：把運算變成資料</h3>
<p>Physical AI 最大的挑戰是資料。網路上有大量文字，但沒有大量機器人資料。影片很多，但不足以涵蓋所需的多樣性和互動類型。</p>
<p>解法是<strong>合成資料生成</strong>——用模擬來創造訓練資料。NVIDIA Cosmos 就是這個魔法的核心。</p>
<p>Cosmos 是什麼？</p>
<ul>
<li>預訓練於網路規模的影片、真實駕駛和機器人資料、3D 模擬</li>
<li>學習了世界的統一表示，能對齊語言、圖像、3D 和動作</li>
<li>執行 Physical AI 技能：生成、推理、軌跡預測</li>
</ul>
<p>它能做什麼？</p>
<ul>
<li>從單張圖像生成逼真影片</li>
<li>從 3D 場景描述生成物理一致的動作</li>
<li>從駕駛遙測和感測器日誌生成環視影片</li>
<li>從規劃模擬器生成多攝影機環境</li>
<li>從場景提示生成邊緣案例</li>
<li>運行互動式閉迴圈模擬：當動作發生，世界回應</li>
</ul>
<p>最關鍵的功能：<strong>Cosmos 推理。</strong> 它能分析邊緣場景，拆解成熟悉的物理互動，推理接下來可能發生什麼。</p>
<p>黃仁勳說：「Cosmos 把運算變成資料，訓練自駕車處理長尾問題，訓練機器人適應每種場景。」</p>
<p>Cosmos 已經被下載數百萬次，全球都在使用，為 Physical AI 新時代做準備。</p>
<h3 id="27-alpamayo會思考的自駕車-ai">2.7 Alpamayo：會思考的自駕車 AI</h3>
<p>今日發布的 Alpamayo 是「世界第一個會思考、會推理的自駕車 AI」。</p>
<p>它的特點：</p>
<ul>
<li><strong>端到端訓練</strong>：從攝影機輸入直接到方向盤、煞車、油門輸出</li>
<li><strong>多重資料來源</strong>：人類示範駕駛 + Cosmos 生成的合成資料 + 數十萬個仔細標註的範例</li>
<li><strong>會解釋行動</strong>：不只執行，還會說明「我要做什麼」和「為什麼」</li>
<li><strong>推理能力</strong>：在每個場景都會推理即將採取的行動</li>
</ul>
<p>為什麼推理能力重要？因為自駕的「長尾問題」——不可能收集每個國家、每種情境、每個可能發生的事情。但如果能把未見過的場景分解成一堆熟悉的子場景，AI 就能推理出解決方案。</p>
<p>現場播放了一段 Alpamayo 的駕駛影片：全程一鏡到底、無人手介入，從「導航到目的地」到「你已抵達」，中間穿越各種交通情境，每一步都在畫面上顯示 AI 的推理過程。</p>
<h3 id="28-八年的垂直整合">2.8 八年的垂直整合</h3>
<p>黃仁勳回顧了 NVIDIA 在自駕車領域的八年旅程：</p>
<blockquote>
<p>「我們很早就推論，深度學習和 AI 將重新發明整個運算堆疊。如果我們要理解如何導航自己、如何引導產業走向這個新未來，我們必須學會建造整個堆疊。」</p>
</blockquote>
<p>AI 是五層蛋糕：</p>
<ol>
<li><strong>最底層</strong>：土地、電力、外殼（在機器人的情況下是車體）</li>
<li><strong>第二層</strong>：晶片（GPU、網路晶片、CPU）</li>
<li><strong>第三層</strong>：基礎設施（Omniverse、Cosmos）</li>
<li><strong>第四層</strong>：模型（Alpamayo）</li>
<li><strong>第五層</strong>：應用程式（Mercedes-Benz）</li>
</ol>
<p>NVIDIA 與 Mercedes-Benz 合作五年，建造了整個堆疊。這是 NVIDIA 第一個完整的垂直整合案例。</p>
<p>時程：</p>
<ul>
<li>2026 Q1：美國上路</li>
<li>2026 Q2：歐洲</li>
<li>2026 Q3-Q4：亞洲</li>
</ul>
<p>而且會持續更新，推出 Alpamayo 的新版本。</p>
<h3 id="29-雙重冗餘的安全架構">2.9 雙重冗餘的安全架構</h3>
<p>Alpamayo 雖然端到端訓練，技能驚人，但沒人能保證永遠完美安全。所以 NVIDIA 建立了雙重軟體堆疊：</p>
<ol>
<li><strong>Alpamayo 堆疊</strong>：端到端訓練，驚人的技能</li>
<li><strong>傳統 AV 堆疊</strong>：完全可追溯，花了六七年建造</li>
</ol>
<p>兩個堆疊互相鏡像。一個策略和安全評估器決定：如果我對這個情境非常有信心，讓 Alpamayo 處理；如果不太確定，切回傳統 AV 堆疊。</p>
<p>黃仁勳說：「我們是世界上唯一一輛同時運行兩個 AV 堆疊的車。所有安全系統都應該有多樣性和冗餘性。」</p>
<p>這套系統剛獲得 NCAP 評為「世界最安全的車」。每一行程式碼、晶片、系統都經過安全認證。</p>
<h3 id="210-機器人的時代">2.10 機器人的時代</h3>
<p>自駕車之後，下一個時代是機器人。黃仁勳邀請了一群機器人上台：四足機器人、人形機器人、送餐機器人、手術機器人、工業機器人&hellip;&hellip;</p>
<p>這些機器人都：</p>
<ul>
<li>內建 Jetson 電腦</li>
<li>在 Omniverse 的 Isaac Sim 和 Isaac Lab 模擬器中訓練</li>
<li>使用同樣的 Physical AI 技術</li>
</ul>
<p>現場展示了機器人在模擬器中學習的過程——這就是機器人學會成為機器人的方式。</p>
<p>合作夥伴包括：Neurobot、Agibot、LG、Caterpillar（最大的機器人）、Surf Robot（送餐到你家，連接 Uber Eats）、Agility、Boston Dynamics、手術機器人、Franca 的操作機器人、Universal Robotics&hellip;&hellip;</p>
<h3 id="211-晶片設計革命cadencesynopsyssiemens">2.11 晶片設計革命：Cadence、Synopsys、Siemens</h3>
<p>NVIDIA 的技術現在成熟到可以反過來革命化創造它的產業——晶片設計。</p>
<p>宣布與三大巨頭的整合：</p>
<p><strong>Cadence</strong>：</p>
<ul>
<li>CUDA-X 整合到所有模擬和求解器</li>
<li>NVIDIA Physical AI 用於不同的物理工廠和工廠模擬</li>
<li>AI 物理學整合</li>
<li>領先物理設計（佈局佈線）和仿真驗證</li>
</ul>
<p><strong>Synopsys</strong>：</p>
<ul>
<li>領先邏輯設計和 IP</li>
<li>同樣整合 NVIDIA 技術</li>
<li>未來會有 Agentic 晶片設計師和系統設計師，就像現在有 Agentic 軟體工程師一樣</li>
</ul>
<p><strong>Siemens</strong>（今日宣布）：</p>
<ul>
<li>整合 CUDA-X、Physical AI、Agentic AI、Nemo、Nemotron、Omniverse</li>
<li>涵蓋 EDA、CAE、數位孿生工具和平台</li>
<li>從設計模擬到生產營運的完整工業生命週期</li>
</ul>
<p>黃仁勳的願景：機器人會在電腦中被設計、在電腦中被製造、在電腦中被測試，「遠在你需要對抗重力之前」。製造工廠本身也會是巨大的機器人。</p>
<h3 id="212-vera-rubin下一代-ai-超級電腦">2.12 Vera Rubin：下一代 AI 超級電腦</h3>
<p>以發現暗物質的天文學家 Vera Rubin 命名，這是 NVIDIA 的下一代 AI 超級電腦。</p>
<p>為什麼需要它？AI 運算需求正在爆炸：</p>
<ul>
<li>模型每年增大 10 倍</li>
<li>Token 生成量每年增加 5 倍（因為 AI 現在會「思考」）</li>
<li>強化學習引入後訓練，運算量再爆炸</li>
<li>成本每年下降 10 倍（代表競爭有多激烈，大家都在搶下一個前沿）</li>
</ul>
<p><strong>問題是</strong>：摩爾定律放緩，電晶體數量只增加 1.6 倍。如果不做極致協同設計（extreme co-design），不可能跟上這種速度。</p>
<p>所以 NVIDIA 打破了自己的規則（通常每代只改一兩顆晶片），這次<strong>重新設計了全部六顆晶片</strong>：</p>
<table>
  <thead>
      <tr>
          <th>晶片</th>
          <th>關鍵突破</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Vera CPU</strong></td>
          <td>88 核心、176 執行緒（空間多執行緒），每瓦效能是業界最強 CPU 的 2 倍</td>
      </tr>
      <tr>
          <td><strong>Rubin GPU</strong></td>
          <td>浮點運算是 Blackwell 的 5 倍，但電晶體只多 1.6 倍</td>
      </tr>
      <tr>
          <td><strong>NVFP4 Tensor Core</strong></td>
          <td>革命性處理單元，動態調整精度和結構，不是簡單的 FP4 數字</td>
      </tr>
      <tr>
          <td><strong>NVLink 6 Switch</strong></td>
          <td>400Gbps SerDes，史上最快；單機架 240TB/s 頻寬（全球網路的 2 倍）</td>
      </tr>
      <tr>
          <td><strong>ConnectX 9</strong></td>
          <td>世界最好的 NIC，可程式化 RDMA，與 Vera CPU 協同設計</td>
      </tr>
      <tr>
          <td><strong>BlueField 4</strong></td>
          <td>虛擬化、安全、網路卸載，加上革命性的 KV Cache 管理</td>
      </tr>
      <tr>
          <td><strong>Spectrum X</strong></td>
          <td>矽光子 AI 乙太網路交換器，512 埠 × 200Gbps，雷射直接連接晶片</td>
      </tr>
  </tbody>
</table>
<h3 id="213-機架革命">2.13 機架革命</h3>
<p>舊的 NGX 機架：43 條線纜、6 條管線、組裝要 2 小時、80% 液冷
新的 Vera Rubin 機架：0 條線纜、2 條管線、組裝只要 5 分鐘、100% 液冷</p>
<p>單一機架規格：</p>
<ul>
<li>144 顆 Rubin GPU（每個 NVLink 72 節點有 72 顆，共 2 個節點）</li>
<li>2200 億個電晶體</li>
<li>重約 2 噸（因為忘了把水排掉，運來時是 2.5 噸）</li>
<li>背面有 2 英里的銅纜（5000 條銅纜），讓 NVLink 脊柱以 400Gbps 連接</li>
</ul>
<h3 id="214-bluefield-4-與-kv-cache-革命">2.14 BlueField 4 與 KV Cache 革命</h3>
<p>這是一個新類別的儲存系統，解決 AI 領域的重大痛點。</p>
<p>問題是什麼？AI 推理時，每生成一個 Token，GPU 就要讀取整個模型和整個工作記憶體（KV Cache），產生一個 Token，再存回去。隨著對話變長、模型變大、使用者變多，這個 KV Cache 會爆炸性成長。</p>
<p>原本放在 HBM 記憶體裡，不夠了。去年用 Grace CPU 的快速記憶體擴展，還是不夠。傳統解法是用北南網路傳到儲存系統，但大量 AI 同時運行時，網路會成為瓶頸。</p>
<p>解法：<strong>BlueField 4 + 機架內 KV Cache 儲存</strong></p>
<p>在機架內放入 KV Cache 儲存節點：</p>
<ul>
<li>每個 BlueField 4 後面有 150TB 記憶體</li>
<li>分配給每個 GPU 後，每個 GPU 額外獲得 16TB 上下文記憶體</li>
<li>用同樣的東西向流量、同樣的 200Gbps 資料率</li>
<li>比原本節點內的 1TB 擴展了 16 倍</li>
</ul>
<p>這對 AI 實驗室和雲端服務商來說是救命的——KV Cache 流量造成的網路壓力是他們最大的痛點之一。</p>
<h3 id="215-效能數據與系統特性">2.15 效能數據與系統特性</h3>
<p><strong>訓練吞吐量</strong>（10 兆參數模型，DeepSeek++ 規模，100 兆 Token，1 個月）：</p>
<ul>
<li>Blackwell（綠色）：需要較多系統</li>
<li>Rubin：吞吐量高得多，只需四分之一的系統數量</li>
</ul>
<p><strong>工廠吞吐量</strong>（每瓦效能，直接關係到資料中心營收）：</p>
<ul>
<li>Blackwell 比 Hopper 高 10 倍</li>
<li>Rubin 再高 10 倍</li>
</ul>
<p><strong>Token 成本</strong>：</p>
<ul>
<li>Rubin 約是 Blackwell 的十分之一</li>
</ul>
<p><strong>其他系統特性</strong>：</p>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>說明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>能源效率</td>
          <td>功耗翻倍，但冷卻水維持 45°C（不需冷水機），省下約 6% 全球資料中心電力</td>
      </tr>
      <tr>
          <td>機密運算</td>
          <td>全系統加密——傳輸中、靜態、運算中。每條匯流排（PCIe、NVLink、CPU-GPU）都加密</td>
      </tr>
      <tr>
          <td>電力平滑</td>
          <td>AI 工作負載會瞬間飆升 25% 電流（all-reduce 運算），現在有全系統電力平滑，不用過度配置</td>
      </tr>
  </tbody>
</table>
<p><strong>宣布</strong>：Vera Rubin 已進入量產。</p>
<hr>
<h2 id="第三部分觀點評論開放壟斷的終極形態">第三部分：觀點評論——開放壟斷的終極形態</h2>
<p>看完這整場活動，讓我分享幾個觀察。</p>
<h3 id="31-開放是最聰明的壟斷策略">3.1 「開放」是最聰明的壟斷策略</h3>
<p>NVIDIA 把所有模型開源、所有工具免費、所有架構公開。這看起來很慷慨，但仔細想想：</p>
<ul>
<li>開源模型吸引開發者 → 開發者在 NVIDIA 硬體上開發 → 生態鎖定</li>
<li>免費工具降低門檻 → 更多人進入 AI 領域 → 更多 GPU 需求</li>
<li>公開架構讓合作夥伴能整合 → 整合越深，切換成本越高</li>
</ul>
<p>這跟微軟當年的策略異曲同工：Windows 本身不是利潤中心，但它鎖定了整個 PC 生態圈。NVIDIA 的 CUDA 也是同樣邏輯，現在更進一步——連模型、工具、架構都開放，但硬體依賴越來越深。</p>
<h3 id="32-垂直整合的深度前所未見">3.2 垂直整合的深度前所未見</h3>
<p>NVIDIA 現在的觸角：</p>
<ul>
<li><strong>晶片層</strong>：GPU、CPU、NIC、DPU、Switch，全部自己設計</li>
<li><strong>系統層</strong>：整機、機架、冷卻、電源，全部自己規格</li>
<li><strong>基礎設施層</strong>：Omniverse（模擬）、Cosmos（世界模型）、NEMO（工具鏈）</li>
<li><strong>模型層</strong>：語言、視覺、物理、生物、機器人、自駕，全領域</li>
<li><strong>應用層</strong>：與 Palantir、ServiceNow、Snowflake 等深度整合</li>
</ul>
<p>這不是賣 GPU 的公司，這是要吃下整個 AI 產業鏈的公司。</p>
<h3 id="33-競爭者的困境">3.3 競爭者的困境</h3>
<p>AMD 在同一個 CES 發布了 MI-455X 和 Helios 機架。但問題是：</p>
<ul>
<li>NVIDIA 有 CUDA 生態鎖定（十幾年的開發者累積）</li>
<li>NVIDIA 有完整的軟體堆疊（從訓練到推理到模擬）</li>
<li>NVIDIA 有開放模型吸引開發者</li>
<li>NVIDIA 有垂直整合的系統優勢</li>
</ul>
<p>AMD 賣的是「晶片」，NVIDIA 賣的是「平台」。這是完全不同層次的競爭。</p>
<p>Intel、Google TPU、Amazon Trainium 面臨同樣的困境。硬體規格可以追上，但生態圈很難複製。</p>
<h3 id="34-physical-ai-是下一個戰場">3.4 Physical AI 是下一個戰場</h3>
<p>這場演講最重要的訊息可能是：<strong>NVIDIA 已經在 Physical AI 領域建立了完整的護城河。</strong></p>
<p>Cosmos 是世界模型，Omniverse 是模擬平台，Alpamayo 和 Groot 是應用模型。訓練、推理、模擬三台電腦的架構已經定義好了。</p>
<p>當機器人、自駕車、工業自動化開始大規模部署，這套架構很可能成為事實標準。就像 CUDA 在 AI 訓練領域的地位一樣。</p>
<h3 id="35-台灣的角色">3.5 台灣的角色</h3>
<p>值得注意的是，NVIDIA 的硬體製造高度依賴台灣：</p>
<ul>
<li>晶片：台積電製造</li>
<li>系統：鴻海、廣達、緯創等組裝</li>
<li>供應鏈：數千家台灣零組件廠商</li>
</ul>
<p>這既是機會也是風險。機會是台灣廠商深度參與 AI 革命；風險是過度依賴單一客戶（NVIDIA 佔台積電先進製程產能的很大比例）。</p>
<hr>
<h2 id="完整發布清單">完整發布清單</h2>
<table>
  <thead>
      <tr>
          <th>類別</th>
          <th>名稱</th>
          <th>說明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>世界模型</strong></td>
          <td>Cosmos</td>
          <td>開放的世界基礎模型，理解世界運作方式，對齊語言</td>
      </tr>
      <tr>
          <td><strong>自駕 AI</strong></td>
          <td>Alpamayo</td>
          <td>首款會思考、會推理的自駕 AI，端到端訓練，今日開源</td>
      </tr>
      <tr>
          <td><strong>機器人</strong></td>
          <td>Groot</td>
          <td>人形機器人模型，整合關節、移動、行走</td>
      </tr>
      <tr>
          <td><strong>語言模型</strong></td>
          <td>Nemotron 3</td>
          <td>混合 Transformer-SSM 架構，可快速或深度推理</td>
      </tr>
      <tr>
          <td><strong>生物模型</strong></td>
          <td>La Proteina, OpenFold3, Evo 2</td>
          <td>蛋白質合成、結構理解、細胞表示</td>
      </tr>
      <tr>
          <td><strong>物理模型</strong></td>
          <td>Earth 2, ForecastNet, CoreDiv</td>
          <td>物理定律理解、天氣預測</td>
      </tr>
      <tr>
          <td><strong>CPU</strong></td>
          <td>Vera</td>
          <td>88 核心、176 執行緒，每瓦效能業界最強 2 倍</td>
      </tr>
      <tr>
          <td><strong>GPU</strong></td>
          <td>Rubin</td>
          <td>5 倍 Blackwell 浮點運算</td>
      </tr>
      <tr>
          <td><strong>Tensor Core</strong></td>
          <td>NVFP4</td>
          <td>動態精度調整的革命性處理單元</td>
      </tr>
      <tr>
          <td><strong>NIC</strong></td>
          <td>ConnectX 9</td>
          <td>可程式化 RDMA，與 Vera 協同設計</td>
      </tr>
      <tr>
          <td><strong>DPU</strong></td>
          <td>BlueField 4</td>
          <td>虛擬化、安全、KV Cache 管理</td>
      </tr>
      <tr>
          <td><strong>Switch</strong></td>
          <td>NVLink 6</td>
          <td>400Gbps SerDes，單機架 240TB/s</td>
      </tr>
      <tr>
          <td><strong>乙太網路</strong></td>
          <td>Spectrum X (矽光子)</td>
          <td>512 埠 × 200Gbps，雷射直連晶片</td>
      </tr>
      <tr>
          <td><strong>超級電腦</strong></td>
          <td>Vera Rubin</td>
          <td>下一代 AI 超級電腦，已進入量產</td>
      </tr>
      <tr>
          <td><strong>合作夥伴</strong></td>
          <td>Siemens</td>
          <td>CUDA-X、Physical AI、Omniverse 整合到工業平台</td>
      </tr>
      <tr>
          <td></td>
          <td>Cadence, Synopsys</td>
          <td>AI 整合到 EDA 工具</td>
      </tr>
      <tr>
          <td></td>
          <td>Mercedes-Benz</td>
          <td>Alpamayo 自駕系統，2026 Q1 美國上路</td>
      </tr>
      <tr>
          <td></td>
          <td>Palantir, ServiceNow, Snowflake</td>
          <td>企業 AI 平台整合</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="結語帝國的輪廓">結語：帝國的輪廓</h2>
<p>黃仁勳用一句話結束演講：</p>
<blockquote>
<p>「我們的工作是創造整個堆疊，讓你們能為世界創造令人難以置信的應用。」</p>
</blockquote>
<p>這句話聽起來謙虛，但背後的意涵是：<strong>NVIDIA 要成為 AI 時代的基礎設施供應商，就像電力公司、自來水公司一樣不可或缺。</strong></p>
<p>不同的是，電力公司只賣電，NVIDIA 不只賣算力，還定義了你怎麼用這些算力。從晶片到系統到模型到工具到架構，全部由 NVIDIA 定義，全部開放給你用——只要你跑在 NVIDIA 硬體上。</p>
<p>這是一種新型態的壟斷：<strong>開放的壟斷、生態的壟斷、標準的壟斷。</strong></p>
<p>CES 2026 的這一天，我們見證的不只是產品發布，而是一個帝國的輪廓逐漸清晰。</p>

        </div>
        <div class="post_extra mb-2">
          
<div class="copy" data-before="分享故事" data-after="已複製">
  <svg class="icon">
    <use xlink:href="#copy"></use>
  </svg>
</div>
        </div>
        <div>
        
        </div>
      </div>
    </div>
    <a href=https://bnextmedia.github.io/AINEXT/ class="post_nav"><span class="post_next">Latest Posts
      <svg class="icon icon_scale">
        <use xlink:href="#double-arrow"></use>
      </svg>
    </span></a>
  </div>

    </main>
    <footer class="footer wrap pale">
  <p>&copy;&nbsp;<span class="year"></span>&nbsp;AINEXT</p>
  <p class="attribution upcase">由 <a href = 'https://bnextmedia.github.io/AINEXT/' target = '_blank' title = '領英個人檔案' rel = 'nonopener'>AINEXT</a> 設計</p>
</footer>


<script src="https://bnextmedia.github.io/AINEXT/js/index.min.0c2fb80a1ade817d7387f0de8ee061e7de6878a65a420dc61a6e0d0b3bb765c4c0394caefa7c40b16d39c7d74283b3cab364aa109f58cad99d149ec813f930c8.js"></script>

    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
