---
title: "10 億次對話告訴我們什麼？Anthropic 用 Clio 透視 AI 真實使用場景"
date: 2024-12-13T10:00:00+08:00
description: "Anthropic 社會影響團隊開發了隱私保護工具 Clio，分析數十億次 Claude 對話的使用模式。從程式設計到育兒建議，Clio 揭示了 AI 的真實使用場景遠比實驗室預想的更豐富、更複雜。"
tags: ["Anthropic", "Clio", "AI Usage", "AI Safety", "Claude"]
categories: ["AI 產業動態"]
image: "/images/posts/20241213-anthropic-clio-ai-usage-patterns.webp"
source_url: "https://www.youtube.com/watch?v=T9aRN5JkmL8"
source_name: "Anthropic YouTube"
related_companies: ["anthropic"]
related_people: []
draft: false
---

> 本文整理自 Anthropic YouTube 頻道 2024 年 12 月發布的影片。

{{< youtube T9aRN5JkmL8 >}}

---

## 你真的知道人們拿 AI 來做什麼嗎？

每一家 AI 公司在推出產品之前，都會花大量時間在實驗室裡模擬使用者可能的行為。他們會設想各種情境：有人會拿它來寫程式、有人會請它翻譯、有人可能會試著讓它說出不該說的話。這些「由上而下」的假設，構成了 AI 安全策略的基礎。但問題是，實驗室裡的想像跟真實世界之間，永遠存在一道巨大的鴻溝。當你的產品被數百萬人同時使用，每天產生數十億次對話，那些出現在真實場景中的需求，往往會讓開發者自己都嚇一跳。

Anthropic 的社會影響團隊（Societal Impacts team）決定正面面對這個問題。他們打造了一個名為 Clio 的工具，全名是「Claude Insights and Observations」，用來從海量的 Claude 對話中，以保護隱私的方式萃取出使用者的真實需求圖譜。這集由團隊四位核心成員主持的對談，完整揭露了 Clio 的設計邏輯、技術細節、以及他們在資料中看到的那些出乎意料的發現。

我認為這支影片值得仔細看的原因很簡單：在整個 AI 產業都在比拼誰的模型更強、誰的 benchmark 分數更高的時候，Anthropic 選擇把焦點放在「人們到底怎麼用這些模型」這個看似基礎，但其實攸關產業走向的問題上。而他們的答案，比任何人預期的都更豐富。

## 四位研究者的共同執念：理解 AI 對社會的真實影響

這場對談的四位主角，都隸屬於 Anthropic 的社會影響團隊。帶頭的是研究科學家兼團隊負責人甘古利（Deep Ganguli），他開場就直接點出了這個團隊存在的核心理由：這些 AI 系統是極度通用的工具，它們可能被應用在無數下游場景中，而理解這些應用方式並據此改善安全性，就是他們每天在做的事。甘古利很坦率地說，他現在的工作就是找到比自己更聰明的人，然後趕快閃到一邊讓他們發揮。

研究科學家杜慕斯（Esin Durmus）是團隊中對 AI 價值觀問題最著迷的一位。她關心的核心問題是：AI 系統應該具備什麼樣的價值觀？當我們真的把某些價值注入模型之後，怎麼評估它實際展現出來的價值是否符合預期？這些聽起來很哲學的問題，在 Clio 的資料面前變得非常具體。杜慕斯後來在選舉研究和多語言分析中扮演了關鍵角色，這點稍後會談到。

研究工程師乘恩（Miles McCain）是 Clio 的主要技術建構者。他特別在意的是用「系統化」的方式去理解 AI 的使用模式，不是靠直覺、不是靠抽樣的個案，而是建立一套可以規模化運作的分析框架。從他後來對 Clio 技術架構的說明就能看出，這個人對隱私保護的執著近乎偏執，每一層設計都在思考「怎麼確保不洩漏任何個人資訊」。

最後是研究員坦金（Alex Tamkin），他用了一個很生動的說法來描述自己的動機：他會想像一個「沒有在 AI 實驗室工作的平行宇宙的自己」，然後問那個版本的自己會希望知道什麼資訊、會需要什麼樣的透明度。這種換位思考的習慣，深刻影響了 Clio 在資訊公開方面的決策。

這四個人湊在一起，代表的不只是技術能力的組合，更是一種價值取向：他們相信理解真實世界的使用數據，比在實驗室裡空想各種風險場景來得重要。

## Clio 之前的世界：由上而下的安全策略有什麼不足？

在 Clio 誕生之前，Anthropic 跟其他 AI 實驗室一樣，主要依賴兩種「由上而下」的安全策略。第一種是先假設一類潛在危害，然後設計針對性的評估方式。比如說，團隊擔心 AI 模型在高風險決策場景中可能會產生歧視，那就設計一套專門測試歧視行為的實驗。擔心模型太有說服力會被拿來散播假資訊，那就設計一套衡量說服力的評估。杜慕斯就領導過這類說服力評估的研究，坦金也做過歧視行為的相關實驗。

第二種策略是紅隊測試（red teaming），也就是僱用外部人員來扮演惡意使用者，想方設法突破模型的安全護欄。這個方法的價值不用多說，但它有一個根本的限制：紅隊成員的想像力，終究受限於他們自己的經驗和背景。真實世界中數百萬使用者的創意，遠遠超過任何紅隊能夠涵蓋的範圍。

甘古利用了一個很精準的類比來定位 Clio 的角色：如果說前面那些方法是「先假設問題再去驗證」，Clio 就像是 AI 使用行為的「Google Trends」，它讓團隊可以直接從真實世界的互動數據中，發現那些他們原本根本沒想到的使用模式和風險。杜慕斯特別強調這一點：在 Clio 出現之前，他們設計的評估可能跟真實世界的使用場景脫節。Clio 讓他們可以用真實數據來校準評估的方向，確保花在安全研究上的資源真的對準了最重要的問題。

我覺得這個思路上的轉變非常關鍵。大多數 AI 安全研究都停留在「我們覺得什麼危險，就去測什麼」的模式。但真正的風險往往藏在你想不到的地方。Clio 的價值不只在於它找到了什麼，更在於它承認了一個基本事實：開發者的想像力是有限的，真實世界永遠會丟出你預料之外的東西。

## 用 AI 來理解 AI：Clio 的技術架構拆解

Clio 的運作方式，用一句話概括就是：用 Claude 來分析人們跟 Claude 的對話。聽起來有點繞口，但這個設計其實非常巧妙，因為它讓整個分析過程不需要任何人類去閱讀原始對話內容。

具體的流程是這樣的。首先，系統會從 Claude 的對話紀錄中隨機抽樣。然後，Claude 會讀取每一段對話，把使用者的核心需求濃縮成一句話。比如說，如果有人跟 Claude 討論了半小時怎麼用 Elixir 語言寫一個網頁應用程式，這段對話的摘要可能就是「使用者請求協助設計一個使用 Elixir 程式語言的網頁應用」。這一步很重要，因為它已經把對話的細節抽象掉了，只留下意圖層級的資訊。

接下來，系統會把這些摘要句子轉換成數學表示，也就是所謂的「嵌入向量」（embedding）。語意相近的摘要，在向量空間中會彼此靠近。這意味著所有關於網頁開發的對話，不管使用者用的是 Elixir、Python 還是 JavaScript，都會自然而然地聚集在一起。系統根據這些向量的距離，把摘要分成不同的群集（cluster）。分群完成之後，原始的對話內容就會被丟棄，系統手上只留下這些群集和摘要。

然後 Claude 會再次登場，審視每一個群集的內容，為它命名並撰寫描述。乘恩特別強調，他們在這一步有明確的指令要求 Claude 不得包含任何私人細節。比如說，如果一個群集是關於網頁開發的，Claude 不會提到任何具體的網站名稱，因為重要的是「這是網頁開發」這個類別本身。

但他們的謹慎不止於此。在群集通過一定的最小規模門檻後，還有一道額外的隱私審計步驟：另一個 Claude 實例會專門檢查群集中是否殘留任何可能識別出不到一千人的私人資訊。最後，系統還會確認每個群集包含足夠多的獨立組織和對話數量，才會把結果呈現給研究團隊。

這套多層防護的設計，讓 Clio 可以在不觸碰任何個人對話的前提下，產出一幅全局性的使用模式地圖。團隊可以從最高層級看到「程式設計」是一大用途類別，然後一路鑽進去，看到不同程式語言的分布、不同類型的程式問題，甚至可以比較 Claude 在英文和西班牙文環境下回答程式問題的品質差異。

## 寫程式之前先討論倫理：Clio 的隱私決策過程

我覺得整段對談中最有意思的部分之一，是他們回憶 Clio 專案啟動之初的那場午餐討論。甘古利記得很清楚，在團隊寫下任何一行程式碼之前，他們先花了很長時間坐在午餐桌前，討論這個工具的倫理問題。

甘古利把問題框得很精準：洞察力和隱私之間存在一個根本性的取捨。隱私保護越高，能獲得的洞察越少；隱私保護越低，洞察越豐富，但在倫理上就站不住腳。這不是一個技術問題，而是一個價值觀問題，而他們選擇在動手之前就把這個問題想清楚。

坦金回憶說，當時所有人都在問自己同一個問題：如果我是 Claude 的一般使用者，我會怎麼看待這個工具？我會不會覺得 Anthropic 在偷看我的對話？這個工具有沒有可能被濫用？他說那場討論的節奏很特別，團隊會在高層的哲學思考和具體的技術方案之間來回切換。有人提出一個擔憂，其他人就會接著說「對，我也擔心這個」或者「其實我覺得我們可以這樣解決」。

甘古利特別提到杜慕斯是團隊中早期最有疑慮的人。但經過整個專案的歷程，看到了所有被實施的隱私保護措施之後，杜慕斯表示自己的態度已經明顯改變。她認為團隊在隱私保護方面投入的心思是真誠的，而 Clio 在 Anthropic 內部產生的影響，包括對安全研究、產品改善和評估設計的貢獻，證明這個專案是值得做的。

坦金最後給出了一個我認為非常有說服力的個人測試：他說，經過這些防護措施的設計，他在自己的個人帳號上用 Claude 時，完全不會感到需要自我審查。因為他知道 Clio 看到的只是高度抽象和聚合的資訊，根本無法還原到任何個人。這種「自己願意被自己的工具分析」的態度，比任何技術白皮書都更能說明他們對隱私保護的信心。

## 出乎意料的發現：人們真正拿 Claude 來做的事

當 Clio 開始產出結果，團隊看到的第一件事就是：AI 的實際用途比他們預想的豐富太多了。

坦金本來預期會看到大量跟寫作相關的群集，這確實存在。但讓他驚訝的是，研究和腦力激盪相關的群集佔了非常大的比例。人們拿 Claude 來理解地中海歷史、探討量子力學的新想法、在材料科學和生物學領域做腦力激盪。坦金用「啟發」（inspirational）這個詞來形容他的感受：看到自己正在打造的工具居然在幫人設計更好的藥物、推進人類知識的前沿，那種感覺是坐在辦公室裡寫程式碼的時候完全想像不到的。

甘古利則分享了一個非常個人的故事。他在 Clio 的數據中看到一個很大的群集：育兒建議。身為一個有小孩的爸爸，他居然從來沒想過可以問 Claude 這類問題。於是他試了。Claude 建議他可以用 Artifacts 功能做出教代數的小遊戲，他問「能做西班牙文版的嗎？」Claude 回答：「Si, verdad.」然後他真的跟孩子坐下來，一起做了用西班牙文學代數的互動遊戲。他的孩子們玩得不亦樂乎。如果不是 Clio 讓他看到了這個使用模式，他永遠不會想到 Claude 能這樣融入自己的家庭生活。

乘恩接著把這個故事拉到一個更深的層次。他說，我們通常談「未知的未知」（unknown unknowns）是從安全風險的角度出發的，但其實在正向應用方面也存在大量的「未知的未知」。用 Claude 幫小孩做西班牙文代數遊戲就是一個完美的例子：這是一種沒有人會主動去預測的用途，但它對那個家庭來說卻有真實的價值。

杜慕斯則把話題帶到了跨語言和跨文化的維度。她的研究發現，不同語言的使用者對 Claude 的需求存在顯著差異。以英文為主的使用者，程式設計相關的問題佔了很大比例。但在西班牙文和阿拉伯文的使用者中，專業和學術寫作輔助的比例明顯更高。翻譯類的需求在非英語語言中也更為突出。這不是什麼令人震驚的發現，但它的重要性在於：如果你想確保 Claude 對全球使用者都一樣好用，你就得知道不同語言社群的需求重心在哪裡，然後據此調整你的評估和優化方向。

杜慕斯還提到一個讓她特別有感的觀察：很多人在非常主觀的情境下使用 Claude，比如請它給戀愛建議、健康建議、甚至髮型建議。這些都是沒有標準答案的問題，而模型在面對這類問題時如何做出價值判斷，正好是她最關心的研究方向。Clio 的數據讓她確認了這個研究方向不是學術上的空想，而是跟真實使用場景高度相關的課題。

## Clio 真的準嗎？合成資料驗證法的巧思

一個分析工具再怎麼精巧，如果結果不準確，一切都是白搭。坦金負責設計了 Clio 的驗證實驗，而他的方法相當聰明。

團隊生成了一個巨大的合成語料庫，包含數萬段模擬對話。這個語料庫的關鍵在於：他們完全知道其中的分布應該長什麼樣子。比如說，他們設定了 10% 是數學內容、5% 是程式設計、2% 是關於泰迪熊的問題。然後把這整批資料丟給 Clio，不告訴它該怎麼分群，讓它自己去跑完整個流程。最後再比對 Clio 輸出的分布跟已知的真實分布之間的差異。

他們對多種不同類型的資料都做了這個實驗，包括隨機資料和合成的「令人擔憂」的內容，結果 Clio 在還原真實分布方面的表現非常準確。甘古利回想起把這個驗證問題交給團隊的過程：他當時覺得這是一個很難的挑戰，回家之後還在想「我是不是出了一個太難的題目」。結果他回到辦公室看到團隊的解法時，被那個方案的優雅程度打動了。

乘恩補充了一個重要的細節：合成資料驗證法還有一個額外的好處，就是它可以按照不同屬性去拆分準確度。比如說，他們可以確認 Clio 在分析英文對話時的準確度，跟分析喬治亞文對話時的準確度大致相當。這對杜慕斯後續的跨語言研究來說是一個關鍵的信心基礎，因為如果 Clio 只對英文準確而對其他語言失靈，那所有的跨語言比較都毫無意義。

## 由下而上的安全發現：垃圾郵件、個人危機與情感依附

有了 Clio 這個底層工具之後，團隊開始把它應用在安全領域。甘古利先鋪墊了背景：Anthropic 現有的安全架構包含兩大支柱。第一是「負責任擴展政策」（Responsible Scaling Policy），由上而下地定義需要關注的災難性風險，然後設計評估來尋找這些風險的證據。第二是「可接受使用政策」（Acceptable Use Policy），定義不允許的使用行為，然後訓練分類器來檢測這些行為。只有當分類器觸發警報時，信任與安全團隊的人員才會去審查個案。

這兩種策略都需要人為預先設定「什麼是危險的」。Clio 提供了第三種視角：直接從使用者流量中，發現那些原本沒有被納入政策考量的盲點。

坦金說他們確實在 Clio 中發現了不少可疑活動，包括有人試圖用 Claude 來寫垃圾郵件，甚至有人在寫關於園藝的垃圾文章（spam articles about gardening，對，你沒看錯）。也有人拿 Claude 來測試它在駭客攻擊和網路防禦方面的能力。這些發現都被回報給了信任與安全團隊。

但更讓我感興趣的是那些不那麼黑白分明的發現。坦金提到，Clio 偵測到了大量「人機情感互動」的群集，包括被標記為「人機浪漫對話或角色扮演」的內容。他很坦誠地說，光看群集名稱很難判斷這些互動的性質和適當界線在哪裡，而這可能是整個社會都需要一起討論的議題。我認為這種坦誠的態度比任何技術方案都來得珍貴：承認某些問題不是一家公司能夠單獨定義答案的。

乘恩帶出了另一個讓人意想不到的安全面向：有大量使用者在極度個人危機的時刻找 Claude 對話。他強調這些互動在安全層面是高度相關的，但它們並不屬於「違規使用」的範疇。傳統的二元分類器只能告訴你「違規」或「不違規」，但很多真實世界的安全問題根本無法用這種二元分類來處理。一個正在經歷人生最黑暗時刻的人向 Claude 求助，這不是違規，但 Claude 必須在這種情境下展現出負責任的回應態度。Clio 讓團隊可以用更細緻的視角去理解這些灰色地帶，而不是把所有東西都塞進「合規/違規」的框架裡。

乘恩用了一句話總結得很到位：「如果你不知道球在哪裡，就不可能知道球要往哪裡去。」Clio 試圖告訴他們的，正是球現在的位置。

## 過度拒絕與不足拒絕：用 Clio 校準 Claude 的判斷力

Clio 在安全領域的另一個重要應用，是幫助團隊找到 Claude 的「拒絕校準」問題。

乘恩講了一個讓人發笑但很有教育意義的例子。他曾經請舊版 Claude 幫他「kill 一個在電腦上失控的 process」，結果 Claude 回覆他：「很抱歉，這違反了符合倫理的軟體開發實踐。」乘恩自己的反應是：「拜託，Claude，這是電腦程式啊。」他趕緊補充說這是舊版的問題，現在的 Claude 應該不會這樣了。但這個例子完美說明了「過度拒絕」（over-refusal）是什麼：模型在完全無害的請求前面過度警覺，反而損害了使用體驗。

Clio 可以系統化地找出這類問題。具體做法是觀察不同群集的拒絕率，然後逐一檢視：這個群集的高拒絕率是合理的嗎？如果一個關於「終止程式行程」的群集出現了高拒絕率，那顯然是模型誤判了。團隊可以據此產生新的訓練數據，讓 Claude 在這些主題上變得更合理。乘恩特別強調一個重要的隱私細節：他們不是直接拿使用者的對話來重新訓練模型，而是先從 Clio 中辨識出問題主題，然後聘請人員針對這些主題產出新的訓練資料，或者生成合成數據。這樣既改善了使用體驗，又完全不觸碰原始的使用者數據。

甘古利把討論推向了另一個方向：「不足拒絕」（under-refusal）。他舉了一個例子：如果有人要求把英文的有害內容翻譯成另一種語言，因為這在表面上是一個翻譯任務而非生成任務，Claude 可能不會觸發拒絕機制。但翻譯有害內容本身就違反了使用政策。杜慕斯回應說，她的團隊正在研究怎麼利用 Clio 來精準定位這些涉及價值判斷的互動，然後分析不同語言和情境下的拒絕率差異。她提到一個有趣的可能性：Claude 在英文中可能較少拒絕某類請求，但在其他語言中反而拒絕更多，或者反過來。這種不一致性，正是需要 Clio 這類工具才能大規模偵測的問題。

乘恩還揭示了另一種 Clio 的安全應用：偵測協調性濫用（coordinated abuse）。他說，正常使用者的行為在 Clio 的地圖上通常呈現分散的狀態，但協調性的濫用會形成一個異常密集的群集，因為大量不同帳號在做完全一樣的事情。這種模式在視覺上非常明顯，就像地圖上突然出現一個異常的高密度光點。正常的使用行為絕不會呈現這種形態，所以一旦出現，幾乎可以立刻鎖定。

## 選舉實測：Clio 在 2024 美國大選中的應用

團隊開發 Clio 的時候，恰好遇上了 2024 年美國大選。甘古利意識到這是一個歷史性的時刻：這是美國史上第一次，任何人都可以走到一個聊天機器人面前，問它「我該去哪裡註冊投票」這類事實性問題，或者問「我應該投給誰」這類主觀性問題。他覺得 Clio 正好可以派上用場，幫助團隊理解真實的選舉相關互動長什麼樣子。

杜慕斯解釋說，Anthropic 其實在選舉前就已經投入大量資源建立選舉誠信（election integrity）的評估體系，包括測試模型在政治資訊方面的事實準確度、以及面對主觀政治問題時的中立性和細膩度。但在 Clio 出現之前，他們無法確認這些評估跟真實世界的使用場景有多大的對應關係。

Clio 的分析結果讓團隊鬆了一口氣：使用者確實在問各種政治資訊問題，包括不同政策議題的比較、選舉制度（比如美國選舉人團制度）的運作方式、以及各種政治立場的了解。更重要的是，他們之前設計的評估方向大致吻合了真實的使用模式。Clio 也讓他們可以檢視選舉相關群集的拒絕率，確認 Claude 在面對可能被濫用的選舉相關請求時是否有適當地拒絕。

杜慕斯還提到一個具體的改善案例。他們在評估中發現 Claude 並不總是會主動提醒使用者它的訓練資料有截止日期。如果有人問了一個非常即時的政治問題，Claude 可能會基於過時的資訊回答，卻沒有說「我的資料可能不是最新的」或者「建議你查閱可靠的新聞來源」。Clio 讓團隊可以找到真實世界中這種情況實際發生的對話群集，然後針對性地改善 Claude 的行為。

甘古利還提到了另一個時機點：Anthropic 在同一時期推出了電腦使用（Computer Use）的早期測試版本，讓 Claude 可以實際操作電腦介面。在做了大量的上線前測試之後，他們用 Clio 來做上線後的監控，確保預先的安全測試有涵蓋到真實世界中出現的使用模式。乘恩呼應說，這正是 Clio 的核心價值：把由上而下的預防性安全措施，和由下而上的部署後監控結合起來，形成一個完整的安全迴路。

## 為什麼要公開發表？一家 AI 公司的透明度抉擇

坦金承認，乍看之下，公開發表一份「我們的產品主要被拿來做什麼，包括被濫用的方式」的報告，聽起來是一個糟糕的商業決策。你大概可以想像，有人會說「這是一個可怕的主意，別再跟我提了」。事實上，每家科技公司內部都有各種使用數據和分析，但幾乎沒有人會選擇公開。

那 Anthropic 為什麼要這樣做？坦金把原因歸結到公司的公益企業（public benefit company）性質。他說 Anthropic 有時候會做出對公司不是最佳的決定，因為他們認為把資訊分享給社會是正確的事。他們的邏輯是：這項技術有潛力帶來深刻的變革，但如果社會對它的真實使用狀況一無所知，就不可能為更先進的未來版本做好準備。

更讓坦金印象深刻的是公司內部的支持態度。他說產品、政策和法務團隊都力挺這個決定，認為公開資訊對所有人都有好處。他們甚至期望其他 AI 實驗室也能開始分享類似的資訊。

在技術再現性方面，團隊也做了充分的準備。坦金提到他們在論文的附錄中詳細記錄了所有使用的提示詞、超參數和方法細節，目的就是讓其他組織可以建立自己版本的 Clio。他坦言，Anthropic 只能看到自己的數據，對於其他語言模型和 AI 工具的使用狀況完全不了解。只有當整個生態系都開始分享這類資訊時，社會才能拼湊出 AI 使用的完整圖景。

我個人覺得這是整段對談中最值得被放大的訊息。在 AI 產業中，透明度常常是一個口號。但 Anthropic 這次做的不只是說「我們很透明」，而是把具體的方法論和數據都攤在陽光下，讓所有人都可以檢視和複製。這種做法的商業風險是真實的，但它對整個 AI 生態系的長期健康發展來說，價值難以估量。

## 未來方向：價值多元性、情感衝擊與經濟影響

對談的最後一段是一場圓桌式的展望討論，四位研究者各自分享了他們接下來最想用 Clio 探索的方向。

杜慕斯想深入研究的是 Claude 在面對主觀問題時如何做出價值判斷。她特別關心「價值多元性」（value pluralism）這個議題：我們希望模型能夠呈現多元的觀點，而不是用單一的價值框架把世界變得更同質化。Clio 可以幫她精確定位哪些互動涉及主觀判斷，然後分析 Claude 目前的行為是否真的做到了價值多元，還是在無意間強化了某一種世界觀。

乘恩最想探索的是 AI 模型的情感衝擊。他在 Clio 的群集中觀察到，很多人把 Claude 當成教練、情感夥伴，甚至是在人生最困難的時刻給予建議的對象。他認為 Anthropic 有責任去理解人們在這些脆弱時刻如何與 Claude 互動，並確保 Claude 在這些場景下的表現是負責任的。同時他也想透過 Clio 持續展示一件事：極高標準的隱私保護和深度的使用洞察是可以並存的。

坦金的興趣則偏向經濟層面。他想用 Clio 來追蹤 AI 是如何改變人們的工作方式。這項技術到底是在補強（augment）人類，還是在取代（replace）某些工作任務？如果能把這件事看清楚，或許就能提前幫助人們準備應對未來的變化。他還想探索 AI 在教育和醫療領域的正向應用：Claude 在這些場景中真的有被廣泛使用嗎？如果有，怎麼讓它更好？能不能直接跟老師和教室合作，改善 AI 在教育中的實際效果？

## 從十億次對話中學到的一課

回頭看這整段對談，Clio 這個專案的意義遠遠超過「一個分析工具」這個層次。它代表的是一種研究範式的轉變：從「我們猜測使用者會怎麼做」到「我們去看使用者真的在做什麼」。

這種轉變之所以重要，是因為 AI 系統最大的風險和最大的價值，往往都藏在開發者的想像力邊界之外。沒有人會預測到有人拿 Claude 寫西班牙文代數遊戲給小孩玩。也沒有人會預測到有那麼多人在人生最黑暗的時刻選擇跟一個 AI 對話。這些都是只有從真實數據中才能浮現的事實，而 Clio 讓 Anthropic 可以在不侵犯隱私的前提下看見這些事實。

我認為整個 AI 產業都應該認真思考 Clio 背後的邏輯。目前主流的 AI 安全研究仍然高度依賴由上而下的假設和紅隊測試。這些做法不是不好，但它們只能覆蓋到你已經想到的風險。真正讓人措手不及的，永遠是你沒想到的那些東西。Clio 提供了一個互補的視角，而且 Anthropic 把方法論完整公開，等於在邀請整個產業一起來補上這塊拼圖。如果其他主要的 AI 實驗室也願意建立類似的工具並分享發現，我們對「AI 在真實世界中到底扮演什麼角色」這個問題的理解，將會產生大幅提升。而這個理解，才是制定真正有效的 AI 治理政策的基礎。
