---
title: "寫出 AI 聖經的人，為什麼說大型語言模型是死胡同？"
date: 2026-02-06T10:00:00+08:00
description: "強化學習之父理察．薩頓在 2019 年寫下被 AI 圈奉為聖經的〈苦澀的教訓〉，主張算力勝過人類知識。但當 LLM 陣營用這篇文章為 scaling 背書時，薩頓本人卻站出來說：你們搞錯了。這場教主與信徒的分裂，揭示了 AI 發展最深層的路線之爭。"
tags: ["Richard Sutton", "The Bitter Lesson", "強化學習", "Scaling Laws", "AI 經典"]
categories: ["AI 技術前沿"]
image: "/images/posts/20260206-the-bitter-lesson.webp"
source_url: "http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
source_name: "Rich Sutton's Blog"
related_companies: ["openai", "google-deepmind"]
related_people: ["richard-sutton"]
draft: false
---

![封面圖](/images/posts/20260206-the-bitter-lesson.webp)

---

## AI 圈最諷刺的一幕

2025 年 9 月，強化學習共同創始人、剛拿到 2024 年圖靈獎的理察．薩頓（Richard Sutton）坐在 Dwarkesh Patel 的 Podcast 裡，說了一句讓整個 AI 產業尷尬的話：大型語言模型是死胡同。

這句話之所以尷尬，是因為過去六年來，矽谷最頂尖的 AI 實驗室把薩頓在 2019 年寫的一篇短文奉為聖經。OpenAI 的研究員用它來為千億美元的算力投資辯護，Google DeepMind 的工程師把它當作判斷研究方向的試金石。Andrej Karpathy 公開說過，這篇文章在前沿 LLM 研究圈已經具有「聖經般的地位」，研究人員會拿它來檢驗每一個新想法是否值得追求。

然後，聖經的作者站出來告訴信徒：你們讀錯了。

這篇被奉為聖經的文章叫做 "The Bitter Lesson"，直譯就是「苦澀的教訓」。它只有短短一千多個英文字，沒有數學公式，沒有實驗數據，卻可能是過去十年對 AI 產業方向影響最大的一篇文章。要理解今天 AI 發展最核心的路線之爭，你得先讀懂這篇文章說了什麼，然後理解它的作者為什麼認為大家搞錯了重點。

## 一位不服氣的強化學習先驅

在談文章之前，得先認識寫文章的人。薩頓不是那種發了一篇論文就出名的學者，他是一個用四十年時間反覆押注同一個信念的人。

1980 年代初期，薩頓還在麻省大學阿默斯特分校讀博士時，和指導教授安德魯．巴托（Andrew Barto）一起研究一個當時多數人不看好的方向：讓機器透過和環境互動、從獎懲回饋中自己學會做決策。這就是後來的強化學習（Reinforcement Learning）。那個年代的主流 AI 研究走的是專家系統路線，研究者費盡心思把人類知識編碼成規則，灌進電腦裡。薩頓和巴托的做法在當時看起來既笨又慢：你不直接告訴機器答案，而是讓它自己去碰壁、去嘗試、去從失敗裡摸索。

這條路走了將近四十年。薩頓發明了時序差分學習（Temporal Difference Learning），這個演算法後來成為強化學習的基石。他和巴托在 1998 年合著的教科書《Reinforcement Learning: An Introduction》被引用超過七萬五千次，至今仍是這個領域的標準教材。他的學生 David Silver 後來主導開發了 AlphaGo，在 2016 年擊敗世界圍棋冠軍。2025 年 3 月，薩頓和巴托共同獲頒圖靈獎，這是計算機科學的最高榮譽。

但在 2019 年，當他寫下〈苦澀的教訓〉時，驅動他的情緒更接近「不服氣」。他回顧了七十年的 AI 發展史，看到同一個模式一再重複：研究者花大量心血把人類知識灌進系統裡，短期內確實有效，但長期來看，那些不靠人類知識、只用算力和通用方法硬幹的系統，每次都贏。每一次。

## 七十年一再驗證的殘酷模式

薩頓在文章裡舉了幾個讓 AI 研究者「痛苦」的案例。

首先是西洋棋。1960 年代到 1990 年代，一整個世代的計算機科學家投入了數十年心血，試圖把人類棋手的直覺和策略知識編碼進電腦程式裡。他們開發了精巧的評估函數，讓電腦「理解」局勢：哪些棋子位置好、哪種結構有優勢、什麼時候該進攻。這些系統確實不斷進步，但進步的速度始終有限。然後 1997 年，IBM 的「深藍」（Deep Blue）擊敗了世界冠軍卡斯帕洛夫（Garry Kasparov）。深藍靠的是每秒搜尋兩億個棋步的暴力計算能力，跟「理解」西洋棋完全無關。多數專注於模擬人類棋藝的研究者對這個結果感到沮喪，因為深藍贏棋的方式跟人類思考完全不同。

語音辨識的故事更殘酷。1970 年代，研究者嘗試讓電腦「理解」語音的結構，把語言學家對音素、語法、語義的知識手工編碼進系統。卡內基美隆大學的 Harpy 系統能辨識大約一千個單字，但極度脆弱，遇到環境噪音就崩潰。到了 1980 年代，一批統計學家提出了完全不同的方法：用隱藏馬可夫模型（HMM）這種通用的統計工具，讓系統從大量語音資料中自己學習模式。IBM 的 Fred Jelinek 團隊用這個方法造出了能處理兩萬個單字的語音打字機。Jelinek 有一句名言被廣為流傳：「每次我開除一個語言學家，語音辨識的準確率就上升一點。」他不是在嘲笑語言學家。他在描述一個殘酷的事實：人類對語言的精細理解，反而成了系統進步的阻礙。

然後是圍棋。圍棋的棋盤有 19×19 = 361 個交叉點，合法局面的數量比宇宙中的原子還多。幾十年來，所有人都認為電腦不可能靠暴力搜尋贏人類，因為搜尋空間太大了。AI 圍棋程式花了大量心血在開局庫、定式資料庫、形勢判斷的人類知識上。然後 2016 年，DeepMind 的 AlphaGo 擊敗了世界冠軍李世乭。AlphaGo 用的是深度神經網路加上蒙地卡羅樹搜尋，比前幾代圍棋程式少了很多人類知識。更驚人的是 2017 年的 AlphaGo Zero，這個版本完全不使用任何人類棋譜，只透過自我對弈就超越了所有前代版本。人類知識不只是不必要的，它反而拖了系統的後腿。

這就是薩頓所說的「苦澀」：研究者投入畢生心血精心打磨的專業知識，最終被一台不懂任何領域知識、只會用算力蠻幹的機器碾壓。這個教訓之所以苦澀，因為它打擊的是研究者最深層的自尊。你對這個領域的理解、你花二十年累積的直覺，長期來看竟然不如讓機器自己去學。

## 苦澀教訓的最大驗證：GPT 的崛起

薩頓寫完這篇文章不到兩年，一場他沒有預見到的革命就提供了苦澀教訓最壯觀的驗證。

GPT 系列模型的核心思路簡單到近乎粗暴：拿一個通用的 Transformer 架構，餵它吃下整個網際網路的文字資料，然後不斷把模型做大、訓練資料加多、算力往上堆。不需要語言學知識，不需要世界模型，不需要把人類的推理規則編碼進去。你只需要一個目標：預測下一個字。然後，神奇的事情發生了。當模型大到一定程度，它開始展現出翻譯、寫程式、數學推理、常識問答等等訓練目標裡從未明確要求的能力。

OpenAI 的研究者們開始引用苦澀的教訓，把它當作自己路線的理論基礎：看吧，通用方法加上夠多的算力，就是會贏。Scaling Laws 的發現更是火上加油。研究人員觀察到，只要你按比例增加模型大小、訓練資料量和算力，模型表現會以一種可預測的方式持續提升。這條線還沒有明顯彎折的跡象，彷彿你只要沿著它繼續投資下去，就能通往 AGI。

苦澀的教訓於是成了矽谷最有力的融資話術。每一輪新的資料中心投資、每一個百億美元的算力採購，背後的邏輯都可以追溯到薩頓那篇一千字的短文。你要跟投資人解釋為什麼要花這麼多錢？很簡單：七十年的 AI 歷史告訴我們，算力就是一切。

## 教主的反擊

但是，薩頓本人從來沒有說過「算力就是一切」。

2025 年 9 月，薩頓在 Dwarkesh Patel 的 Podcast 上做了一件讓 LLM 信徒難堪的事：他明確表示，大型語言模型根本不符合苦澀教訓的精神，反而可能是下一個即將被推翻的範式。

薩頓的邏輯是這樣的。苦澀教訓說的是，通用的學習和搜尋方法會勝過嵌入人類知識的方法。但 LLM 的訓練資料是什麼？是人類寫的文字。數十億頁的網頁、書籍、論文、程式碼，全部都是人類知識的結晶。在薩頓看來，LLM 本質上是在模仿人類，做人類說你該做的事。它沒有在自己學習理解世界，只是在消化人類理解世界後產出的二手紀錄。

薩頓用了一個精確的比喻來區分兩種智慧。LLM 就像一個記憶力驚人的圖書館員：它讀過世界上幾乎所有的書，能快速回答你任何問題，甚至能融會貫通、舉一反三。但它的所有知識都是從書本裡來的，它從來沒有走出過圖書館。真正的智慧應該像一個探險家，走進未知的荒野，踢到石頭會痛、找到水源會高興、被蛇咬了會學著閃避。探險家的知識來自和世界的直接互動，而不是閱讀別人的遊記。

這個區分不只是哲學辯論，它指向一個很實際的技術瓶頸。LLM 依賴人類產出的文字資料來學習，但這些資料是有限的。全世界的高品質文字資料終究會被吃完，到時候你拿什麼繼續訓練？合成資料嗎？那不過是圖書館員在讀自己寫的書，越讀越封閉。

## Ilya Sutskever 也覺得風向變了

有趣的是，連 LLM 陣營內部的人也開始鬆動。Ilya Sutskever 是 OpenAI 的共同創辦人，也是最早推動神經網路 scaling 的先驅之一。他比任何人都清楚 LLM 走了多遠。但他在離開 OpenAI、創辦 SSI 之後，公開說了一段耐人尋味的話：「2010 年代是 scaling 的時代，現在我們重新回到了驚奇與發現的時代。」

這句話的潛台詞是：光靠把模型做大、資料加多、算力堆上去，可能已經不夠了。你需要新的想法。

不過，產業並沒有停下來等待新的想法。OpenAI 在 2024 年底推出 o1 模型，2025 年推出 o3 系列，走的是一條被稱為「推理時計算」（test-time compute）的新路線。過去的 scaling 是在訓練階段砸算力：模型越大、訓練越久，表現越好。推理時計算則是在使用階段砸算力：遇到難題時，讓模型花更多時間「思考」，拆解問題、嘗試不同路徑、反覆驗證答案。o3 在一個叫做 ARC-AGI 的推理測試上拿到了 87.5% 的分數，但代價是每道題用了超過一千美元的算力。

這是苦澀教訓的變體：算力依然是核心武器，只是戰場從訓練轉移到了推理。但 Ilya Sutskever 的警告依然成立：報酬遞減是對數級的，你得花十倍的錢才能提升一點點表現。這條路能走多遠？

## 從圖書館走進荒野

薩頓給出的答案很乾脆：別修了，換一條路。

他在 2025 年的學術演講中提出了一個叫做 OaK 的架構藍圖（Options and Knowledge Architecture）。這個架構的核心思路是讓 AI 像動物一樣，從「經驗之流」中持續學習。系統不斷感知環境、採取行動、接收回饋，然後根據結果調整自己的行為。訓練完就部署、部署完就凍結？不行。AI 應該一輩子都在學。

OaK 的概念建立在薩頓四十年的強化學習功底上。系統會把觀察到的世界轉化為大量可預測的知識片段（薩頓稱之為「通用價值函數」），再把短期動作組合成長期策略（稱為「選項」），搭配學習到的世界模型來做規劃。目標不是做一個只能下圍棋的系統，而是一個能在任何環境中持續適應的通用架構。

但薩頓自己也承認，OaK 目前還做不到。最大的障礙是「災難性遺忘」（catastrophic forgetting）：深度神經網路在學習新東西的時候，會把之前學過的東西忘掉。人類不會因為學了日文就忘了英文，但目前的神經網路會。在這個問題被解決之前，真正的持續學習只是一個願景。

這裡有一個微妙的諷刺。薩頓批評 LLM 不算真正的學習，但他自己提出的替代方案目前也做不出來。LLM 陣營至少有能用的產品，OaK 還停留在投影片上。學術願景和工程現實之間，永遠存在這種張力。

## 這場辯論真正在問的問題

薩頓和 LLM 陣營的分歧，表面上看是技術路線之爭。但往下挖，其實是一個更根本的問題：什麼才算「學習」？

LLM 從人類文字中提取模式，然後用這些模式生成新的文字。你可以說它在「學習」人類的語言和知識，也可以說它只是在做非常高級的複製貼上。薩頓認為真正的學習必須來自與世界的直接互動，就像嬰兒不是靠讀書學會走路的，而是靠一次次跌倒。

但這裡有一個苦澀教訓本身沒有回答的問題：如果未來某個 LLM 的變體真的發展出了持續學習、自主探索的能力呢？如果推理時計算的路線走到盡頭之前，就已經催生出某種「會思考」的系統呢？薩頓的判斷是基於他對 AI 歷史的深刻理解，但歷史不一定會完全重演。

對於我們這些不在前沿實驗室工作的人來說，這場辯論最實際的啟示可能是這樣的：不要把你的職涯押注在任何一個特定的技術範式上。七十年來，AI 的歷史反覆證明一件事，就是那些看起來最不可撼動的主流方法，最終都會被取代。專家系統如此、淺層神經網路如此、手工特徵工程如此。LLM 可能也不例外。

薩頓用四十年的職業生涯押注強化學習，看著它從邊緣走到舞台中央。他寫苦澀教訓的原意從來就不是替 scaling 背書。他想說的是：不要太相信自己當下的理解，因為最終勝出的方法，往往是你現在覺得「太笨」的那個。

這個提醒在 2026 年依然成立。只是這一次，被提醒的對象可能包括那些把苦澀教訓當成 scaling 護身符的人。
