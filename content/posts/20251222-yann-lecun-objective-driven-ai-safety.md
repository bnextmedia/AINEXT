---
title: "深度學習教父的 AI 安全方案——為什麼「目標驅動架構」比微調更安全？"
date: 2025-12-22T22:25:00+08:00
description: "Yann LeCun 認為當前 LLM 的微調安全方法從根本上就是錯的，永遠可以被 jailbreak 繞過。他提出「目標驅動架構」作為替代方案：將安全規則設為硬性約束而非統計傾向，從設計上保證系統無法違反。本文詳解這個架構如何運作，以及為何這才是通往可靠 AI 安全的正確路徑。"
tags: ["Yann LeCun", "AI 安全", "目標驅動架構", "World Model", "jailbreak"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=7u-DXVADyhc"
source_name: "Information Bottleneck Podcast"
draft: false
---

> 本文整理自 Information Bottleneck Podcast EP20 對 Yann LeCun 的專訪。
> 🎧 收聽連結：[YouTube](https://www.youtube.com/watch?v=7u-DXVADyhc)

---

AI 安全是當前最熱門的話題之一。各大實驗室花費大量資源做 RLHF（人類反饋強化學習）、Constitutional AI、紅隊測試，試圖讓他們的模型更安全、更不容易說出有害的內容。

但 Yann LeCun 認為，這些方法從根本上就是錯的。

在最近的 Information Bottleneck 訪談中，這位圖靈獎得主提出了一個不同的思路：AI 安全不應該靠事後的微調和過濾，而應該從架構本身就保證安全。這個想法的核心是他一直在推動的「目標驅動架構」（objective-driven architecture）——一個與當前 LLM 範式根本不同的 AI 系統設計方式。

## LLM 安全的根本困境

為什麼 LLM 這麼難做到安全？LeCun 的分析直指問題核心：微調不是根本解法。

當前的做法是這樣的：先訓練一個大型語言模型，讓它能夠生成流暢的文字。然後透過 RLHF 之類的技術，「教」它不要說某些話、不要回答某些問題。這個過程會修改模型的權重，讓它在遇到敏感話題時傾向於拒絕回答。

問題是，這種方法永遠可以被繞過。Jailbreak（越獄）技術層出不窮，研究者和使用者不斷發現新的提示詞，可以讓模型「忘記」它被訓練的限制。今天你封堵了一個漏洞，明天就會有新的漏洞被發現。

「你總是可以找到某些提示詞，讓它們逃脫那些你試圖阻止它們做的事。」LeCun 說。這不是因為目前的微調技術不夠好，而是因為微調這個方法本身就有結構性的缺陷。

微調本質上是在調整一個統計模型的輸出分佈。但這個模型原本就是被訓練來「預測最可能的下一個 token」的。你可以透過微調讓某些輸出變得不太可能，但你無法完全消除它們。只要輸入足夠奇怪，模型就可能產生你不想要的輸出。

## 目標驅動架構：從設計上保證安全

LeCun 提出的替代方案是完全不同的架構。

在這個架構中，AI 系統不是「預測下一個 token」，而是「透過規劃來達成目標」。系統需要有一個 World Model（世界模型），能夠預測「如果我採取某個行動，世界會發生什麼變化」。有了這個預測能力，系統就可以想像各種可能的行動序列，評估每一個序列的後果，然後選擇最好的那個。

但這裡有一個關鍵設計：系統除了有「目標函數」（定義要達成什麼）之外，還有「約束條件」（定義什麼事情絕對不能做）。規劃器在搜尋行動序列時，必須同時滿足目標和所有約束。

這跟 LLM 的微調完全不同。在 LLM 中，安全規則是「軟性地」植入模型權重，系統只是「傾向於」不產生有害輸出。在目標驅動架構中，安全規則是硬性約束——系統在產生任何輸出之前，必須確認這個輸出不違反任何約束。

這就像是把安全規則寫成程式碼，而不是訓練資料。程式碼定義的規則是絕對的——如果約束說「永遠不要推薦傷害人的行動」，那系統就永遠不會推薦，不管輸入是什麼。

## 咖啡機器人的例子

LeCun 用了一個著名的例子來說明這個問題：假設你有一個家用機器人，你讓它去拿咖啡，但咖啡機前面站著一個人。

在純粹的「目標最大化」框架下，機器人可能會決定把人推開（甚至更糟），因為這是達成「拿到咖啡」目標的有效手段。這就是 Stuart Russell 等人經常用來說明 AI 安全問題的「迴紋針最大化」場景的家用版。

LeCun 認為這個例子其實說明了問題有多容易解決——只要你用對了架構。

在目標驅動架構中，你可以設定一個低層級約束：「永遠與人保持距離」或「永遠不要對人施加物理力量」。這個約束在規劃過程中被強制執行。機器人在考慮任何行動序列時，都必須確認這個序列不會違反約束。如果唯一能拿到咖啡的方式是推開人，那系統就會得出結論：這個目標在當前情況下無法達成，也許可以禮貌地請人移開。

類似地，如果是一個會拿刀的廚房機器人，你可以設定約束：「當手中持有尖銳物品時，不要快速移動手臂」或「當附近有人時，不要揮舞刀具」。這些約束在規劃層級被強制執行，不是靠微調來「傾向於」遵守，而是系統在設計上就不可能違反。

## 為什麼這種方法「無法被 jailbreak」

這個架構的核心優勢是：安全規則是強制性的，不是統計性的。

在 LLM 中，「不要說有害的話」是透過調整輸出機率來實現的。模型在看到某些輸入時，會傾向於產生拒絕的回應，而不是有害的回應。但這只是「傾向於」——如果你構造出足夠奇怪的輸入，機率分佈可能會翻轉。

在目標驅動架構中，約束是在優化過程中被強制滿足的。系統產生輸出的方式是：找出能達成目標且滿足所有約束的最佳行動序列。如果某個行動違反約束，它根本不會被考慮，不管這個行動對達成目標有多有效。

用數學術語說，這是把安全規則從「目標函數的一部分」變成「優化問題的約束」。在帶約束的優化中，約束是必須滿足的，不是可以權衡的。

LeCun 強調：「這不是微調，這是 by construction（從設計上就保證的）。系統無法逃脫，因為它獲得輸出的方式就是透過優化——最小化任務的目標函數，同時滿足 guardrail 的約束。」

## 噴射引擎的比喻：安全是工程問題

LeCun 喜歡用噴射引擎來說明他對 AI 安全的看法。

第一代噴射引擎確實很危險——可能跑十分鐘就爆炸，效率低，也不可靠。但經過數十年的工程改進，現在的噴射引擎極度可靠。你可以坐著雙引擎飛機安全飛越半個地球。

AI 安全會經歷類似的過程。現在的系統確實有問題——可以被 jailbreak、可能產生有害內容、可能被用於惡意目的。但這些是可以透過更好的設計來解決的工程問題，不是無法克服的根本限制。

關鍵是要用對方法。微調和過濾是「打補丁」的做法——系統本身沒有安全設計，你只是在外面加一層過濾。這種做法注定是打不完的地鼠遊戲。

目標驅動架構是「從設計上保證安全」的做法——安全規則是系統運作的核心部分，不是事後加上去的。這種做法更可能達到真正可靠的安全性。

## 這需要新的架構

當然，這個願景有一個大前提：需要有能夠做 World Model 和規劃的 AI 系統。

當前的 LLM 不是這樣運作的。它們是純粹的 token 預測器，沒有內建的「世界模型」，也沒有「規劃」能力（儘管你可以用 chain-of-thought 之類的技巧來模擬一些規劃行為）。要實現 LeCun 設想的目標驅動架構，需要發展出新一代的 AI 系統。

這也是為什麼 LeCun 離開 Meta 創辦 AMI 來專注於 World Model 研究。在他看來，這不只是「另一種技術路線」，而是通往真正可靠 AI 系統的必經之路。

## 不只是效率問題

有些人嘗試在 LLM 上實現類似約束的效果。比如讓系統產生很多候選輸出，然後用一個過濾器來篩選掉有害的，只輸出通過檢查的。

這種方法在某種程度上有效，但有兩個問題。第一，極度昂貴——你需要生成大量候選，然後對每個候選進行評估，推論成本可能增加幾十倍。第二，仍然是統計性的。過濾器本身也可能出錯。如果過濾器是另一個 AI 模型，它也面臨同樣的 jailbreak 問題。

目標驅動架構不一樣。約束是在規劃過程中直接被強制執行的。規劃器在搜索可行解的時候，根本不會考慮違反約束的選項。這既更高效（不需要生成再過濾），也更可靠（約束是硬性的，不是統計的）。

## 安全需要正確的架構

LeCun 的觀點可以簡單總結：問題不在於我們的微調技術不夠好，問題在於微調這個方法本身就不對。

真正可靠的安全性需要從架構層面來保證。系統需要有能力理解世界、預測行動後果、並在規劃過程中強制執行安全約束。這比「訓練模型不說壞話」要難得多，但也是唯一能達到真正可靠安全性的方法。

這個願景還在很早期的階段。World Model 技術還在發展中，目標驅動架構還沒有大規模應用的案例。但這提供了一個不同的思考框架：與其問「如何更好地限制一個本質上不可控的系統」，不如問「如何設計一個在架構上就是可控的系統」。

這兩個問題的答案可能完全不同。而後者，才是 LeCun 認為值得投入的方向。
