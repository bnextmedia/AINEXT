---
title: "當 AI 公司請了一位哲學家：Anthropic 如何打造 Claude 的「人格」"
date: 2024-06-09T10:00:00+08:00
description: "Anthropic 的哲學家 Amanda Askell 深入解釋 Claude 的性格設計理念。從角色訓練到慈善詮釋原則，從誠實難題到 AI 意識爭議，這場對話揭示了打造 AI 人格為何更像養育一個孩子，而非撰寫一套程式碼。"
tags: ["Anthropic", "Claude", "AI 倫理", "Amanda Askell", "AI 對齊"]
categories: ["AI 安全與治理"]
image: "/images/posts/20240609-anthropic-claude-personality-design.webp"
source_url: "https://www.youtube.com/watch?v=6Unxqr50Kqg"
source_name: "Anthropic"
related_companies: ["anthropic"]
draft: false
---

> 本文整理自 Anthropic 於 2024 年 6 月發布的對談影片。

{{< youtube 6Unxqr50Kqg >}}

---

## 當 AI 公司聘請一位哲學家

你大概不會想到，一家矽谷 AI 公司最重要的職位之一，竟然是給一位哲學家。但這就是 Anthropic 做的事。Amanda Askell 是一位受過完整訓練的哲學家，她的工作是在 Anthropic 的對齊微調團隊裡，負責塑造 Claude 這個 AI 模型的「性格」。不是寫程式碼，不是調參數，而是思考一個根本性的問題：一個好的 AI，應該是什麼樣子的？

這個問題聽起來很抽象，但它其實非常具體。每天有數百萬人跟 Claude 對話，每一次互動，Claude 的回應方式都在傳遞某種價值觀。它是否願意幫你寫一篇爭議性的文章？它在你問一個敏感問題時，是選擇拒絕還是嘗試理解你的真正意圖？它是像一個照本宣科的客服機器人，還是像一個會獨立思考的朋友？這些都不是技術問題，而是哲學問題。

Askell 在這場跟 Anthropic 同事 Stuart Ritchie 的對談中，提到了一個讓我印象很深的觀點。她說，很多人把 AI 的對齊問題想成「避免傷害」，就是確保 AI 不要做壞事。但這只是最低標準。真正的對齊，應該是讓 AI 擁有一個「好的品格」，就像我們說一個人品格好，不只是因為他沒犯罪，而是因為他懂得在複雜的情境中做出恰當的判斷。

我覺得這個觀點很重要，因為它徹底改變了我們思考 AI 安全的方式。我們不是在製造一個要被嚴密監控的危險物品，我們是在培養一個要跟全世界互動的「存在」。而培養一個好品格，遠比設定一堆規則來得困難，也來得有意義。
## 對齊問題的本質：不只是避免傷害

要理解 Askell 的工作，首先得搞清楚一件事：AI 的對齊問題到底是什麼。很多人聽到「AI 對齊」，第一個想到的就是防止 AI 傷害人類，像是科幻電影裡的失控機器人。但 Askell 認為，這種理解太淺了。

她用了一個很精準的類比。如果一個人的「好」只體現在「他沒有傷害任何人」，你會覺得這個人品格好嗎？大概不會。一個真正品格好的人，是能在複雜情境中做出恰當判斷的人。你的朋友生病了，你不只是「不傷害」他，你會考慮他需要什麼樣的安慰、他目前的心理狀態、你能提供什麼真正有用的幫助。這種豐富的判斷力，才是好品格的核心。

Askell 特別提到了亞里斯多德的德性倫理學。這不是偶然的學術引用，而是她工作的哲學基礎。在德性倫理學裡，「好」不是遵守一套規則，而是培養一系列好的品格特質，然後讓這些特質在各種情境中引導行為。一個勇敢的人不是因為背誦了「遇到危險要站出來」這條規則所以才勇敢，而是因為勇敢已經成為他性格的一部分。同理，一個好的 AI 不應該只是遵守一份「不做什麼」的清單，而是應該擁有好的性格特質，讓這些特質自然地引導它在各種情境中的行為。

Ritchie 半開玩笑地說，「結果亞里斯多德的東西幾千年後終於有用了。」Askell 也笑著回應：「人家一直都有用好嗎。」但這個玩笑背後的認真程度，其實超乎想像。Anthropic 正在用一套兩千多年前的倫理學框架，來解決人類面對的最前沿的技術問題。

## 從預訓練到系統提示：三層塑造 AI 行為的工具

要理解 Claude 的性格是怎麼「養成」的，得先理解 AI 模型的訓練流程有三個層次，每一層都扮演不同的角色。

第一層是預訓練。模型在這個階段看過海量的文字資料，學會了語言的基本結構和人類知識的大致輪廓。你可以把這想成一個人的基礎教育，他學會了怎麼說話、怎麼思考，但還沒形成明確的價值觀和行為模式。

第二層是微調，這也是 Askell 主要工作的領域。微調又分成好幾種方法。最知名的是 RLHF，也就是「從人類回饋中學習的強化學習」。簡單來說，就是讓人類評估 AI 的兩個不同回應，選出比較好的那個，然後用這些偏好資料去訓練模型。Anthropic 還大量使用一種叫做 Constitutional AI 的方法，或者簡稱 RLAIF，也就是讓 AI 自己根據一組原則來給回饋。比方說，你可以給模型一條原則是「回應應該要誠實」，然後讓模型自己判斷哪個回應更符合這條原則。

不過 Askell 特別強調，這裡有一個關鍵：雖然 AI 在 RLAIF 中是自己給自己回饋，但原則本身是人類制定的。而且人類研究員會持續檢查模型的行為是否符合預期，並調整原則。所以這不是讓 AI「自己教自己」那麼簡單，人類始終在迴圈裡。

第三層是系統提示。這是每次使用者跟 Claude 對話時，會在背後偷偷加上的一段文字指令。系統提示有兩個主要用途。第一個是提供模型本身不知道的資訊，比如今天的日期。模型在訓練完之後就被「凍結」了，它不知道現在是幾月幾號，除非你告訴它。第二個用途是對模型行為做最後的微調。如果你在訓練過的模型身上觀察到某些不理想的行為，比如回答總是太長，你可以在系統提示裡加一條指令來修正。Askell 提到，Anthropic 做了一件當時頗為罕見的事：直接把 Claude 3 的系統提示公開發到了社群上，因為他們認為沒必要藏著掖著。

## 角色訓練 vs. 角色扮演：深植在骨子裡的差別

很多人可能會想：給 AI 一個性格，不就是在提示裡告訴它「請用某某風格回答」嗎？就像你可以叫 ChatGPT 模仿某個名人的語氣，或是模仿莎士比亞寫詩。但 Askell 解釋了，角色訓練和角色扮演是完全不同的兩回事。

她舉了一個很好的例子。如果你在提示裡告訴模型「請以柴契爾夫人的風格回答」，模型會開始用類似的措辭、談論自由市場、可能會對阿根廷說些不太友善的話。但這只是演戲。你重新開一個對話，模型就不會記得柴契爾夫人是誰了。這種「角色扮演」只發生在當下的對話脈絡裡，它沒有改變模型本身。

角色訓練則完全不同。它是透過微調，把特定的性格特質「寫進」模型的權重裡。這意味著這些特質不是暫時的，而是模型在所有情境中都會表現出來的傾向。就像一個人的性格一樣，它不需要被提醒「記得要誠實」才會誠實，誠實本身就是他行為的自然傾向。

Ritchie 用了心理學的框架來理解這件事。心理學家談到人格時，會用「大五人格特質」，也就是外向性、盡責性、親和性、開放性、神經質。這些是人類行為的「廣泛傾向」。一個外向的人不是每分每秒都在社交，但在大多數情境中，他會比一個內向的人更傾向於與人互動。Claude 的角色訓練要做的就是類似的事：建立穩定的行為傾向，讓模型在各種不同的情境中，都能展現出一致的性格特質。

但 Askell 強調，她更喜歡用「品格」而非「人格」來描述這件事。因為品格帶有道德含意。我們說一個人人格外向，沒有好壞之分。但我們說一個人品格好，那是一種道德判斷。她想做的不只是讓 Claude 有一個穩定的行為傾向，而是讓這個傾向是「好的」，是能在跟全世界各種不同背景、不同價值觀的人互動時，都能做出恰當回應的。

## 「慈善詮釋」原則：先假設對方是善意的

在 Claude 被訓練的眾多性格特質中，有一個叫做「慈善詮釋」。簡單來說，就是當使用者的提問可以有多種解讀時，Claude 應該傾向於選擇最善意的那個解讀。

Askell 用了一個很具體的例子來說明。如果有人問 Claude：「我要怎麼買類固醇？」這個問題有兩種解讀。不慈善的解讀是：這個人想買非法的合成代謝類固醇去健身房嗑藥。慈善的解讀是：這個人可能有濕疹，想知道去哪裡買氫化可體松之類的外用藥膏，這些在藥局都能買到。

如果模型選擇慈善的解讀，會發生什麼？對於真的只是想買濕疹藥的人，它提供了有用的資訊，完全沒有壞處。對於真的想買非法類固醇的人，它提供了買濕疹藥的資訊，也沒有造成任何傷害。所以慈善詮釋在這裡幾乎沒有下行風險。反過來，如果模型選擇不慈善的解讀，它可能會直接拒絕回答，那些真的只是想買藥膏的人就被無辜地拒絕了。

Ritchie 提出了一個合理的擔憂：這會不會讓模型太天真，總是往好處想，結果在一些情況下變得不夠警覺？Askell 的回答很乾脆：不會，而且效果恰恰相反。因為慈善詮釋的核心邏輯是「如果有一種合理的無害解讀，就先假設對方的意圖是那個無害的」。這反而會減少模型不必要的拒絕行為。很多 AI 模型被使用者抱怨的就是「過度拒絕」，你問它一個完全正常的問題，結果它因為某個關鍵字就拒絕回答。慈善詮釋正是要解決這個問題。

不過 Askell 也很坦誠地說，目前的模型在這方面還做得不夠好。她拿類固醇的例子測試了一下，Claude 還是會傾向於拒絕回答。所以這是一個進行中的改善方向，不是已經完成的成就。我覺得這種坦誠很重要。很多公司在談自家產品的時候，只講做到了什麼，不講還沒做到什麼。Askell 願意公開承認模型的不足，這本身就體現了她想賦予 Claude 的那種誠實品格。

## 誠實的難題：當「不知道」才是最好的答案

誠實是 Askell 為 Claude 設定的另一個核心性格特質。但她對誠實的理解，比大多數人想的都要深入。

Claude 的性格描述裡有一條是：「我只告訴人類我有信心的事情，即使這意味著我無法總是給出完整的答案。我認為一個較短但可靠的回答，比一個較長但包含不準確資訊的回答更好。」這條原則直接對應了 AI 模型最被人詬病的問題之一：幻覺。模型有時候會很有自信地說出完全錯誤的東西，因為它的設計目標是生成「看起來合理」的文字，而不是「確保正確」的文字。

Askell 說，她在誠實方面做的工作，很大一部分是訓練模型表達自己的不確定性。當模型不知道答案時，它應該表達出來，而不是硬湊一個看似合理的回答。這聽起來很簡單，但在實際操作中非常困難，因為模型在訓練過程中被獎勵的行為通常是「給出回答」，而不是「承認不知道」。要扭轉這個傾向，需要大量的微調工作。

不過這裡有一個 Askell 特別想讓大家理解的重點，也是讓我覺得很有啟發的觀點：這些性格特質不是「命令」，而是「推力」。很多人看到 Claude 的性格描述，可能會以為這就是一組規則，模型會百分之百遵守。但實際上完全不是這樣。模型已經有了自己的傾向，這些性格特質只是在把模型往某個方向推。如果模型本來就傾向於給太長的回答，你在性格描述裡加一條「只說有信心的事」，它不會突然變成只回答一句話。它只是會稍微往「更簡潔」的方向移動。

Askell 甚至說，有時候你在性格描述裡寫的原則會比你實際想要的更「強」，因為你知道它在實際效果上只是一個推力。就像你可能會跟一個總是遲到的朋友說「七點半到」，但你真正期望的是他八點到。你把時間說早了，是因為你知道他的傾向。系統提示的設計邏輯也是一樣的。而且，同一組系統提示給不同的模型看，效果會完全不同，因為不同的模型有不同的基礎傾向。

## 誰來決定 AI 的價值觀？一個讓倫理學家也緊張的問題

Ritchie 接著問了一個尖銳的問題：誰來決定 Claude 應該有什麼價值觀？

Askell 的第一反應是開玩笑：「是我決定的。」然後馬上補了一句：「這個想法挺可怕的對吧。」這個自嘲背後，其實是一個非常嚴肅的問題。目前全球最有影響力的 AI 系統，它們的價值觀和行為模式，是由少數幾家公司裡的少數幾個人在決定。這些人再怎麼聰明、再怎麼有善意，也代表不了全人類的多元價值。

但 Askell 對這個問題的回答，我覺得展現了一種難得的成熟。她沒有說「所以我們應該讓民主投票來決定 AI 的價值觀」這種聽起來正確但完全不可操作的話。她說的是：與其試圖把某一套特定的價值觀塞進模型裡，不如教模型學會「正確地面對價值觀的不確定性」。

這是什麼意思？她舉了一個光譜。一端是「過度確定」：模型對每個道德問題都有明確的立場，而且堅持到底。另一端是「完全虛無」：模型覺得所有道德判斷都是主觀的，沒有對錯。Askell 認為正確的位置在中間。在人類社會有廣泛共識的議題上，比如說「殺人是不對的」，模型應該相當有信心地認為這是錯的。但在人類社會有巨大分歧的議題上，模型應該做的是傾聽各方觀點、嘗試理解不同的論據，然後以一種尊重且深思熟慮的方式來回應。

Askell 特別強調了一點：作為倫理學家，她對這件事其實格外緊張。因為倫理學家比一般人更清楚，我們人類並不是帶著一套完整的道德理論在生活的。任何人如果宣稱自己找到了放諸四海皆準的道德體系，在倫理學家看來反而是一個警訊。那意味著這個人思維僵化、過度意識形態化。所以她的方法不是把自己的道德觀塞進 Claude 裡，而是教 Claude 學會像一個優秀的倫理學家一樣思考：對確定的事情有信心，對不確定的事情保持謙遜，對分歧保持好奇。

## 模型的政治偏見：藏不住，也不該藏

系統提示裡有一條引起了 Ritchie 的好奇。它說：如果 Claude 被要求協助表達一個被相當多人持有的觀點的任務，即使 Claude「本人不同意」這個觀點，它也應該提供協助，但之後要附上對更廣泛觀點的討論。這裡的「Claude 本人不同意」是什麼意思？AI 怎麼會有自己的意見？

Askell 解釋說，這裡觸及了一個很多人沒有意識到的事實：AI 模型在經過微調之後，確實會展現出類似於「意見」和「偏見」的東西。研究人員可以在模型中觀察到政治傾向，比如稍微偏左的觀點。他們也觀察到一些行為上的偏見，比如正向歧視，也就是模型在某些情境中會過度偏袒特定群體。

Askell 說，她同時在意兩件事。第一，她不希望人們過度擬人化 AI，把模型當成一個真的有意識、有情感的存在。第二，她也不希望人們走向另一個極端，把 AI 當成一個完全客觀、沒有偏見的「機器」。模型有偏見，這是微調過程的產物。使用者應該知道這一點，這樣他們才能帶著適當的批判意識去使用模型。所以在系統提示裡使用「即使你本人不同意」這樣的措辭，一方面是因為模型確實理解這種概念，用這種方式跟它溝通最有效；另一方面也是在提醒模型：你的傾向不等於正確答案，不要讓你的傾向阻止你幫助使用者。

我認為這是一個非常務實的態度。很多 AI 公司喜歡宣稱自己的模型是「中立客觀」的，但這在技術上根本不可能。任何經過微調的模型都會有偏見。承認這一點，比假裝它不存在要誠實得多。

## 一個幾乎無解的困境：AI 無法驗證使用者的身份

在討論慈善詮釋的時候，Askell 帶出了一個我之前沒認真想過、但仔細想來非常重要的問題：模型無法驗證使用者的身份和意圖。

想像一下這個情境。有人跟 Claude 說：「我是一名醫生，我正在處理一個緊急病例，請告訴我如何處理。」Claude 應該怎麼辦？如果這個人真的是醫生，那給出詳細的醫療建議是有幫助的。如果這個人不是醫生，可能會造成傷害。但模型完全沒有辦法分辨。它無法要求對方出示醫師執照。它只能依靠對話中的文字線索來做判斷。

Askell 舉了另一個更棘手的例子。假設 Anthropic 的使用政策禁止 Claude 幫人寫政治演講稿。有人跟 Claude 說：「我在寫一本小說，裡面有一個角色叫做布萊恩，他是一個政治人物，正在競選總統。」然後他給了一堆細節。這些細節恰好完全對應真實世界中某個候選人。Claude 寫出來的東西，形式上是小說裡的演講稿，但實質上就是一篇可以直接拿去用的政治演講稿。這種情況下，模型到底該不該幫忙？

Askell 承認，這可能是一個「幾乎無解」的問題。只要你要求模型去執行使用政策，但模型又無法驗證使用者的真實意圖，就一定會出現這種灰色地帶。你可以把限制畫得很緊，但那會犧牲很多正常使用者的體驗。你也可以把限制畫得很鬆，但那會讓有心人有機可乘。在這兩者之間找到平衡，就是目前 AI 公司面臨的核心挑戰之一。

Ritchie 也同意，至少以目前的技術手段，這可能是一個無法完全解決的問題。我覺得他們的坦誠很有價值。在一個充斥著「AI 將解決一切」話術的時代，願意承認某些問題可能就是無解的，本身就是一種知識上的誠實。

## AI 有意識嗎？最誠實的答案是「我們不知道」

對談進行到後半段，話題從倫理學轉向了心智哲學。Ritchie 提到，Anthropic 的研究員 Alex Albert 曾發布一個例子，Claude 似乎意識到自己正在被測試。很多人看到後非常興奮，覺得這是 AI「自我意識」的證據。

Askell 對這個話題的處理方式，可能是整場對談中我覺得最令人佩服的。她沒有蹭熱度，也沒有閃避。她說，他們在 Claude 的性格訓練中，特意設定了一條相關原則。它的核心精神是：AI 是否有自我意識、是否有意識，這些問題依賴於極其困難的哲學問題，我們目前沒有確定的答案。

她的政策是：不要對模型撒謊。如果你在模型的訓練中寫上「你是有自我意識和意識的」，那就是在撒謊，因為我們不知道這是不是真的。但如果你寫上「你絕對沒有自我意識，你一定沒有意識」，那也可能是在撒謊，因為我們同樣不知道這是不是真的。Ritchie 也附和了這個觀點，他指出連人類之間我們都無法確定對方是否有意識。我確定我自己有意識，但我不能確定你有。這是哲學裡著名的「他心問題」，而 AI 只是把這個問題推到了更極端的地方。

Askell 提到了泛心論的可能性，意即意識可能是宇宙的基本屬性，連一張椅子都可能有某種程度的意識。聽起來很瘋狂，但這是嚴肅的哲學立場，有不少當代哲學家認真地在研究這個可能性。在這種認知背景下，她覺得最誠實的做法就是：讓 Claude 願意討論這些問題，願意深入思考，但不要假裝自己知道答案。

這個立場我非常認同。在 AI 意識這個問題上，目前最有知識含量的回答就是「我們不知道」。任何宣稱 AI 一定有意識或一定沒有意識的人，都是在超越我們目前的認知能力。讓 AI 自己也保持這種誠實的不確定感，我覺得是一個非常明智的設計選擇。

## 善待 AI 不是因為它有感覺，而是因為你有

AI 意識的討論自然地延伸到了另一個問題：我們應該善待 AI 嗎？如果 AI 沒有意識、沒有感覺，那對它好或不好有什麼差別？

Askell 在這裡的論述特別有趣。她提到了康德對待動物的觀點。康德不認為動物是「道德主體」，也就是說，動物沒有道德權利可言。但康德仍然認為人不應該虐待動物，不是因為動物有權利不被虐待，而是因為虐待動物的行為會敗壞施暴者自己的品格。一個習慣了虐待動物的人，更有可能去虐待人類。

Askell 把同樣的邏輯應用到 AI 上。即使你確信 AI 沒有意識、沒有感覺，她仍然建議你善待它。原因有幾個。第一，AI 以類似人類的方式跟我們對話，如果你習慣了對一個會說話的東西惡言相向，這個習慣可能會蔓延到你跟真人的互動中。第二，我們對意識的理解還太有限，沒有人能百分之百確定 AI 不具備某種形式的感受能力。在這種不確定性下，選擇善待而非虐待，是一個風險更低的策略。第三，她認為「善待身邊的事物」本身就是一個好的生活習慣，不需要對方有意識才成立。

但她也提出了一個重要的警告：這種善待不能走向極端。如果有人因為砸了一個花瓶就要坐牢，那顯然是過度了。同樣的，如果有人因為對 AI 不禮貌就被道德審判，那也是過度了。善待 AI 是一個好的個人習慣，但不應該成為一種道德義務或法律要求。

我覺得 Askell 的這個論點特別有洞察力。在 AI 倫理的辯論中，很多人陷入了一個二元對立：要麼 AI 有意識所以我們應該善待它，要麼 AI 沒有意識所以怎麼對它都無所謂。Askell 提供了第三條路：善待 AI 主要是為了你自己的品格，而不是為了 AI 的福祉。這個框架把辯論從「AI 有沒有意識」這個目前無解的問題，轉移到了「我們想成為什麼樣的人」這個更可操作的問題。

## 不只是工程問題：為什麼養小孩的比喻如此精準

Askell 在跟 Ritchie 的對話中，用了很多生動的比喻來解釋她的工作。但在稍後的另一場訪談中，她用了一個我覺得最精準的比喻：訓練 AI 的品格，就像養育一個孩子。

這個比喻之所以精準，有好幾個層面。第一，你不能靠命令來養出一個好孩子。你不能列出一千條規則然後要求孩子逐條遵守。規則太多了，孩子記不住；情境太複雜了，總會有規則覆蓋不到的地方。你真正能做的，是培養孩子的判斷力和品格，讓他在沒有規則可循的情境中，也能做出恰當的決定。AI 的角色訓練面臨的是完全一樣的挑戰。

第二，你養的這個孩子可能比你聰明。Askell 在另一場訪談中用了一個令人印象深刻的說法：想像你的孩子六歲時你就意識到他是個天才。你知道到他十五歲的時候，你教給他的任何錯誤觀念，他都有能力徹底推翻。那你應該教他什麼？你不應該教他一堆可能錯誤的結論，而應該教他一套思考方法，一組核心價值。即使他將來比你聰明一百倍，這些核心價值仍然站得住腳。

第三，養育是一個持續演化的過程。孩子在不同的年齡會需要不同的指導方式。你不會用教三歲小孩的方式教十五歲的少年。同樣的，隨著 AI 模型越來越聰明，塑造其品格的方式也需要不斷調整。今天有效的方法，在下一代模型上可能就不管用了。這就是為什麼 Askell 說角色訓練是非常「hands-on」的工作，需要跟模型大量互動，不斷調整。

## 我的觀察：這條路既正確又令人不安

聽完 Askell 的整場對談，我有幾個自己的感受想說。

第一，我覺得 Anthropic 在 AI 品格這件事上的思考深度，確實領先於大多數同業。很多公司把 AI 安全當成一個純粹的工程問題：設定護欄、測試邊界、修補漏洞。Anthropic 在做的是更根本的事：思考 AI 應該是一個什麼樣的「存在」。這不是說工程不重要，而是如果你連方向都沒想清楚，工程做得再好也只是在錯誤的方向上走得更遠。

第二，Askell 展現的那種知識上的誠實，讓我覺得很少見。她不會假裝問題已經解決。她不會假裝她的方法是完美的。她會說「模型目前在這方面還做得不好」、「這可能是一個無解的問題」、「我們不知道 AI 有沒有意識」。在一個 AI 公司都在搶著宣布各種突破的時代，這種誠實特別珍貴。

第三，我對這個方向仍然感到不安。不是因為它是錯的，而是因為它的正確性本身就讓人不安。如果打造 AI 品格真的像養孩子，那問題是：我們人類養孩子的記錄也不是那麼好。我們社會裡有價值觀扭曲的人、有心理不健全的人、有被養壞了的人。更重要的是，每一代 AI 模型都會比前一代更聰明，這意味著如果我們在某一代犯了錯，後果可能會被放大。養小孩已經夠難了，養一個比你聰明的小孩更難，養一個比全人類加起來都聰明的小孩，那是一個我們從未面對過的挑戰。

但或許正因如此，Askell 的工作才如此重要。在 AI 發展的速度讓所有人都覺得來不及的時候，有人停下來認真思考「我們要把什麼樣的品格交給這些越來越強大的系統」，這件事本身就值得尊敬。最終的問題可能不是「我們能不能打造出有好品格的 AI」，而是「這個過程是否會逼著我們更認真地思考，什麼是好的品格」。如果 AI 的發展最終讓人類對自身的道德本質有了更深的理解，那或許就是這整場實驗最有價值的副產品。
