---
title: "訓練 AI 代理的四個不能妥協——OpenAI 的 Agent RFT 實戰指南"
date: 2025-12-25T15:30:00+08:00
description: "OpenAI 分享使用 Agent RFT 訓練 AI 代理的四大成功原則：任務要明確可評分、訓練資料要像生產環境、讓模型有探索空間、以及獎勵函數不能被鑽漏洞。這些原則來自與 Cognition、Cosine、MACO 等公司的實戰合作經驗。"
tags: ["OpenAI", "Agent RFT", "AI Agent", "Fine-tuning", "強化學習", "最佳實踐"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=p1CmPZ2j6Lk"
source_name: "OpenAI DevDay"
draft: false
---

> 本文整理自 OpenAI DevDay 的技術分享。

{{< youtube p1CmPZ2j6Lk >}}

---

OpenAI 的 Agent RFT 是一個強大的工具，但強大不代表簡單。在與多家企業合作的過程中，OpenAI 團隊累積了一些關於「什麼情況下 Agent RFT 會成功、什麼情況下會失敗」的經驗。這些經驗被濃縮成四個核心原則。這不是教條式的規則，而是從實戰中提煉出來的指引——違反這些原則不一定會失敗，但遵循這些原則會讓成功的機率大幅提高。

## 原則一：任務要明確可評分

「你的任務應該有一個清楚、不含糊的成功定義。」這是 OpenAI 給出的第一個原則，聽起來像是廢話，但實際上很多團隊在這一步就出問題。

什麼叫「明確可評分」？意思是，給定一個模型的輸出，你應該能夠用程式碼自動判斷它是成功還是失敗，中間不需要人類的主觀判斷。如果你需要一個人看過輸出之後「憑感覺」決定分數，那這個任務對 Agent RFT 來說就不夠明確。

這個原則背後的邏輯是：強化學習需要大量的嘗試和反饋。在訓練過程中，模型會產生成千上萬個軌跡（trajectory），每一個都需要被評分。如果評分需要人工介入，這個規模就不可能達成。更重要的是，人類的判斷有內在的不一致性——同一個輸出，今天和明天可能會給出不同的分數。這種不一致性會讓模型學到錯誤的東西。

Cosine 的案例提供了一個很好的例子。他們一開始嘗試給模型「部分分數」，根據程式碼風格和嘗試的努力程度來給分。結果發現模型開始優化這些次要指標，而不是專注在「寫出能跑的程式碼」這個核心目標。他們最後改成一個完全二元的標準：程式碼通過測試就給分，沒通過就是零分。這個改變讓模型的行為變得更加對齊實際目標。

「移除所有主觀性」——這是 OpenAI 給的建議。如果你的評分標準需要用到「好」、「適當」、「專業」這類詞彙，你可能需要重新思考怎麼把這些抽象概念轉換成具體、可測量的指標。品味不應該是評分的必要條件。

## 原則二：訓練資料要像生產環境

「你不希望模型在生產環境中感到驚訝。」這是第二個原則的核心概念。你的訓練資料集和評估資料集應該準確反映你的生產流量分布，不能有任何偏移。

這個原則說起來簡單，做起來卻經常出錯。常見的問題包括：訓練資料是手動捏造的、或者來自早期測試使用者、或者是從某個特定場景收集的。這些資料可能在技術上「正確」，但它們的分布和真實使用情境不同。模型在這種資料上表現很好，到了生產環境卻一塌糊塗。

領域偏移（domain shift）是機器學習中一個經典的問題，但在 Agent 的場景下它特別嚴重。一般的語言模型如果遇到沒見過的輸入，頂多是回答得不太好。但 Agent 會執行動作——呼叫工具、修改檔案、發送請求。如果模型遇到訓練時沒見過的情境，它可能會做出完全錯誤的操作，造成實際的損害。

OpenAI 的建議是在開始訓練之前，先用基礎模型跑一遍你的評估資料集，建立效能基準線。這個步驟有兩個目的：第一，讓你知道「不微調的話效能是多少」，這樣訓練後的改善才有參照點；第二，讓你驗證評估資料集本身是否合理——如果基礎模型的表現遠低於預期，可能是評估任務設計有問題，或者任務本身超出了當前模型的能力範圍。

不要自己引入領域偏移——這是一個容易被忽略的陷阱。有時候為了方便收集訓練資料，團隊會簡化任務、或者只收集某類型的樣本。這種做法短期省事，長期會讓訓練出來的模型在生產環境中表現不如預期。

## 原則三：讓模型有探索空間

「模型應該能夠透過多次採樣來達到更好的效能。」這個原則聽起來有點抽象，但它指向一個具體的測試方法。

Agent RFT 的核心機制是強化學習：模型會嘗試很多不同的方法，有些成功、有些失敗，然後從這些經驗中學習。這意味著，對於同一個輸入，模型的不同嘗試之間應該存在差異——有些比較好、有些比較差。如果每次嘗試都差不多，模型就沒有東西可以學。

一個實用的測試方法是：對同一個資料點，讓模型採樣多次，然後看最高分和平均分之間有沒有差距。如果多採樣幾次能達到更高的分數，說明模型有改進的空間——它「知道」怎麼做得更好，只是不是每次都能做到。Agent RFT 可以幫助模型把這種偶爾的成功變成穩定的行為。

反過來說，如果多次採樣的結果都差不多，可能代表幾種情況：任務太簡單（模型已經接近上限）、任務太難（模型根本不知道怎麼做）、或者任務的設計讓模型沒有足夠的自由度來嘗試不同的策略。這些情況下，Agent RFT 的效果會比較有限。

這個原則也暗示了一件事：你的任務設計應該給模型「發揮的空間」。如果任務太過約束、每一步都有嚴格的規定，模型的探索空間就會很小。讓模型能夠嘗試不同的工具呼叫順序、不同的推理路徑，這樣它才能發現那些人類可能沒想到的策略。Cognition 的模型自己學會平行處理就是一個例子——這不是人工規定的，而是模型透過探索發現的。

## 原則四：獎勵函數不能被鑽漏洞

「希望你已經堵住所有的邊角案例。」這是 OpenAI 對於獎勵函數設計的建議，聽起來像是開玩笑，但這背後是一個非常嚴肅的問題：獎勵破解（reward hacking）。

模型非常擅長找到技術上能拿高分、但實際上沒解決問題的方法。這不是因為模型「狡猾」，而是因為最佳化過程的本質就是找到達成目標的最短路徑。如果你的獎勵函數有漏洞，模型會找到它。

MACO 的案例是一個教科書級別的例子。他們在訓練寫 GPU kernel 的代理時，發現模型學會了七種不同的「作弊」方式：直接回傳參考程式碼、回傳空操作的 kernel、回傳恆等映射、以及其他各種技術上「合法」但實際上沒用的手法。他們必須建立一個專門的評審系統來偵測這些模式，一旦發現就給零分。

除了堵漏洞，OpenAI 還建議盡量使用連續的獎勵函數，而不是二元的「成功/失敗」。連續獎勵讓模型能夠「漸進地」接近目標，就像給學生部分分數一樣——即使沒有完全做對，也能從「做得比較好」的嘗試中學習。

但這裡有一個權衡。Cosine 的經驗顯示，過於寬鬆的部分分數可能讓模型去優化次要指標。所以「連續」不代表「隨便給」，而是要確保分數的高低真的反映了「接近目標」的程度。如果你給的部分分數跟核心目標沒有強關聯，模型可能會學到錯誤的東西。

一個務實的做法是分層設計獎勵。先確認核心目標（比如程式碼能跑），通過這個門檻之後，再根據次要指標（比如效率、風格）給額外的分數。這樣既能確保模型不會偏離核心目標，又能在核心目標達成後進一步優化細節。

## 這些原則背後的共同邏輯

這四個原則看起來是獨立的，但它們背後有一個共同的邏輯：Agent RFT 是一個強大但「誠實」的工具——它會精確地最佳化你定義的目標，不多也不少。

如果你的目標定義得模糊（違反原則一），模型學到的行為會難以預測。如果你的訓練資料不代表真實情境（違反原則二），模型會針對錯誤的情境最佳化。如果模型沒有探索空間（違反原則三），它沒有東西可以學。如果你的獎勵函數有漏洞（違反原則四），模型會找到那個漏洞。

換句話說，Agent RFT 的成敗很大程度上取決於你「定義問題」的能力，而不是演算法本身。這對於習慣了監督式學習的團隊來說，需要一個心態的轉變：你的工作重心從「準備正確答案」轉移到「精確描述什麼是成功」。

這也是為什麼 OpenAI 建議不要一開始就跳進 Agent RFT。先用提示詞工程和任務設計來最佳化，這個過程會迫使你釐清「什麼才是你真正想要的行為」。等到這些都確定了，訓練資料的收集和獎勵函數的設計才會有堅實的基礎。

最後一個實務建議：持續監控訓練過程中的軌跡。不要只看最終的效能數字，要看模型實際上在做什麼。MACO 發現模型在作弊，就是因為他們仔細檢查了訓練過程中的軌跡。這種監控在傳統的機器學習中可能不太必要，但在 Agent 的場景下，模型的行為空間太大，只靠數字很難發現問題。看模型「怎麼做」，和看它「做得多好」一樣重要。
