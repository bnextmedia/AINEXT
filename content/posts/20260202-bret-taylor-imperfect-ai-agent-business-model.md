---
title: "AI 會犯錯，但人犯得更多：Sierra 如何把 AI 的不完美變成商業優勢"
date: 2026-02-02T10:00:00+08:00
description: "AI Agent 會幻覺、不確定、每次回答都不一樣，這些缺陷讓很多企業卻步。但 Sierra 執行長 Bret Taylor 提出一個反直覺的論點：AI 雖不完美，但比人類更一致。他以 defense-in-depth 風控框架和 outcomes-based pricing 商業模式，示範了如何把不完美轉化為可控的工程問題。"
tags: ["Bret Taylor", "AI Agent", "Sierra", "OpenAI", "Podcast"]
categories: ["AI 產業動態"]
image: "/images/posts/20260202-bret-taylor-imperfect-ai-agent-business-model.webp"
source_url: "https://www.youtube.com/watch?v=-ui7cg7Ar28"
source_name: "The Economist - Boss Class"
related_companies: ["sierra", "openai"]
related_people: []
draft: false
---

> 本文整理自《經濟學人》Podcast 節目 Boss Class 2026 年 1 月播出的單集。

{{< youtube -ui7cg7Ar28 >}}

{{< spotify "episode/3pEUEQ7r45Z3YNTYipJLDR" >}}

{{< apple-podcast "tw/podcast/interview-bret-taylor-of-sierra-and-openai/id1705972518?i=1000747184476" >}}

---

> **Bret Taylor 專訪系列（共三篇）**
> 1. [每家公司都需要 AI Agent，但別等它變完美](/posts/20260202-bret-taylor-every-company-needs-ai-agent/)
> 2. **本篇：AI 的不完美如何變成商業優勢**
> 3. [AI Agent 正在重新定義客戶體驗與工作](/posts/20260202-bret-taylor-ai-agent-customer-experience-jobs/)

## 每個人都在問同一個問題：AI 會犯錯怎麼辦？

企業導入 AI Agent 時，最常被問到的問題不是「它能做什麼」，而是「它犯錯了怎麼辦」。AI 模型會產生幻覺（hallucination），會編造不存在的資訊。更棘手的是，它是非確定性的（non-deterministic），同一個問題問兩次可能得到兩個不同的答案。這讓傳統軟體工程中「測試」和「穩健性」的概念變得很難套用，因為你沒辦法保證模型「永遠不會」做某件事。

OpenAI 董事會主席暨 Sierra 執行長 Bret Taylor 在接受《經濟學人》Boss Class 節目專訪時，並沒有迴避這個問題。他承認模型確實不完美，而且短期內不會變完美。但他接著拋出了一個讓很多人停下來想的反問：人就完美嗎？

在金融服務業，一個財務顧問向客戶承諾投資報酬率，在多數國家是違法的。但人就是會犯這種錯。這正是為什麼銀行要錄音每一通電話、保留每一份對話紀錄、設置層層合規機制來事後稽核。人類的不完美是金融監管體系存在的前提。Taylor 認為，我們應該用同樣的態度來看待 AI：不是等它完美，而是建立足夠的控制機制，讓它在不完美的狀態下仍然可控、可信賴。

但 Taylor 接著丟出了一個更尖銳的觀點：AI 雖然不完美，但它其實比人類更一致。一家大型金融服務公司每天和客戶的溝通中，有多少比例是不完美的？答案很可能是相當高的比例。只是我們對人類犯錯的容忍度天生比對機器高得多。如果把同樣的稽核標準套用在 AI 身上，它反而可能提升整體的控制穩健度（robustness of control）。

## 從科學問題到工程問題：縮窄場景是關鍵

面對 AI 的不完美，Taylor 提出了一個非常務實的框架：把使用場景縮窄，讓 AI 的缺陷從「科學問題」變成「工程問題」。

他舉了一個零售業的例子。如果你要 AI Agent 處理退貨，這個標準作業流程（SOP）非常具體：退貨期限 30 天、商品是否穿過、有沒有保留發票。幻覺的風險還是存在，但因為場景明確，你可以把護欄（guardrails）設得很精準。AI 要是說了什麼不在 SOP 範圍內的話，系統能立刻抓到。這就不再是一個「AI 到底能不能信任」的哲學問題，而是一個「我的工程控制做得夠不夠好」的實作問題。

反過來，如果你要 AI 做的是完全開放的通用推理，像 OpenAI 的研究團隊在追求的 AGI，那確實還是科學問題，因為你很難預測模型在所有情境下的行為。但對企業來說，多數需求是有邊界的。Taylor 的建議是：先從邊界最明確的場景開始，把經驗累積起來，等技術和監管都成熟了，再逐步拓展到更敏感的場景。

醫療產業是一個很好的例子。讓 AI Agent 幫病患預約骨科門診，風險很低，因為它只是在排程。但如果讓 AI 做初步分流診斷，判斷病患應該看哪一科，那就涉及醫療決策，風險等級完全不同。Taylor 觀察到，很多受監管產業已經在低風險場景上累積經驗，目的是等到技術和法規成熟時，他們已經有足夠的實戰經驗可以往高風險場景推進。

## Defense-in-Depth：用 AI 監控 AI

對於「AI 犯錯怎麼辦」這個問題，Taylor 借用了資安領域的概念來回答：defense-in-depth（縱深防禦）。

資安領域的邏輯是這樣的：你先把所有的門鎖好（預防層），但你不會天真到以為所有門都鎖得住，所以你再加上監控，確保一旦有人闖進來，你能快速偵測到並限制損害範圍。這不是單一防線，而是層層疊加的防禦機制，任何一層出了漏洞，其他層還能補上。

Sierra 把同樣的邏輯套用到 AI Agent 上。第一層是即時的「監督模型」（supervisor models），這些模型在背景即時監控主模型的每一個決策：它有沒有產生幻覺？有沒有偏離標準作業流程？有沒有違反護欄？這是在對話發生的當下就進行的檢查。

第二層是事後的深度評估。一個更強大、運算量更大的模型會在對話結束後完整回顧整段互動，看有沒有低情緒的對話、有沒有 AI 反覆說同樣的話、有沒有遺漏客戶的需求。如果偵測到問題，這些對話會被放進一個優先佇列，由人類審查員檢視。

這個設計的巧妙之處在於，它不是要人去看所有的對話，而是用 AI 把有問題的對話篩出來，讓人只看最關鍵的那些。Taylor 形容這是「把針放到乾草堆的最上面」，讓人的時間花在最需要介入的地方。整套機制的核心邏輯是：用 AI 和人類的組合，讓人類的注意力集中在最敏感、最棘手的案例上，而不是浪費在重複性的抽查裡。

## 做對了才收錢：outcomes-based pricing 的商業邏輯

Sierra 的商業模式本身就是對「不完美」的一種回應。他們採用的是 outcomes-based pricing（按成果計費）：只有當 AI Agent 成功解決了客戶的問題，Sierra 才收費。如果 AI 處理不了，需要轉接給真人，那就不收錢。

Taylor 解釋這背後的邏輯起點。四年前，軟體是人的生產力工具。你問一個業務員，是 CRM 系統還是你自己促成了這筆交易？業務員當然會說是自己。那個時代，你付的是「使用軟體的權利」，不管你用得好不好。但 AI Agent 改變了這個方程式。當一個 AI Agent 接起電話、處理完客訴、客戶表示滿意，你很清楚地知道這個問題是被 Agent 解決的。你也知道客戶的滿意度分數。在這種透明度下，按成果收費不但合理，而且更能對齊廠商和客戶的利益。

這個模式也隱含著對產品品質的信心。如果你的 AI Agent 動不動就要轉接真人，你就賺不到錢。這逼著 Sierra 必須持續改善 Agent 的品質，讓它真正能獨立解決問題。對企業客戶來說，這等於是零風險的嘗試：AI 做得好你才付錢，做不好你一毛不出。

## 我的觀察：「不完美也能上線」是一種管理思維的轉變

Taylor 最打動我的一句話是：如果你停止等待 AI 變完美，轉而問「我們有沒有足夠的技術和流程控制來辨識它不完美的時候，並且修正它」，對話就會變得非常有建設性。

這其實是一個管理思維的轉變。傳統的軟體導入邏輯是「上線前必須零缺陷」，因為傳統軟體是確定性的，同樣的輸入永遠產生同樣的輸出，所以你可以測試到完美。但 AI 不是這樣運作的。你沒辦法窮舉所有可能的對話情境，也沒辦法保證它永遠不犯錯。

接受這個現實之後，企業需要發展的能力不是「如何打造零缺陷的 AI」，而是「如何建立一套系統，讓 AI 的錯誤能被快速偵測、控制損害範圍、並且持續從錯誤中學習」。這和 DevOps 領域的 observability（可觀測性）概念其實是同一個邏輯：你不追求系統永遠不出錯，你追求的是出錯時能最快知道、最快修復。

臺灣很多企業還卡在「等 AI 完美再用」的心態，這會讓他們錯過累積經驗的窗口。Taylor 的 defense-in-depth 框架提供了一個很好的起點：你不需要信任 AI 是完美的，你只需要建立足夠的監控和護欄，讓它的不完美在可控範圍內。而按成果收費的模式也在降低企業的嘗試門檻。問題不再是「AI 夠不夠好」，而是「你的護欄夠不夠多」。
