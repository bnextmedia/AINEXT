---
title: "當 Scaling 撞牆，這間新 AI 實驗室押注「資料效率」"
date: 2026-01-29T10:00:00+08:00
description: "Google Ventures 投資的新 AI 實驗室 Flapping Airplanes，不追逐更大的模型與更多的算力，而是回頭問一個根本問題：為什麼人類只需要百萬分之一的資料，就能達到類似的智能？"
tags: ["Flapping Airplanes", "Google Ventures", "Scaling Laws", "資料效率", "AI 研究"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=46CqGthkTmQ"
source_name: "Google Ventures"
draft: false
---

> 本文整理自 Google Ventures 發布的投資宣布訪談影片。

{{< youtube 46CqGthkTmQ >}}

---

如果你在兩年前問 Ben Asher 要不要創辦一間 AI 實驗室，他會直接拒絕。不是因為沒信心，而是因為時機不對——當時整個產業還在驗證一個假說：只要持續擴大模型規模、餵入更多資料、堆疊更多算力，AI 就會變得更聰明。這個被稱為 Scaling Laws 的信念，主導了過去幾年幾乎所有頂尖實驗室的研究方向。

但現在不一樣了。Ben 在 Google Ventures 的訪談中坦言，Scaling 實驗已經跑完了，結果也很清楚：它確實有效，但它沒有神奇地解決所有問題。這讓他看到一個新的機會窗口。

## 一個反直覺的判斷：算力已經不是瓶頸

Ben 的核心論點相當反直覺。當整個產業還在為算力短缺焦慮、為資料中心選址傷腦筋時，他認為真正的限制因素已經轉變——我們現在處於一個「理想受限」（ideal-constrained）的時代，而非「算力受限」或「資料受限」的時代。白話說：我們需要的不是更多 GPU，而是更好的想法來運用這些已經存在的龐大運算資源。

這並非空泛的哲學宣言。Ben 指出了一個令人震驚的數字：人類大腦使用的「訓練資料」大約只有當前前沿模型的百萬分之一，卻能展現出更穩健、更具創造性的智能。這個差距不是小數點後的優化問題，而是數量級的根本差異。

如果能讓模型用百萬分之一的資料達到同樣效果，商業價值也是百萬倍的——企業不再需要蒐集數兆個 token 才能讓 AI 做好一份工作，而是可以用合理的資料量快速部署。

## 從大腦找靈感，但不是複製大腦

Flapping Airplanes 這個古怪的公司名稱，其實藏著他們的研究哲學。Ben 用飛機比喻現在的 AI：巨型客機靠著強大引擎和精密工程飛行，但鳥類用完全不同的方式達成同樣目的。他們不打算直接模仿鳥類（那會變成試圖複製人腦），但想借用一些鳥類的設計直覺，做出介於兩者之間的東西——會拍翅膀的飛機。

共同創辦人之一的 Aiden 曾在 Neuralink 工作，這段經歷讓他深刻體會到大腦的奇特權衡。人腦運作緩慢、資源有限，卻在某些任務上輕鬆超越最強大的前沿模型。為什麼？

團隊目前的一個研究方向是「稀疏性」（sparsity）。大腦極度稀疏——三歲小孩的突觸數量比成人多 60%，之後大量修剪。這暗示著一種非常不同於當前深度學習正統的學習機制。在傳統機器學習中，稀疏性是一種正則化手段，通常用於資料少、參數多的情境。但過去五年，整個領域都處於資料多、參數受限的狀態，所以稀疏性不受重視。

Ben 認為風向正在轉變。當算力變得充裕、高品質資料反而成為稀缺資源時，那些被忽視的、受大腦啟發的架構設計，可能會重新變得重要。

---

## 我的觀察

這場訪談讓我想起過去一年產業內部悄悄發生的論述轉向。2024 年底開始，越來越多頂尖研究者公開質疑「只要繼續 Scale 就會通往 AGI」的信念。Ilya Sutskever 離開 OpenAI 後創辦的 SSI、Anthropic 對 RLHF 的持續投入、甚至 Sam Altman 自己在多次訪談中承認「Pre-training 可能已經撞牆」——這些跡象都指向同一個方向：Scaling 不是終點，只是起點。

Flapping Airplanes 的有趣之處在於，他們不只是說「Scaling 不夠」，而是提出一個具體的替代賭注：資料效率。這讓我想到一個更大的問題：如果未來的 AI 競爭不再是「誰有更多 GPU」，而是「誰能用更少資料做到更多事」，那整個產業的競爭格局會如何重組？

這對資源有限的後進者來說，可能是個好消息。當遊戲規則從「軍備競賽」轉向「創意競賽」，小型但聰明的團隊就有了突圍的機會。Flapping Airplanes 的存在本身，就是這個轉變的早期信號。
