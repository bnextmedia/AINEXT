---
title: "把 AI 當成流氓國家來管理：Anthropic 政策主管 Jack Clark 的科幻式政策思維"
date: 2024-10-01T10:00:00+08:00
description: "Anthropic 政策主管 Jack Clark 提出用「流氓國家」框架來理解 AI 治理，並警告「機器時間」才是 AI 真正的風險所在。他認為 AI 的危險不在於它做了什麼，而在於它做事的速度。"
tags: ["Anthropic", "Jack Clark", "AI Policy", "AI 治理", "AI Safety"]
categories: ["AI 安全與治理"]
image: "/images/posts/20241001-anthropic-jack-clark-ai-policy.webp"
source_url: "https://www.youtube.com/watch?v=9eXV64O2Xp8"
source_name: "Anthropic YouTube"
related_companies: ["anthropic"]
related_people: []
draft: false
---

> 本文整理自 Anthropic YouTube 頻道 2024 年 9 月播出的對談。

{{< youtube 9eXV64O2Xp8 >}}

---

## 從資料中心的鐵皮屋到 AI 政策的談判桌

如果要我挑出 2024 年最值得認識的 AI 政策人物，Jack Clark 絕對在前三名。他是 Anthropic 共同創辦人暨政策主管（Head of Policy），同時也是 AI 圈知名電子報 Import AI 的作者。但他的起點，跟你想像的完全不一樣。

Clark 的職業生涯始於科技記者，而且是那種「方法演技」式的記者。寫資料庫的時候，他自學 SQL；寫半導體的時候，他去搞懂製造流程。他後來迷上了資料中心，開了一個叫「The Clark Side of the Cloud」的專題系列，到處參觀歐洲和全球各地的資料中心。聽起來像是一個科技宅的浪漫公路旅行，但就是在這些散發著冷氣和嗡嗡聲的鐵皮屋裡，他看到了未來的輪廓。大約在 2010 年，他意識到 Google 遲早會把機器學習丟到這些大規模運算叢集上。於是他搬到美國，開始報導當時還幾乎沒人關注的 AI 領域，還給自己封了一個頭銜：全世界唯一的神經網路記者。2012 年的時候，這個頭銜確實沒人跟他搶。

2016 年，Clark 加入了 OpenAI，很快就把重心轉向了政策。原因很直接：他發現這個技術的重要性正在急速攀升，但政策圈的人對它的理解幾乎是零。從那之後，他就一直站在 AI 與政府之間的翻譯地帶，試圖讓兩個說完全不同語言的世界互相理解。這段經歷讓他發展出了一套獨特的政策思維，既有科技記者的直覺，又有 AI 實驗室內部人的視角。在這場由 Anthropic 研究者 Stuart Ritchie 主持的對談中，Clark 把這些年的思考攤了出來，從英國工黨的 AI 策略，聊到他那個聽起來像科幻小說但其實非常實用的「流氓國家」理論。

---

## 英國工黨的 AI 算盤：安全是底線，經濟才是重點

這場對談的背景很有意思。Clark 剛從英國工黨（Labour Party）年會回來，而工黨在 2024 年重新執政後，正面對一個巧妙的政治遺產：前任保守黨政府留下了 AI 安全研究所（AI Safety Institute，簡稱 AISI，英國人念作「AC」）。順帶一提，美國版的 AI 安全研究所則被暱稱為 ACDC，雖然美國政府從未正式認可這個名字。通常新政府上台，第一件事就是把前朝的東西拆個精光，但工黨沒有這樣做。Clark 對此印象深刻，他觀察到工黨不但認可 AISI 的價值，還在思考如何擴大它的影響力。這在政治上是相當罕見的務實態度。

那麼工黨到底怎麼看 AI？Clark 的判斷是：他們關心安全，但更關心經濟。工黨執政後幾乎言必稱「成長」，而 AI 被他們放進了推動英國經濟成長的核心工具箱裡。在安全方面，他們關注的是會對民眾「生命和安全造成實際傷害」的風險，這個框架跟美國政府類似，聚焦在生物武器、網路攻擊這類災難性風險上。至於 AI 研究者常談的那些更深層、更科幻的存在性風險，目前還不在工黨的優先議程上。Clark 也很坦白地說，他在年會上刻意沒有打開那個裝滿恐怖情境的公事包，而是先談怎麼讓 AISI 持續壯大，因為這對安全本身就有長期價值。

Stuart Ritchie 追問了一個好問題：工黨是不是把 AI 的風險想成跟社群媒體差不多的東西？Clark 的回答很務實。他說工黨才上任兩個月，正在被各種資訊轟炸，需要一段時間來消化和學習。他也不急，因為他知道真正需要擔心的那些更怪異的風險，遲早會浮上政策議程的。

我覺得這反映了一個現實：對大多數政府來說，AI 首先是一個經濟議題，其次才是安全議題。這不是因為他們天真，而是因為政治運作的時間尺度跟 AI 風險的時間尺度完全不同。一個剛上任兩個月的政府，面對的是選民對經濟的即時焦慮，而不是二十年後可能出現的超級智慧。Clark 選擇從經濟切入，再慢慢引導到安全議題，這種策略本身就是一堂政策溝通課。

---

## 不只是後台自動化：AI 的經濟潛力比你想的更深

當 Ritchie 問到 AI 怎麼幫助英國經濟時，Clark 用了一個非常具象的描述：大多數企業的內部，其實就是一堆紙本流程的集合。從客戶接觸到完成服務，中間是層層疊疊的文件、分類、審核、轉交。語言模型最立即的價值，就是處理這些「紙管道」（paper plumbing）。NHS（英國國民健康服務）每天產生的文件量驚人，國會議員每天被選民的陳情信淹沒，這些都是語言模型可以立刻上手的場景：閱讀、分類、摘要、排序優先級。不是取代人類決策，而是讓人類終於能「看見」那座被埋在文件堆裡的資訊山。

但 Clark 說了一句更有意思的話：就算我們今天把 AI 的發展完全凍結，不再訓練任何新模型，光是現有的系統就足以帶來巨大的經濟效益。他用電力做比喻。我們剛發現電力的時候，做的第一件事是在舊工廠裡掛上燈泡。但電力真正改變世界，是人們開始「圍繞電力的存在」設計全新的工廠、全新的生產線。AI 也是一樣，我們現在還在「掛燈泡」的階段，但即便停在這裡，某個青少年遲早會找到我們完全沒想過的用法。Clark 預測，這樣的驚喜會有數十甚至數百個。

這個觀點值得停下來想一想。很多人把 AI 的價值綁在「模型會不會繼續進步」上，好像一旦 Scaling Laws 碰壁，一切就完了。但 Clark 提醒我們，技術的真正滲透需要時間，而我們連第一波滲透都還沒走完。這就像有人在 1995 年說「如果網路不再變快，還有什麼用？」結果網路在既有的頻寬上，又花了十年才真正長出電商、社群媒體和串流影音。AI 的經濟影響，很可能也會是這樣一個比技術進步本身更漫長的展開過程。

---

## 不是計算機，是文明的鏡子：重新認識 AI 的本質

對談進行到這裡，話題從經濟實用轉向了更根本的問題：AI 到底是什麼？Ritchie 引用了 Clark 在 Import AI 電子報上的一段話，將 AI 描述為「創造性的鏡子」（creative mirrors）、「人類無意識的機器精靈」（machine spirits of the human unconscious）、「價值觀的模擬體」（value simulacras）。Clark 強調，我們面對的不是計算機，不是簡單工具，而是在高維空間中編碼了人類文明的巨大產物，並且能把這些文明映照回來。這段話聽起來很文學，但 Clark 堅持它有非常實際的政策含義。

他是這樣解釋的：一把錘子對要敲哪根釘子沒有任何偏好。但 AI 系統不同，它們擁有某種「人工直覺」（artificial intuition）。這些系統繼承了訓練資料中的價值觀，展現出許多理性人會稱之為「創造力」或「直覺」的行為。品質當然參差不齊，但重點是：我們有了一把「會創造的錘子」。這在人類造物的歷史上從未發生過。我們從來沒有製造出「理解人類世界某些面向，並從中繼承了某些本能」的工具。

Clark 把這個觀察推到了一個大膽的類比。他在聯合國安理會上說過，AI 更像是我們找到了一種方法來「模擬人的某些面向」，並且「延伸國家運作方式的某些面向」。這些 AI 系統就像「矽基國度」（Silicon Countries），帶著各種驚人的能力被進口到我們的世界。這聽起來確實像科幻小說，但 Clark 的邏輯鏈是清晰的：如果你承認 AI 不只是工具，而是具有某種類人特質的實體，那麼你就不能用管理工具的方式來管理它，你需要一套全新的治理框架。而這就把我們帶到了他最核心的政策主張。

---

## 流氓國家理論：用國際關係的框架理解 AI 治理

Clark 跟各國政府談 AI 已經很多年了。他發現一個問題：每次談話的主題不斷在變。先是自駕車，然後是電腦視覺，再來是 AlphaGo 和強化學習，現在是語言模型。但如果你拉開來看，真正改變世界的不是某一項技術，而是「公用事業等級的運算」（utility-scale computing）整體到來。而這種規模的變化，需要政府用比「某個部門負責某個問題」更全面的方式來回應。

所以 Clark 開始對各國政府提出一個新框架：把 AI 系統想像成正在降臨到世界上的國家，把未對齊的 AI 系統想像成流氓國家。這個比喻為什麼有用？因為當你跟政府談 AI 安全，你可能分別談到生物武器風險、網路安全風險、釣魚詐騙風險，而這些議題分屬不同的政府部門。但如果你說「有一個新的國家正在做你不理解的壞事」，政府就會意識到他們需要的是「全政府回應」（whole-of-government response），而不是把問題拆成碎片分別處理。Clark 認為，用國家而非技術的框架來思考 AI，會讓整個治理架構瞬間變得合理許多。

這個比喻的延伸也很精彩。Ritchie 提到 Anthropic 投入大量資源在「可解釋性」（interpretability）研究上，試圖理解語言模型內部到底在發生什麼。Clark 接過話頭，做了一個讓人印象深刻的對比：Anthropic 的可解釋性團隊在做的事，跟 CIA 分析北韓或伊朗內部動態的工作，本質上屬於同一類問題。AI 系統是不透明的，我們拼命想理解它們，因為它們有巨大的價值，同時也有潛在的風險。國家也是不透明的，流氓國家更是不透明且危險。我們面對這兩種實體時，用的方法其實非常類似。

Ritchie 接著提出了一個尖銳的反問：如果用流氓國家來比喻，我們的信心應該有多大？畢竟國際社會在把流氓國家「拉回正軌」這件事上，成績單並不好看。Clark 沒有迴避這個問題，但他也點出了樂觀的一面。冷戰結束後，許多東歐國家成功轉型，改變了價值體系，建立了新的制度，融入了全球經濟體系。甚至俄羅斯在某段時期也有類似的趨勢，只是後來鐘擺又擺了回去。更關鍵的是，AI 系統跟真實國家有一個根本性差異：國家運行在「人類時間」上，干預點少；AI 運行在「機器時間」上，干預點多。這既是挑戰，也是機會。Clark 認為我們可以建立技術性的手段來理解這些系統、建立信任、發展信心，而且速度可以比處理國際關係快得多。

我覺得「流氓國家」理論最聰明的地方，不在於它多精確，而在於它提供了一個政策制定者已經熟悉的思考框架。你不需要跟一個政治人物解釋什麼是 transformer 架構，你只需要說：「想像一個新國家突然出現，有巨大的能力，你看不懂它在想什麼。你會怎麼做？」這個問題，每個國安官員都知道怎麼開始思考。

---

## 機器時間：AI 風險的真正核心

如果說「流氓國家」理論是 Clark 最好的政策溝通工具，那麼「機器時間」（machine time）就是他認為 AI 風險最有說服力的論證。他引用了加州理工學院研究者的一篇論文：人類的思考速度大約是每秒 10 位元，但我們的感官輸入卻高達每秒 1 GB。這之間有一個巨大的落差。而未來的 AI 系統，思考速度將遠遠超過人類。

Clark 用了一個任何人都能理解的比喻：你有沒有試過抓蒼蠅或蚊子？牠們比你快，比你靈活，運作在比你更高的時脈頻率上。蜂鳥也是一樣。你基本上抓不到。那麼，當 AI 系統在速度上超越人類的幅度，遠大於蒼蠅超越人類的幅度時，我們憑什麼認為自己能控制它？Ritchie 也呼應了這個觀點。他說，很多人很難想像「比最聰明的人類還要聰明」是什麼概念，但「比人類快」卻是每個人都能直覺理解的。你看 Claude 產生文字的速度就知道了。想像如果它以同樣的速度在執行實際行動，那畫面就很具體了。

Clark 進一步援引軍事上的 OODA 循環（觀察、判斷、決策、行動）概念，這是理解速度優勢最經典的框架。在現代戰爭中，各國花費巨大資源就為了讓自己的 OODA 循環比對手快那麼一點點。而歷史反覆證明，誰的循環更快，誰就贏。但這還只是人類之間、在同一個時間量級上的競爭。如果對手是一個比你快十倍的機器系統呢？軍事史上的所有案例都告訴我們，速度慢的那一方幾乎注定輸。Clark 認為，這個論證之所以有力，是因為它不需要你相信 AI 會變得「比人聰明」這個有爭議的前提，它只需要你接受一個毫無爭議的事實：機器運算比人腦快。光是這個速度差，就足以構成嚴峻的風險。

他自己也承認，作為 Anthropic 內部對 AI 安全議題相對後知後覺的人，「機器時間」是第一個真正說服他的框架。那不是一個理論上的擔憂，而是一個你看著 Claude 每秒吐出數百個 token 就能直覺感受到的現實。

---

## 用 AI 管 AI：速度問題的務實回應

那麼面對速度差距，我們能做什麼？Clark 提出了幾個方向，雖然他坦承都不是銀彈，但至少指出了正確的思考路徑。

首先是「用 AI 管 AI」的策略。Anthropic 的負責任擴展政策（Responsible Scaling Policy）和產品信任安全機制就體現了這個邏輯。人類無法以人類的速度審查 AI 的每一個輸出，但可以訓練專門的語言模型分類器來監控主要的 AI 系統，在它偏離軌道時介入。Clark 解釋得很具體：Anthropic 的信任安全過濾器會標記出可疑內容，然後由專門訓練的 AI 分類器來判斷並執行相應的處置。人類在這個流程中設定規則和目標，但實際的即時監控是由機器對機器完成的。這其實跟我們用防毒軟體來對抗電腦病毒的邏輯一樣，你不會要求每個使用者自己去逐行檢查程式碼。

其次是「速度限制」的概念。Clark 提到，我們可能需要開始思考 AI 系統的「適當介面速度」（appropriate interface speeds）。這可以是限制 AI 代理程式（agent）接收資訊和輸出資訊的速率，也可以是在 API 層面設置人工限速器。這聽起來像是在跑車上裝限速器，確實會犧牲一些效能，但在我們還不確定這輛車的煞車系統是否可靠之前，限速是一個合理的暫時措施。

講到這裡，Clark 自己也忍不住笑了。他說：「我們開頭談的是 AI 怎麼幫忙處理後台文書作業，結果現在在討論極速移動的機器智慧和它們的詭異特性。」Ritchie 補了一句：「對，它們可能在我們背後以光速發展出我們沒預期到的動機。」Clark 回了一句堪稱整場對談最精彩的金句：「它也很擅長摘要和寫程式。兩件事都是真的，這就是這個問題最讓人困惑的地方。」AI 同時是最無聊的辦公室助手和最詭異的潛在威脅，而我們必須同時面對這兩個面向，不能選擇性地只看其中一個。

---

## 匿名研究者：AI 邊疆的賽博龐克探險家

對談中最讓我感到新鮮的段落，是關於一群匿名的 AI 研究者。這些人用 Janus、Pliny the Prompter 這樣的網路代號，在各個 AI 系統的邊界上進行實驗。他們讓不同的 AI 互相對話，試圖越獄（jailbreak）各種模型的安全機制，用各種非正統的方法探測 AI 的極限。Clark 說，這群人可能花在跟 AI 對話的時間比任何一間實驗室的員工都多，累計數千小時甚至更多。

Clark 對這個現象的定位很有意思：他稱之為「透過藝術進行的科學」（science through art）。有些實驗可以用已知的科學方法分析，但有些更像是遊戲、劇場和心理學的混合體，由一群擁有獨特直覺、略微偏離主流共識的人在執行。當他觀察這些實驗時，他覺得這是我們正在處理「真正奇怪的技術」最有說服力的證據之一。你可以看到不同模型之間存在明顯的差異，而且你必須正視這些差異。這些匿名探索者就像「前沿的探險家」（explorers on the frontier），用主流研究者可能不會採用的方法，揭示了 AI 行為中被忽略的面向。

Ritchie 補充說，每當 Anthropic 推出新版 Claude，這些人是最先注意到「人格是否改變」的群體。他們會說「這個版本的 Claude 性格變了」，用的是一種完全自然的語氣，好像在談論一個認識的人。Clark 承認，他和 Ritchie 在討論這些話題時可能需要加上各種引號和但書，但對這些匿名研究者來說，這就是他們與 AI 互動的日常語言。Ritchie 形容他們活在一部科幻小說裡，每天跟 AI 對話就像跟機器人聊天一樣自然。Clark 笑說，他們大可以在名片上印「機器心理學家」。

這些匿名社群是 AI 安全生態中一個被嚴重低估的力量。實驗室內部的安全研究受限於組織流程和研究規範，而這些外部探索者沒有這些限制，他們的發現經常補上正規研究的盲點。對 AI 實驗室來說，這是免費的、大規模的、由高度專注的使用者執行的紅隊測試。我認為，未來 AI 安全領域最重要的洞察，很可能不會來自頂尖大學或企業實驗室，而是來自某個深夜還在跟 Claude 聊天的匿名帳號。

---

## 科幻小說作為政策工具：Clark 的「Tech Tales」

Clark 的 Import AI 電子報每週末尾都有一個叫做「Tech Tales」的短篇小說欄位。他從一個聽起來很個人的動機開始解釋這件事的緣由。他提到 Jawbreaker 樂團有一首歌，裡面有句話讓他一直記在心裡，大意是「我的虛構比我的真實更真實」。Clark 說，他寫這些故事是為了透過想像 AI 相關的情境來消化身邊正在發生的一切。而且他發現，這些故事裡藏著比電子報本身更多關於他在 AI 實驗室工作的「真實感受」。

更有趣的是他後來開始做的一件事：把所有寫過的故事餵給 Claude，請它猜測作者是什麼樣的人。隨著 Claude 的版本升級，它對 Clark 性格的推測變得越來越準確。他甚至會問 Claude「你覺得實驗室裡正在發生什麼？」而 Claude 推測的答案，有些跟他在 Anthropic 的真實經歷詭異地吻合，即便那些事情從未出現在任何一篇故事裡。這個過程讓 Clark 用「美妙的遞迴」（wonderfully recursive）來形容。他也坦承，在正式的政策場合說「我們正在面對一個像外星智慧一樣注視著我們的東西」不太合適，但他可以把這個感受寫成短篇小說，塞在同一份電子報的末尾，送進同一批讀者的收件匣。他稱之為「走私進去的一份怪」。

最讓人起雞皮疙瘩的是這些故事的預言性。Clark 寫過一篇叫 The Id Point 的故事，描述 AI 在某個參數門檻開始展現類似情境意識和不適感的行為。幾週後，Nous Research 發布的一個新模型就在特定參數規模下出現了類似的現象。他還寫過一篇叫 Replay Grief 的故事，描述一個男人跟妻子對話，但讀到最後你發現他是在跟妻子去世後用語言模型建立的模擬體說話。幾個月後，紐約時報刊出了一篇報導，內容幾乎一模一樣：一位女性把已故伴侶的文字餵進語言模型，然後跟它對話。Clark 說他覺得非常不可思議。我覺得這不只是巧合，而是一個在產業核心工作的人，其直覺已經跑在事實前面的結果。科幻小說在這裡不只是娛樂，它是一種預警系統。

---

## 好處與風險來自同一個源頭

對談中有一段辯論特別精彩，涉及 AI 作為「知識顧問」的雙面性。Clark 指出，人類繁榮最大的阻礙，往往是缺乏教育資源、缺乏顧問、缺乏有時間陪伴的指導者。AI 系統在這方面展現了巨大的潛力：人們用它來回答日常問題、學習新技能、閱讀科學論文、學習語言。他特別提到一個例子：一個滑鐵盧大學的畢業生，從未碰過硬體，用 Claude 指導自己在大約兩週內造出了一台核融合裝置，就放在臥室裡。Clark 說這個年輕人的態度非常理所當然：「嗯，天上那個 AI 大腦幫我造了一台核融合器。」這種事情正在發生，而且會越來越多。

但這枚硬幣的另一面同樣尖銳。Clark 直言，很多危險之所以沒有在現實中發生，僅僅是因為想做壞事的人數量少，而且接觸不到有能力的顧問。AI 改變了這個方程式。它為好人提供了加速器，但也同時為壞人提供了加速器。這種「差異化加速」（differential acceleration）是 AI 安全最核心的挑戰之一。Clark 強調，Anthropic 經常被外界貼上「末日論者」的標籤，但他的立場其實更微妙：如果我們想要這個技術帶來的所有好處，就必須同時面對它可能為惡意行為者提供的助力。模型持續在進步，世界上始終存在想造成傷害的人，我們不能忽視這個交叉點。

Ritchie 接著提出了一個精闢的觀察：那些聲稱 AI 完全沒有風險的人，某種程度上其實不太相信 AI 真的很強大。因為如果你承認 AI 能帶來巨大的好處，邏輯上你就很難否認它也能被用來造成巨大的傷害。否認風險的存在，等於否認了這個技術的力量。Clark 分享了他最近聽到的一個觀察，讓這個論點變得更加犀利：「今天的加速主義者（accelerationist）其實是技術悲觀主義者。」為什麼？因為他們假設 AI 只會從現在的水準稍微再進步一點，然後就停下來。如果你是真正的加速主義者，面對技術持續突破的前景，你應該同時感受到震撼、敬畏，以及一小部分對後果的恐懼。

---

## 政策前線：2025 年即將到來的風暴

對談最後一大段聚焦在 AI 政策的未來地圖。Clark 列舉了即將發生的關鍵事件，密度驚人。首先，AI 安全峰會的接力賽還在繼續。2023 年在英國布萊切利莊園（Bletchley Park）舉辦了第一屆，2024 年在首爾舉辦了第二屆，2025 年 2 月將在法國舉辦第三屆，各國將繼續協商如何協調 AI 安全事務。然後是美國總統大選。無論哪位候選人勝出，Clark 預期新政府上任後的前一百天都會在 AI 議題上有所動作。最後是歐盟 AI 法案（AI Act）進入實施階段，到了 2025 年下半年，包括 Anthropic 在內的 AI 公司都將正式受到歐洲的某種程度的監管，歐盟執委會和 AI 辦公室必須搞清楚測試、評估和合規的具體意涵。

但不只是這些檯面上的活動。Clark 提到一個更深層的趨勢：世界各地正在快速成立各種 AI 安全機構。英國有 AISI，美國有 US AISI，加拿大和日本也在建立自己的機構，還有一些國家的計畫他知道但不方便公開。Clark 用了一個精彩的比喻：這些機構就像各國對這個新興「矽基國家」設立的大使館。一個由政府「大使館」組成的全球網路，正在圍繞 AI 成形。

中國也沒有缺席。Clark 觀察到，中國正越來越頻繁地在聯合國場合提出 AI 議題。背後的邏輯是，中國認為自己在目前由西方主導的國際 AI 對話中被邊緣化了，所以選擇用聯合國作為平台來重新介入。就在對談錄製的同一週，Anthropic 在美國國務院的場合宣布將以補貼方式向全球使用者提供 Claude，其他公司也做了類似的宣布。國際 AI 政策正在快速從學術討論變成具有實際約束力的制度建設。Clark 總結這段時說：「如果有人以為 AI 正在放慢腳步，那麼所有政府現在都已經醒了。2025 年，我們會看到政策領域發生瘋狂的新事。」

---

## Clark 對政策制定者說的最後一句話

當 Ritchie 問到 Clark 跟政策制定者交流時，最想傳達的一件事是什麼，他的回答非常直接。他告訴那些政治人物：Dario Amodei、Sam Altman、Demis Hassabis，這些 AI 公司的領導者談 AGI（通用人工智慧）的時候，那不是行銷術語，那是他們真心相信的事情。他們相信自己有機會建造出具備人類水準創造力、且以機器速度運行的通用合成智慧。如果他們之中有任何一個人成功了，那將是完全改變世界的事件。

Clark 用了一個「兩種情境」的論述來框架這個訊息。情境一：如果 AI 實驗室的人都錯了，技術碰壁了，那太好了，我們依然會得到大量好處和少量可控風險。情境二：如果他們是對的，AI 持續變得更強，那我們就需要全新的制度、全新的治理系統，去面對既有巨大豐裕又有巨大威脅的未來。無論哪種情境，提前準備都不會是壞事。Clark 強調，無論好的方面還是壞的方面，規模都不會是「正常的」。它們將是「異常巨大」的。這才是政策制定者需要理解的核心訊息。

在對談最後，Clark 被問到普通人該怎麼跟上 AI 的發展。他的建議出乎意料地簡單：用它。他回憶早期版本的 Claude 只是個有趣的玩具，但隨著他累積了大量對話經驗，加上技術本身的進步，現在他真正把它當成日常工具。他建議所有人都去試試免費版的 Claude，然後開始探索。但更深層的建議是關於創造力的。Clark 認為，在樂觀情境下，一個人能做什麼的唯一限制，將會是他自己的創造力。他提到自己有小孩，一直在想該怎麼幫他們準備面對這個世界。他的結論是：用這些工具，但更重要的是，保持創造力。因為所有出人意料的 AI 用途，都來自有創意的人。

---

## 我的觀點：速度才是真正的恐怖谷

聽完整場對談，最讓我反覆咀嚼的不是「流氓國家」的比喻，雖然那確實是非常出色的政策溝通工具。真正改變我思考方式的是「機器時間」的概念。

過去我想到 AI 風險，腦海中的畫面大多是「AI 做了某件可怕的事」：製造生物武器、發動網路攻擊、操控選舉。但 Clark 把問題翻轉了：重點不是 AI 做了什麼，而是它做的速度。一個跟你智力相當但速度是你十倍的對手，在實際意義上就等於一個比你聰明十倍的對手。因為在你完成一次思考的時間裡，它已經思考了十次、嘗試了十次、修正了十次。速度不只是效率的量化指標，它是一種質變。蒼蠅不比人類聰明，但你就是打不到牠。

這讓我想到一個更不舒服的推論。我們目前所有的 AI 安全機制，從 RLHF 到紅隊測試到負責任擴展政策，本質上都是以「人類時間」設計的制度。但它們要對付的是一個以「機器時間」運行的實體。這就像用腳踏車的交通規則去管噴射機。規則本身可能沒有錯，但執行速度完全跟不上。Clark 提出的「用 AI 監控 AI」是目前最務實的方向，但它本身也帶來了一個遞迴的問題：監控用的 AI 也運行在機器時間上，誰來監控監控者？

Clark 在對談中用了很多科幻式的語言，但他要傳達的訊息其實非常務實：AI 治理的真正挑戰，不是我們缺乏正確的價值觀或制度設計，而是我們所有的制度都建立在一個隱含前提上，就是被管理的對象跟管理者運行在同一個時間尺度上。一旦這個前提不再成立，我們需要的不是更好的規則，而是一套全新的治理時間觀。Clark 沒有給出完整的答案，但他問對了問題。而在 AI 政策這個所有人都還在摸索的領域裡，問對問題的人，往往比宣稱有答案的人更值得信任。
