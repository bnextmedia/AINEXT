---
# ═══════════════════════════════════════════════════════════════
# 基本資訊
# ═══════════════════════════════════════════════════════════════
title: "理察·薩頓"
slug: "richard-sutton"
name_en: "Richard S. Sutton"
name_zh: "理察·薩頓"
nickname: "強化學習之父"
name_variations:
  - "Rich Sutton"
  - "Richard Sutton"
  - "RL 之父"

# ═══════════════════════════════════════════════════════════════
# 照片
# ═══════════════════════════════════════════════════════════════
photo: "/images/people/richard-sutton.jpg"
photo_alt: "Richard Sutton 頭像照"
photo_credit: "Wikimedia Commons, CC BY-SA 4.0"
photo_source_url: "https://commons.wikimedia.org/wiki/Category:Richard_S._Sutton"
photo_year: 2024

# ═══════════════════════════════════════════════════════════════
# 個人背景
# ═══════════════════════════════════════════════════════════════
nationality: "美國/加拿大"
birth_year: 1956
birthplace: "美國"

# ═══════════════════════════════════════════════════════════════
# 現職
# ═══════════════════════════════════════════════════════════════
current_position: "教授、研究科學家、首席科學顧問"
current_organization: "阿爾伯塔大學 / Keen Technologies / Amii"
current_org_slug: ""

# ═══════════════════════════════════════════════════════════════
# 人物分類
# ═══════════════════════════════════════════════════════════════
person_type:
  - "科學家"
  - "圖靈獎得主"

ai_domain:
  - "強化學習"
  - "時序差分學習"
  - "馬可夫決策過程"
  - "策略梯度"
  - "Actor-Critic"

ai_impact_areas:
  - "大型語言模型"
  - "遊戲 AI"
  - "機器人控制"
  - "自駕車"
  - "推薦系統"

# ═══════════════════════════════════════════════════════════════
# 重要性標籤
# ═══════════════════════════════════════════════════════════════
importance_level: "A"
importance_summary: "強化學習之父，定義了整個 RL 領域的理論框架和核心演算法。沒有他的工作，就沒有 AlphaGo、沒有 ChatGPT 的 RLHF"

key_contributions:
  - "時序差分學習（TD Learning）"
  - "Actor-Critic 架構"
  - "Dyna 架構（整合學習與規劃）"
  - "選項框架（Options Framework）"
  - "策略梯度方法的形式化"
  - "《強化學習導論》教科書"
  - "「苦澀的教訓」論文"

# ═══════════════════════════════════════════════════════════════
# 獎項與榮譽
# ═══════════════════════════════════════════════════════════════
awards:
  - name: "ACM 圖靈獎"
    year: 2024
    category: "計算機科學"
    shared_with: ["Andrew Barto"]
    citation: "For developing the conceptual and algorithmic foundations of reinforcement learning"
  - name: "英國皇家學會院士"
    year: 2021
    category: "科學"
    shared_with: []
    citation: ""
  - name: "加拿大皇家學會院士"
    year: 2016
    category: "科學"
    shared_with: []
    citation: ""
  - name: "AAAI Fellow"
    year: 2001
    category: "人工智慧"
    shared_with: []
    citation: "For significant contributions to many topics in machine learning, including reinforcement learning, temporal difference techniques, and neural networks"

# ═══════════════════════════════════════════════════════════════
# 學經歷
# ═══════════════════════════════════════════════════════════════
education:
  - institution: "麻州大學阿默斯特分校"
    degree: "電腦科學博士"
    year: 1984
  - institution: "麻州大學阿默斯特分校"
    degree: "電腦科學碩士"
    year: 1980
  - institution: "史丹佛大學"
    degree: "心理學學士"
    year: 1978

career_history:
  - organization: "阿爾伯塔大學"
    position: "教授"
    years: "2003-至今"
    org_slug: ""
    highlight: "創立強化學習與人工智慧實驗室（RLAI）"
  - organization: "Google DeepMind"
    position: "傑出研究科學家"
    years: "2017-2023"
    org_slug: "deepmind"
    highlight: "協助創立 DeepMind Alberta"
  - organization: "Keen Technologies"
    position: "研究科學家"
    years: "2023-至今"
    highlight: ""
  - organization: "AT&T Labs"
    position: "首席技術人員"
    years: "1998-2002"
    highlight: ""
  - organization: "GTE Laboratories"
    position: "首席技術人員"
    years: "1985-1994"
    highlight: ""

# ═══════════════════════════════════════════════════════════════
# 代表性著作/論文
# ═══════════════════════════════════════════════════════════════
key_publications:
  - title: "Reinforcement Learning: An Introduction"
    year: 1998
    venue: "MIT Press（第二版 2018）"
    coauthors: ["Andrew Barto"]
    citation_count: "75,000+"
    significance: "強化學習領域的聖經級教科書，培養了整整一代 RL 研究者"
  - title: "Learning to Predict by the Methods of Temporal Differences"
    year: 1988
    venue: "Machine Learning"
    coauthors: []
    citation_count: "10,000+"
    significance: "時序差分學習的奠基論文"
  - title: "Policy Gradient Methods for Reinforcement Learning with Function Approximation"
    year: 1999
    venue: "NeurIPS"
    coauthors: ["David McAllester", "Satinder Singh", "Yishay Mansour"]
    citation_count: "10,000+"
    significance: "策略梯度方法的形式化，是 PPO、RLHF 等現代方法的基礎"
  - title: "The Bitter Lesson"
    year: 2019
    venue: "個人網站"
    coauthors: []
    citation_count: "數百次正式引用，業界廣泛傳閱"
    significance: "影響深遠的 AI 哲學論文，主張通用方法加算力終將勝過人類設計的啟發式方法"

# ═══════════════════════════════════════════════════════════════
# 組織經歷
# ═══════════════════════════════════════════════════════════════
founded_companies:
  - name: "RLAI Lab（強化學習與人工智慧實驗室）"
    year: 2003
    outcome: "運作中"
    description: "阿爾伯塔大學的研究實驗室，培養超過 100 位 RL 研究者"
  - name: "Openmind Research Institute"
    year: 2023
    outcome: "運作中"
    description: "專注於 AI 學習與智慧基礎問題的非營利研究機構"

# ═══════════════════════════════════════════════════════════════
# 師承與門生
# ═══════════════════════════════════════════════════════════════
mentors:
  - name: "Andrew Barto"
    relation: "博士導師"

notable_students:
  - name: "David Silver"
    current_role: "DeepMind 首席研究科學家、AlphaGo 之父"
    people_slug: ""
  - name: "Doina Precup"
    current_role: "DeepMind 蒙特婁負責人、McGill 教授"
    people_slug: ""
  - name: "Csaba Szepesvári"
    current_role: "DeepMind 研究科學家"
    people_slug: ""

# ═══════════════════════════════════════════════════════════════
# 重要觀點與言論
# ═══════════════════════════════════════════════════════════════
notable_quotes:
  - quote: "70 年 AI 研究最大的教訓是：利用算力的通用方法最終是最有效的，而且優勢巨大"
    context: "「苦澀的教訓」論文核心觀點"
    date: "2019-03"
    source: "The Bitter Lesson"
  - quote: "我們必須學到這個苦澀的教訓：把我們認為的思考方式建進 AI，長期來看行不通"
    context: "批評依賴人類啟發式方法的 AI 研究"
    date: "2019-03"
    source: "The Bitter Lesson"
  - quote: "我們需要能像我們一樣發現的 AI 代理，而不是包含我們已發現事物的代理"
    context: "談 AI 設計哲學"
    date: "2019-03"
    source: "The Bitter Lesson"

stance_on_ai_safety: "務實派"
ai_safety_summary: "相對務實，認為 AI 研究應專注於通用學習方法而非人為設計的約束。主張讓 AI 自己學習，而非將人類知識硬編碼進去"

# ═══════════════════════════════════════════════════════════════
# 關聯
# ═══════════════════════════════════════════════════════════════
related_people:
  - slug: "andrew-barto"
    relation: "博士導師、圖靈獎共同得主、終身合作者"
  - slug: "david-silver"
    relation: "學生"

related_companies:
  - slug: "deepmind"
    relation: "前任職"

# ═══════════════════════════════════════════════════════════════
# 社群連結
# ═══════════════════════════════════════════════════════════════
social:
  twitter: ""
  linkedin: ""
  google_scholar: "https://scholar.google.com/citations?user=6m4wv6gAAAAJ"
  wikipedia_en: "https://en.wikipedia.org/wiki/Richard_S._Sutton"
  wikipedia_zh: ""
  personal_website: "http://www.incompleteideas.net/"

# ═══════════════════════════════════════════════════════════════
# 搜尋關鍵字
# ═══════════════════════════════════════════════════════════════
tags:
  - "Richard Sutton"
  - "薩頓"
  - "強化學習"
  - "Reinforcement Learning"
  - "RL"
  - "圖靈獎"
  - "時序差分學習"
  - "TD Learning"
  - "Q-Learning"
  - "Actor-Critic"
  - "阿爾伯塔大學"
  - "DeepMind"
  - "苦澀的教訓"
  - "Bitter Lesson"
  - "RLHF"
  - "AlphaGo"

# ═══════════════════════════════════════════════════════════════
# 元資料
# ═══════════════════════════════════════════════════════════════
description: "理察·薩頓（Richard Sutton）是「強化學習之父」，2024 年圖靈獎得主。他定義了強化學習的理論框架和核心演算法，他的著作《強化學習導論》培養了整整一代研究者。沒有他的工作，就沒有 AlphaGo、沒有 ChatGPT 使用的 RLHF 技術。"
last_updated: "2026-01-26"
data_sources: ["Wikipedia", "Google Scholar", "ACM", "University of Alberta", "Amii"]
draft: false
---

## 一句話認識他

> **理察·薩頓定義了「強化學習」這個領域——AlphaGo 打敗世界棋王、ChatGPT 學會符合人類偏好的回答，背後都是他發明的演算法。**

---

## 為什麼他對 AI 如此重要？

想像你在訓練一隻狗。狗做對了動作，你給牠零食獎勵；做錯了，你不給。久而久之，狗學會了坐下、握手、接飛盤。

這種「做對給獎勵、做錯給懲罰」的學習方式，就叫做**強化學習**（Reinforcement Learning, RL）。而理察·薩頓正是定義這個領域的人。

強化學習與其他機器學習方法的關鍵差異是：**它不需要人類告訴它「正確答案」是什麼，只需要告訴它「做得好不好」**。這讓 AI 能夠學會那些連人類都難以說清楚規則的任務。

薩頓從 1980 年代開始，與他的導師 Andrew Barto 一起建立了強化學習的數學基礎和核心演算法。他們的貢獻包括：

1. **時序差分學習（TD Learning）**：讓 AI 能夠從「連續的經驗」中學習，不需要等到任務結束才知道好壞

2. **Actor-Critic 架構**：結合「行動者」（選擇動作）和「評論者」（評估好壞）兩個系統，是現代強化學習的標準架構

3. **策略梯度方法**：直接優化 AI 的行為策略，是 ChatGPT 使用的 RLHF（人類回饋強化學習）的理論基礎

這些聽起來很抽象，但它們的應用已經深入我們的生活：

- **AlphaGo**（打敗世界圍棋冠軍）：薩頓的學生 David Silver 領導開發，核心就是強化學習
- **ChatGPT**：使用 RLHF 來讓模型的回答更符合人類偏好，這是策略梯度方法的應用
- **推薦系統**：YouTube、抖音用 RL 來決定下一個推薦什麼影片給你
- **自駕車**：用 RL 來學習在複雜路況中做出正確決策
- **機器人控制**：讓機器人學會走路、抓取物品

2024 年，薩頓與 Andrew Barto 共同獲得圖靈獎，表彰他們「發展了強化學習的概念性和演算法基礎」。這是對他們四十多年研究工作的最高肯定。

---

## 關鍵貢獻

### 時序差分學習 TD Learning（1988）

1988 年，薩頓發表了〈Learning to Predict by the Methods of Temporal Differences〉，正式提出時序差分學習。

**為什麼重要**：

在時序差分之前，要讓 AI 學習一個任務的價值，必須等到整個任務完成後才能更新。比如下棋，必須等到整盤棋結束，才知道中間每一步是好是壞。

TD Learning 的突破是：**不用等到結束，每走一步就可以更新**。AI 會用「下一步的預測值」來更新「這一步的預測值」——這叫做「自舉」（bootstrapping）。

這大幅加速了學習效率，讓強化學習能夠處理更複雜、更長的任務。

### Actor-Critic 架構

薩頓在博士論文中提出了 Actor-Critic 架構，將強化學習系統分成兩個部分：

- **Actor（行動者）**：負責選擇動作
- **Critic（評論者）**：負責評估動作的好壞

這種架構結合了「基於價值」和「基於策略」兩種方法的優點，成為現代強化學習的標準設計模式。

今天的 PPO（Proximal Policy Optimization）、A3C 等主流演算法，都是 Actor-Critic 架構的變體。

### Dyna 架構

Dyna 是薩頓提出的一種整合「學習」和「規劃」的架構。

**核心想法**：AI 不僅從真實經驗中學習，還會建立一個「世界模型」，在「想像中」模擬經驗來學習。

這讓 AI 能夠更有效率地利用有限的真實經驗，在「腦中」進行更多練習。這個概念對後來的模型基礎強化學習（Model-Based RL）影響深遠。

### 《強化學習導論》教科書（1998/2018）

薩頓與 Barto 合著的《Reinforcement Learning: An Introduction》是強化學習領域的「聖經」。

這本書被引用超過 75,000 次，培養了整整一代的 RL 研究者。幾乎每個學習強化學習的人都讀過這本書——包括開發 AlphaGo 的 David Silver 團隊。

第一版於 1998 年出版，第二版於 2018 年更新，至今仍是該領域最權威的入門教材。

### 「苦澀的教訓」（2019）

2019 年 3 月，薩頓在個人網站發表了一篇短文〈The Bitter Lesson〉，只有一千多字，卻在 AI 社群引發巨大迴響。

**核心論點**：

> 「70 年 AI 研究最大的教訓是：利用算力的通用方法最終是最有效的，而且優勢巨大。」

薩頓觀察到，AI 研究的歷史不斷重複一個模式：

1. 研究者試圖把「人類知識」建進 AI 系統
2. 短期內確實有效，研究者也感到滿意
3. 長期來看卻停滯不前，甚至阻礙進步
4. 最終，基於「搜尋」和「學習」的通用方法配合大量算力，取得突破性成功

這個「教訓」是「苦澀的」，因為它意味著人類精心設計的巧思，最終比不過簡單方法加大量算力。

這篇文章被廣泛視為預言了後來大型語言模型的成功——GPT 系列正是「簡單方法 + 大量算力」的代表。

---

## 人生軌跡

### 早年與求學

1956 年出生的理察·薩頓，在史丹佛大學攻讀心理學學士（1978 年）。他對「學習」這個問題的興趣，最初來自心理學而非電腦科學。

之後他進入麻州大學阿默斯特分校，在 Andrew Barto 的指導下攻讀碩士（1980）和博士（1984）。他的博士論文題目是〈Temporal Credit Assignment in Reinforcement Learning〉——這奠定了他後來四十年研究的基礎。

### 產業界經歷（1985-2002）

博士畢業後，薩頓在產業界工作了近二十年：

- **GTE Laboratories**（1985-1994）：作為首席技術人員，繼續強化學習研究
- **AT&T Labs**（1998-2002）：參與將 RL 應用於實際問題

這段產業經歷讓他深刻理解 AI 技術的實際應用挑戰。

### 阿爾伯塔大學（2003-至今）

2003 年，薩頓加入加拿大阿爾伯塔大學電腦科學系，並創立了**強化學習與人工智慧實驗室（RLAI Lab）**。

RLAI Lab 成為全球強化學習研究的重鎮，培養了超過 100 位研究者。實驗室目前有 10 位主要研究員，持續推動 RL 研究的前沿。

### DeepMind 與 AlphaGo（2017-2023）

2017 年，薩頓以傑出研究科學家身份加入 Google DeepMind，並協助在艾德蒙頓創立 DeepMind Alberta 研究中心。

這段期間，他的學生 David Silver 領導的團隊開發出 AlphaGo、AlphaZero 等里程碑式的 AI 系統，將強化學習推向公眾視野。

2023 年，薩頓轉到 Keen Technologies 擔任研究科學家，同時保持阿爾伯塔大學和 Amii（Alberta Machine Intelligence Institute）的職位。

### 圖靈獎（2024）

2024 年，ACM 宣布將圖靈獎頒發給薩頓和他的導師 Andrew Barto，表彰他們「發展了強化學習的概念性和演算法基礎」。

這是對他們四十多年師生合作的最高肯定。頒獎詞特別提到，他們的工作直接促成了 ChatGPT 等現代 AI 系統的成功——這些系統使用 RLHF 技術來對齊人類偏好。

---

## 師承與門生

薩頓的學術傳承極為輝煌。

**師承**：他的博士導師 Andrew Barto 是強化學習的共同奠基者。師徒二人合作了四十多年，共同撰寫了領域經典教科書。

**門生**：薩頓在阿爾伯塔大學培養了眾多頂尖人才：

- **David Silver**：DeepMind 首席研究科學家，領導開發 AlphaGo、AlphaZero，被稱為「AlphaGo 之父」

- **Doina Precup**：DeepMind 蒙特婁負責人，McGill 大學教授

- **Csaba Szepesvári**：DeepMind 研究科學家，《Algorithms for Reinforcement Learning》作者

這些學生將強化學習從學術研究帶入產業應用，創造了打敗世界圍棋冠軍等歷史性成就。

---

## 獎項與榮譽

### 2024 年 ACM 圖靈獎

薩頓與 Andrew Barto 共同獲得 2024 年圖靈獎，頒獎詞為：「表彰他們發展了強化學習的概念性和演算法基礎。」

ACM 在公告中指出：「Barto 和 Sutton 在 1980 年代開始的一系列論文中，引入了強化學習的主要思想，建構了數學基礎，並發展了重要的演算法。」

### 其他重要榮譽

- 2021 年：英國皇家學會院士（FRS）
- 2016 年：加拿大皇家學會院士
- 2001 年：AAAI Fellow（美國人工智慧學會）

---

## 觀點與立場

### 「苦澀的教訓」

薩頓最著名的觀點來自他的〈The Bitter Lesson〉論文。他認為：

1. **通用方法終將獲勝**：搜尋和學習這兩種通用方法，配合大量算力，最終會打敗人類設計的特定領域方法

2. **不要把人類知識硬編碼進 AI**：「我們必須學到這個苦澀的教訓：把我們認為的思考方式建進 AI，長期來看行不通。」

3. **讓 AI 自己發現**：「我們需要能像我們一樣發現的 AI 代理，而不是包含我們已發現事物的代理。」

這個觀點在 GPT 系列模型成功後得到廣泛認可——這些模型正是「簡單架構 + 大量資料 + 大量算力」的產物。

### 對 LLM 的看法

在接受 Dwarkesh Patel 採訪時，薩頓表示他認為當前的 LLM 路線可能是「死路」——這與 Yann LeCun 的觀點類似。他認為真正的智慧需要與環境互動、學習因果關係，而不僅僅是處理文字。

### 重要發言摘錄

> 「70 年 AI 研究最大的教訓是：利用算力的通用方法最終是最有效的，而且優勢巨大。」
>
> ——〈The Bitter Lesson〉，2019

> 「我們必須學到這個苦澀的教訓：把我們認為的思考方式建進 AI，長期來看行不通。」
>
> ——〈The Bitter Lesson〉，2019

> 「我們需要能像我們一樣發現的 AI 代理，而不是包含我們已發現事物的代理。」
>
> ——〈The Bitter Lesson〉，2019

---

## 延伸閱讀

### 必讀文章

- [The Bitter Lesson (2019)](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)：薩頓最具影響力的論文，只有一千多字，卻影響了整個 AI 社群的思維方式

### 重要著作

- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)：強化學習領域的聖經級教科書，可免費線上閱讀

### 相關報導

- [Dwarkesh Podcast: Richard Sutton](https://www.dwarkesh.com/p/richard-sutton)：深度訪談，討論 RL 的未來和 LLM 的局限
- [University of Alberta: Computing science professor wins Turing Award](https://www.ualberta.ca/en/folio/2025/03/computing-science-professor-wins-turing-award.html)

---

## 照片來源建議

以下是可供挑選的 Richard Sutton 照片來源：

1. **Wikimedia Commons**：有多張 CC 授權照片
   - [Wikimedia Commons: Richard S. Sutton](https://commons.wikimedia.org/wiki/Category:Richard_S._Sutton)

2. **Amii 官網**：有官方肖像照
   - [Amii: Richard S. Sutton](https://www.amii.ca/people/richard-s-sutton)

3. **阿爾伯塔大學**：有學術場合的照片

建議選用 Amii 官網或 2024 年圖靈獎相關的照片。

---

## 相關人物

- [安德魯·巴托 Andrew Barto](/people/andrew-barto/)：博士導師、圖靈獎共同得主、終身合作者
