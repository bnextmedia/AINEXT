<!DOCTYPE html>
<html lang="en" >
  <head>
  <title>Jeff dean 的 35 年 ai 旅程——從大學論文到 gemini | AINEXT</title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


<meta name="keywords" content="AINEXT">
<meta property="og:locale" content='en_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="Jeff Dean 的 35 年 AI 旅程——從大學論文到 Gemini">
<meta property="og:description" content="
本文整理自 Stanford AI Club 邀請 Jeff Dean 的演講。


  
    ">
<meta property="og:url" content="https://bnextmedia.github.io/AINEXT/posts/20251223-jeff-dean-35-years-ai-journey/">
<meta property="og:image" content="https://bnextmedia.github.io/AINEXT/images/images/logo.png">
<link rel="canonical" href="https://bnextmedia.github.io/AINEXT/posts/20251223-jeff-dean-35-years-ai-journey/">

<link rel="apple-touch-icon" sizes="180x180" href='https://bnextmedia.github.io/AINEXT/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='https://bnextmedia.github.io/AINEXT/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='https://bnextmedia.github.io/AINEXT/favicon-16x16.png'>
<link rel="manifest" href='https://bnextmedia.github.io/AINEXT/site.webmanifest'>

<link rel="stylesheet" href="https://bnextmedia.github.io/AINEXT/css/styles.648e60c6d86809f863ae1346848574b9c685732794e7851c7d3e557de9ddd293bc1c209f963d5041785c2fd4268470bdfde453a99f550b655d1b4f825eed4682.css" integrity="sha512-ZI5gxthoCfhjrhNGhIV0ucaFcyeU54UcfT5Vfend0pO8HCCflj1QQXhcL9QmhHC9/eRTqZ9VC2VdG0&#43;CXu1Ggg==">
</head>

  <body>
    <div class="nav-drop">
  <div class="nav-body">
      <a href="https://bnextmedia.github.io/AINEXT/" class="nav_item">首頁</a>
      <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav_item">文章列表</a>
    <div class="nav-close"></div><div class="color_mode">
  <label for="mode">Toggle Dark Mode</label>
  <input type="checkbox" class="color_choice" id="mode">
</div>

  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href=https://bnextmedia.github.io/AINEXT/ class="nav-brand nav_item">
        <img src="https://bnextmedia.github.io/AINEXT/images/logo.png" alt="AINEXT " class="logo-light" width="130px" />
        <img src="https://bnextmedia.github.io/AINEXT/images/logo-dark.png" alt="AINEXT " class="logo-dark" width="130px" /></a>
    
    
    <div class="nav-links">
        <a href="https://bnextmedia.github.io/AINEXT/" class="nav-link">首頁</a>
        <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav-link">文章列表</a>
    </div>
    
    <div class="nav_bar-wrap">
      <div class="nav_bar"></div>
    </div>
  </nav>
</header>

<style>
 
.nav-links {
  display: flex;
  flex-direction: row;
  gap: 1.5rem;
  align-items: center;
  white-space: nowrap;
}

.nav-link {
  color: var(--text);
  text-decoration: none;
  font-size: 0.95rem;
  padding: 0.5rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover {
  color: var(--theme);
}

 
@media (min-width: 768px) {
  .nav_bar-wrap {
    display: none;
  }
}

 
@media (max-width: 767px) {
  .nav-links {
    display: none;
  }
  
  .nav_bar-wrap {
    display: grid;
  }
}

 
.logo-dark {
  display: none;
}

html[data-mode="dark"] .logo-light {
  display: none;
}

html[data-mode="dark"] .logo-dark {
  display: block;
}

 
@media (prefers-color-scheme: dark) {
  html:not([data-mode="light"]) .logo-light {
    display: none;
  }
  
  html:not([data-mode="light"]) .logo-dark {
    display: block;
  }
}

 
.nav {
  position: relative !important;
  background: var(--bg);
  padding: 0.5rem 0;
  border-bottom: 1px solid var(--border);
}

.mt {
  margin-top: 2rem !important;
}

 
.post {
  padding-top: 1rem;
}

.post_date {
  margin-top: 0;
}

 
.archive {
  padding-top: 1rem;
}

.archive_title {
  margin-top: 0;
}
</style>


    <main>
      
  <div class="wrap mt post">
    <div><p class=post_date>23. December 2025</p>
      <h1 class="post_title">Jeff Dean 的 35 年 AI 旅程——從大學論文到 Gemini</h1>
      <div class="post_body">
        <div class="post_inner">
        
        
          <blockquote>
<p>本文整理自 Stanford AI Club 邀請 Jeff Dean 的演講。</p>
</blockquote>
<div class="media-embed">
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/AnTw_t21ayE" 
      allowfullscreen 
      title="YouTube Video"
      loading="lazy"
    ></iframe>
  </div>
</div>

<style>
.media-embed {
  max-width: 100%;
  margin: 1rem 0;
}

.video-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
}

.video-container iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border: 0;
  border-radius: 12px;
}
</style>

<hr>
<h2 id="1990-年一個大學生以為-32-核就能改變世界">1990 年，一個大學生以為 32 核就能改變世界</h2>
<p>1990 年，Jeff Dean 在大學畢業前寫了一篇關於神經網路的論文。當時他剛接觸到這個領域，立刻被迷住了。「這是一個很棒的抽象概念，」他回憶，「我們可以用它來建構模式辨識系統，解決各種問題。」於是他決定做一個野心勃勃的畢業專題：用系上那台 32 核處理器的電腦來並行訓練神經網路。</p>
<p>他實作了兩種現在我們稱之為「資料平行」(data parallelism) 和「模型平行」(model parallelism) 的訓練方式，研究當處理器數量增加時，訓練速度如何提升。結果呢？「我完全錯了，」Jeff Dean 笑著說，「要訓練出真正好用的神經網路，需要的不是 32 倍的運算力，而是一百萬倍。」</p>
<p>這個「錯誤」說明了一件事：神經網路的潛力比當時任何人想像的都大，但實現這個潛力需要的運算規模，也遠超過 1990 年代的技術能提供的。Jeff Dean 畢業後去做了其他事，但他一直惦記著這個想法。</p>
<hr>
<h2 id="2012-年google-廚房裡的一場對話">2012 年，Google 廚房裡的一場對話</h2>
<p>二十多年後的某一天，Jeff Dean 在 Google 的員工休息區 (micro-kitchen) 碰到了 Andrew Ng。「嘿 Andrew，你怎麼在這？」Andrew 解釋說他剛開始每週來 Google 一天，還在摸索要做什麼。然後他提到：「我在史丹佛的學生開始用神經網路在語音辨識上取得不錯的成果。」</p>
<p>Jeff Dean 的眼睛亮了起來。「我們應該訓練超大型的神經網路。」</p>
<p>這場廚房對話成為 Google Brain 的起點。當時 Google 的資料中心裡沒有 GPU，只有大量的多核心 CPU。於是他們建構了一個叫做 DisBelief 的軟體系統，讓神經網路的訓練可以分散到數百台機器上執行。</p>
<p>這個系統的運作方式，用 Jeff Dean 的話說，「在數學上完全是錯的」。他們讓兩百個模型副本同時訓練，每個副本會下載當前的參數、用一批資料計算梯度、然後把更新送回參數伺服器。問題是，所有副本都在非同步地做這件事，當你的更新送回去時，參數可能已經被其他一百九十九個副本改過了。</p>
<p>「這讓很多人很緊張，因為這不是你『應該』做的事，」Jeff Dean 回憶，「但它居然有效，所以我們就繼續用了。」</p>
<p>這個「數學上錯誤但實際上有效」的系統，讓 Google 在 2012 年訓練出比當時任何人都大 50 到 100 倍的神經網路。他們拿一千萬張 YouTube 影片的隨機截圖來訓練，完全不給任何標籤，只是讓模型學習如何重建原始圖像。訓練完成後，他們發現模型頂層的某些神經元會對特定概念產生強烈反應——其中一個神經元的最強刺激是貓臉，即使它從來沒被「教過」什麼是貓。這就是著名的「貓論文」(cat paper) 的由來。</p>
<hr>
<h2 id="當語音辨識變好世界需要新的晶片">當語音辨識變好，世界需要新的晶片</h2>
<p>Google Brain 團隊很快就把神經網路應用到語音辨識上，訓練出一個錯誤率遠低於當時 Google 產品的模型。Jeff Dean 做了一個簡單的估算：如果語音辨識變得更好，更多人會想用。假設一億人每天對著手機講三分鐘，我們需要多少運算資源？</p>
<p>答案讓他嚇了一跳：如果用現有的 CPU 來跑這個新模型，Google 需要把資料中心的電腦數量翻倍。只是為了一個功能。</p>
<p>「我們需要專用硬體，」Jeff Dean 意識到。神經網路有一些很好的特性可以被利用：它們對低精度運算很容忍，不需要 32 位元浮點數；而且當時所有的神經網路本質上都是密集線性代數運算——矩陣乘法、向量內積。如果能設計一個晶片專門做低精度的線性代數運算，效率會比通用 CPU 高很多。</p>
<p>於是 Google 開始設計 TPU（Tensor Processing Unit）。第一代 TPU 在 2015 年部署到資料中心，專門用於推論（inference）。當他們把它跟同期的 CPU 和 GPU 比較時，發現它快了 15 到 30 倍，能源效率則提升了 30 到 80 倍。這篇論文後來成為電腦架構頂級會議 ISCA 五十年歷史上被引用最多的論文。</p>
<p>但推論只是一半的問題。訓練需要更大規模的運算，於是 Google 開始設計「機器學習超級電腦」——把數千顆 TPU 用高速網路連接起來，形成一個龐大的運算叢集。從 2017 年的 TPU v2（256 顆晶片組成一個 Pod）到最新的 Ironwood（9,216 顆晶片組成一個 Pod），每個 Pod 的峰值運算能力提升了 3,600 倍，能源效率也提升了 30 倍。</p>
<hr>
<h2 id="transformerattention-就是你所需要的一切">Transformer：「Attention 就是你所需要的一切」</h2>
<p>2017 年，Jeff Dean 的幾位同事發表了一篇論文，標題是《Attention Is All You Need》。這篇論文提出了 Transformer 架構，徹底改變了自然語言處理的面貌。</p>
<p>在 Transformer 之前，處理序列資料的主流方法是 LSTM（長短期記憶網路）。LSTM 的運作方式是一個字一個字地處理，每處理一個字就更新一個內部狀態向量。問題在於，所有的歷史資訊都必須壓縮進這個固定大小的向量裡。當序列很長的時候，早期的資訊往往會被「遺忘」。</p>
<p>Transformer 的核心觀察是：與其強迫模型把所有資訊塞進一個向量，不如讓模型保留所有過去的狀態，然後在需要的時候「注意」(attend to) 相關的部分。這個「注意力機制」讓模型可以直接存取任何位置的資訊，而不用擔心中間的資訊被覆蓋掉。</p>
<p>論文的數據很驚人：Transformer 可以用比 LSTM 少 10 倍的參數、少 10 到 100 倍的運算量，達到相同的語言模型品質。換句話說，同樣的預算可以訓練一個大 10 倍的模型，或者用十分之一的成本達到同樣的效果。</p>
<p>Transformer 很快就被應用到各個領域。2020 年，另一組 Google 研究員把它應用到電腦視覺，證明 Vision Transformer (ViT) 可以用 4 到 20 倍更少的運算量，達到當時最好的圖像辨識準確度。</p>
<hr>
<h2 id="稀疏模型不是每個神經元都需要參與">稀疏模型：不是每個神經元都需要參與</h2>
<p>傳統神經網路有一個「浪費」的特性：不管輸入是什麼，整個模型的每一個參數都會參與運算。Jeff Dean 覺得這很不合理。「如果模型裡有專門處理不同事情的部分，為什麼每次都要全部啟動？」</p>
<p>這個觀察催生了稀疏模型（Sparse Models）的研究方向，其中最著名的是「專家混合模型」(Mixture of Experts, MoE)。在 MoE 架構中，模型包含很多「專家」子網路，每次推論時只有一小部分專家會被啟動——通常是 1% 到 5%。一個路由機制會根據輸入決定該啟動哪些專家。</p>
<p>這是一個很划算的交易：模型可以有巨大的參數量（表達能力更強），但每次推論的實際運算量只有傳統模型的幾分之一。Jeff Dean 團隊的實驗顯示，稀疏模型用大約八分之一的訓練成本就能達到相同的準確度。</p>
<p>「現在你聽到的大多數模型，包括 Gemini，都是稀疏模型，」Jeff Dean 指出。這是一個被低估的技術突破——它讓我們可以在不成比例增加成本的情況下，大幅擴展模型的規模。</p>
<hr>
<h2 id="讓模型展示解題過程">讓模型「展示解題過程」</h2>
<p>2022 年，Jeff Dean 的同事發現了一個簡單但威力驚人的技巧：如果你想讓模型解數學題，不要只給它「問題→答案」的範例，而是給它「問題→解題過程→答案」的範例。</p>
<p>這就是「思維鏈」(Chain-of-Thought) 提示法。當你在提示中示範如何一步步解題，模型就會學著在回答時也展示它的推理過程。這不只是讓答案更容易理解——模型產生每一個 token 時都會進行一輪運算，所以當它「展示解題過程」時，實際上是在用更多的運算來得出答案。</p>
<p>差別很明顯。在 GSM8K（一個小學程度的數學測試集）上，使用思維鏈提示讓準確率從接近隨機猜測跳到約 15%。</p>
<p>「現在回頭看，」Jeff Dean 說，「2022 年我們還在慶祝模型能答對 15% 的八年級數學題——那種『約翰有五個玩具，聖誕節又收到兩個』的題目。」</p>
<hr>
<h2 id="從小學數學到-imo-金牌">從小學數學到 IMO 金牌</h2>
<p>三年後，一切都不一樣了。</p>
<p>Google 用 Gemini 2.5 Pro 的一個變體參加了今年的國際數學奧林匹克 (IMO)。IMO 是全球最頂尖的中學生數學競賽，兩天六題，每題都是專業數學家等級的難度。</p>
<p>結果：六題答對五題，獲得金牌。</p>
<p>Jeff Dean 在演講中展示了其中一題的解答。題目本身需要對數論有深入理解，而模型產生的證明不只是正確的，評審還稱讚它的優雅。這已經不是「能算數學」的問題了——這是能夠進行嚴謹數學推理的能力。</p>
<p>這種進步來自多個技術的累積。思維鏈讓模型學會拆解問題；蒸餾 (Distillation) 讓小模型能夠學習大模型的「軟標籤」（一個分布，而不只是一個正確答案），大幅提升學習效率；而「在可驗證領域做強化學習」則讓模型可以不斷嘗試、從正確與錯誤的回饋中改進。數學和程式設計特別適合這種方法，因為你可以用定理證明器或編譯器來自動驗證答案是否正確。</p>
<hr>
<h2 id="三十五年從完全錯誤到居然有效">三十五年，從「完全錯誤」到「居然有效」</h2>
<p>從 1990 年的 32 核電腦到 2025 年的萬顆 TPU 叢集；從「貓論文」到 IMO 金牌；從 DisBelief 的「數學上錯誤」到 Transformer 的「注意力就夠了」。Jeff Dean 的職涯幾乎就是深度學習發展史的縮影。</p>
<p>他在演講結尾提到，AI 會深刻影響醫療、教育、科學研究和媒體創作——當然也有錯誤資訊等風險。「但如果做得好，我們的 AI 輔助未來會是光明的。」</p>
<p>或許最值得玩味的是那個 1990 年的「錯誤」。Jeff Dean 以為需要 32 倍運算力就能讓神經網路變得實用，結果需要的是一百萬倍。但這個「錯誤」的背後是一個正確的直覺：這個抽象概念值得追求。三十五年後，這個直覺被證明是對的——只是規模比任何人想像的都要大得多。</p>

        </div>
        <div class="post_extra mb-2">
          
<div class="copy" data-before="分享故事" data-after="已複製">
  <svg class="icon">
    <use xlink:href="#copy"></use>
  </svg>
</div>
        </div>
        <div>
        
        </div>
      </div>
    </div>
    <a href=https://bnextmedia.github.io/AINEXT/ class="post_nav"><span class="post_next">Latest Posts
      <svg class="icon icon_scale">
        <use xlink:href="#double-arrow"></use>
      </svg>
    </span></a>
  </div>

    </main>
    <footer class="footer wrap pale">
  <p>&copy;&nbsp;<span class="year"></span>&nbsp;AINEXT</p>
  <p class="attribution upcase">由 <a href = 'https://bnextmedia.github.io/AINEXT/' target = '_blank' title = '領英個人檔案' rel = 'nonopener'>AINEXT</a> 設計</p>
</footer>


<script src="https://bnextmedia.github.io/AINEXT/js/index.min.0c2fb80a1ade817d7387f0de8ee061e7de6878a65a420dc61a6e0d0b3bb765c4c0394caefa7c40b16d39c7d74283b3cab364aa109f58cad99d149ec813f930c8.js"></script>

    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
