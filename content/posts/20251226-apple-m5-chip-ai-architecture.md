---
title: "M5 晶片的 AI 野心：為什麼 Apple 要在每個 GPU 核心塞入神經加速器"
date: 2025-12-26T12:00:00+08:00
description: "Apple M5 晶片的 AI 效能是 M4 的 3.5 倍，關鍵在於一個架構創新：在每個 GPU 核心內建神經加速器。這篇文章解析這個設計的技術意義，以及它如何支撐 Apple 的裝置端 AI 戰略。"
tags: ["Apple", "M5", "Apple Silicon", "AI 晶片", "GPU 架構"]
categories: ["AI 技術前沿"]
source_url: ""
source_name: "Apple FY2025 Q4 財報"
image: "/images/posts/apple-m5-chip-keynote.jpg"
draft: false
---

「M5 在每個 GPU 核心都塞入了神經加速器，專門為 AI 工作負載進行超級加速。」

這是 Tim Cook 在財報電話會議上的原話。短短一句話，透露了 Apple 晶片架構的重大變革。

M5 的 AI 效能是 M4 的 3.5 倍。這個數字本身就很驚人——M4 已經是業界頂尖的筆電晶片，一年內效能提升 3.5 倍，在半導體產業並不常見。但更值得關注的是「怎麼做到的」：不是靠製程微縮，不是靠堆更多電晶體，而是靠架構創新。

這篇文章試圖解析 M5 的設計邏輯，以及它對 Apple 裝置端 AI 戰略的意義。

## 傳統架構：Neural Engine 是獨立單元

要理解 M5 的創新，先要理解傳統 Apple Silicon 的架構。

從 A11 Bionic 開始，Apple 在晶片中加入了「Neural Engine」，專門用來加速機器學習運算。Neural Engine 是一個獨立的處理單元，和 CPU、GPU 平行運作。當 App 需要執行 AI 任務（如人臉辨識、語音轉文字），系統會把任務分配給 Neural Engine 處理。

這種設計的優點是專業化。Neural Engine 針對矩陣運算、張量處理進行了深度優化，效率遠高於通用 CPU。缺點是資料搬移。當 AI 任務需要和圖形處理結合時（例如即時濾鏡、AR 特效），資料必須在 GPU 和 Neural Engine 之間來回傳遞，產生延遲和功耗。

這個問題在「裝置端 AI」時代變得更加明顯。Apple Intelligence 強調 AI 功能要即時、無縫、融入日常使用。如果每次調用 AI 都要把資料從 GPU 搬到 Neural Engine，再把結果搬回來，使用者體驗會受影響。

## M5 的解法：把神經加速器塞進 GPU

M5 的架構創新，就是打破 GPU 和 Neural Engine 的界線。

根據 Tim Cook 的描述，M5 在「每個 GPU 核心」都內建了神經加速器。這意味著 GPU 在處理圖形運算的同時，可以直接執行 AI 推理，不需要把資料送到外部的 Neural Engine。

這種設計有幾個優勢。

第一，降低延遲。資料不用在不同處理單元之間搬移，AI 推理可以和圖形渲染同步完成。對於需要即時 AI 處理的應用（如相機的場景辨識、遊戲的智慧 NPC），這是關鍵優化。

第二，提高效率。資料搬移是耗電大戶。把 AI 運算整合進 GPU 核心，可以減少記憶體頻寬占用，進而降低功耗。對於電池供電的筆電和手機，這很重要。

第三，簡化開發。開發者不用煩惱如何在 CPU、GPU、Neural Engine 之間分配任務，統一用 GPU 處理即可。Apple 的 Metal 框架應該會提供更簡潔的 API。

當然，這種設計也有代價。在 GPU 核心內建神經加速器，會增加晶片面積和設計複雜度。這可能是 M5 仍採用和 M4 相同製程（推測為台積電 3nm）卻能大幅提升 AI 效能的原因——不是靠更小的電晶體，而是靠更聰明的架構。

## 為什麼是現在？Apple Intelligence 的硬體需求

M5 的架構變革，不是為了規格競爭，而是為了支撐 Apple Intelligence。

Apple Intelligence 的核心理念是「裝置端優先」。絕大多數 AI 任務在本地裝置完成，只有超出裝置算力的任務才會送到 Private Cloud Compute 處理。這個設計保護隱私，但對裝置晶片的 AI 效能提出極高要求。

考慮幾個使用場景：

**場景一：相機即時處理。** 你打開 iPhone 17 的相機，AI 需要同時做：場景辨識（人像、風景、夜景）、物件偵測（人臉、寵物、食物）、曝光調整、色彩優化、雜訊消除。這些任務必須在你按下快門前完成，延遲要控制在毫秒級。

**場景二：即時翻譯。** AirPods Pro 3 支援 Live Translation，可以即時翻譯對話。語音訊號從麥克風進來，經過語音辨識、翻譯、語音合成，再從喇叭播出，整個流程要在一兩秒內完成才有實用價值。

**場景三：寫作輔助。** 你在 Mail App 寫信，Apple Intelligence 可以幫你改寫語氣、校正文法、生成摘要。這需要在本地運行語言模型，對算力要求不低。

這些場景的共同特點是：AI 運算要和其他任務（圖形處理、音訊處理、UI 渲染）同步進行，不能排隊等候。傳統的獨立 Neural Engine 架構會成為瓶頸，而 M5 的整合式設計正是為此而生。

## Private Cloud Compute：當裝置端不夠用時

即使 M5 再強大，有些任務仍然超出裝置算力範圍。例如生成長篇文章、處理複雜的多輪對話、或運行超大參數的 AI 模型。這時就需要 Private Cloud Compute（PCC）接手。

PCC 是 Apple 的雲端 AI 基礎設施，專為 Apple Intelligence 設計。它的運作方式是：當裝置判斷任務超出本地算力，會把請求加密後送到 Apple 的資料中心，由伺服器處理完畢後回傳結果，處理過程中的資料不會被儲存。

Tim Cook 在財報會議上提到，Apple 在休士頓建成了一座專門生產 PCC 伺服器的工廠，「幾週前剛開始出貨」。這些伺服器使用 Apple 自研晶片，而非 NVIDIA GPU，這是一個值得注意的訊號。

為什麼不用 NVIDIA？可能有幾個原因。

第一，成本。NVIDIA 的 AI GPU 價格高昂，且供不應求。Apple 自研晶片可以降低對外部供應商的依賴，長期成本更可控。

第二，優化。Apple 的 AI 模型是為 Apple Silicon 設計的，用自家晶片運行可以達到最佳效能和功耗比。

第三，隱私。Apple 強調 PCC 的資料處理不會留下痕跡。用自研晶片，Apple 可以從硬體層面確保這個承諾，避免第三方硬體帶來的潛在風險。

CFO Kevan Parekh 在財報會議上說，2025 財年的資本支出包含「建設 Private Cloud Compute 環境」的投資。這暗示 Apple 正在大規模擴建 AI 資料中心。

## 對開發者的意義：Core ML 的升級

M5 的架構變化，最終會透過軟體框架傳遞給開發者。

Apple 的機器學習框架是 Core ML，它讓開發者可以在 App 中整合 AI 模型，並自動利用裝置的 Neural Engine 加速。隨著 M5 的推出，Core ML 應該會進行相應升級，讓開發者可以利用 GPU 內建的神經加速器。

這對 App 開發有什麼影響？

首先，AI 功能會變得更流暢。過去，開發者如果想在遊戲中加入即時 AI（例如智慧 NPC、動態難度調整），可能擔心效能開銷。M5 的整合式架構降低了這個門檻。

其次，更多創新應用會出現。當 AI 推理可以和圖形渲染同步進行，一些過去不可能的應用變得可行。例如，即時生成的遊戲場景、基於 AI 的影片編輯、智慧 AR 體驗。

Tim Cook 在財報會議上提到：「我們看到開發者開始利用我們的裝置端基礎模型，創造全新的使用者體驗。」這表示 Apple 不只提供晶片和框架，還開放了預訓練的 AI 模型給開發者使用。

## 結語：晶片是 AI 戰略的基石

M5 的發布，讓 Apple 的 AI 戰略版圖更加完整。

裝置端有 A19 Pro 和 M5 晶片，提供強大的本地 AI 算力。雲端有 Private Cloud Compute，用自研晶片處理複雜任務。軟體層有 Apple Intelligence 和 Core ML，把這些能力包裝成對使用者和開發者友善的介面。

這套垂直整合的體系，是 Apple 區別於 Google、Microsoft、Meta 的核心優勢。那些公司依賴 NVIDIA GPU 和公有雲基礎設施，Apple 則從晶片設計到資料中心全部自己掌控。

當然，垂直整合也有風險。如果晶片設計出問題、或軟體開發進度落後，整個體系都會受影響。而且，Apple 的封閉生態系統能否在 AI 時代持續吸引開發者，還有待觀察。

Tim Cook 說得很篤定：「Apple Silicon 讓 Apple 產品成為體驗 AI 力量的最佳場所。」這句話是宣傳話術，還是真實實力的反映，未來幾年的市場會給出答案。
