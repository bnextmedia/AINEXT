---
title: "從看影片到走進影片：Google Genie 3 讓「世界模型」從論文走進瀏覽器"
date: 2026-01-31T10:00:00+08:00
description: "Google DeepMind 正式推出 Project Genie，讓使用者用一句話描述場景，就能生成一個可以走進去探索的互動世界。這項基於 Genie 3 的技術，是 AI 生成式內容從「可以看」跨入「可以玩」的關鍵一步，也讓世界模型首次成為消費級產品。"
tags: ["Google DeepMind", "Project Genie", "Genie 3", "World Model", "生成式 AI"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=Ow0W3WlJxRY"
source_name: "Google for Developers"
draft: false
---

> 本文整理自 Google for Developers 頻道 2026 年 1 月 29 日發布的 Release Notes 單集。

{{< youtube Ow0W3WlJxRY >}}

---

## 什麼是「世界模型」？跟影片生成有什麼不同？

過去兩年，大家已經對 AI 生成圖片、生成影片這件事不再陌生。你打一段文字，Midjourney 給你一張圖；你寫一句描述，Sora 或 Veo 幫你產出一段影片。但不管圖片多精緻、影片多流暢，你跟這些內容的關係始終是「觀看者」。你只能看，不能碰，不能走進去，不能決定鏡頭往哪裡移動。

世界模型（World Model）要做的事情，正好在這裡跨出了一步。Google DeepMind Genie 團隊的 Diego Rivas 在節目中解釋，影片生成模型是你給它一段文字，它吐出一段固定長度的影片；世界模型不一樣，它會根據你的描述開始一幀一幀地生成一個環境，而在每一幀，你都可以決定要往左走、往右走，或者對眼前的東西做點什麼。它不是在播放一段預先算好的影片，而是即時回應你的操作來決定下一幀該長什麼樣子。

這個概念其實不算全新。世界模型最早出現在強化學習的研究中，研究人員建構虛擬環境來訓練 AI 代理人（Agent）學習如何在世界中行動。但過去的世界模型服務的對象是 AI，不是人。現在 Google 把這項技術翻轉過來，讓人類成為走進這些世界的主角。你不再只是訓練 AI 的旁觀者，你自己就是那個在生成世界裡探索的人。

## Project Genie：世界模型的第一個消費級產品

Google 把 Genie 3 這個世界模型包裝成一個叫做 Project Genie 的網頁應用程式，由 Google Labs 和 Creative Lab 團隊聯手開發，目前開放給美國的 Google One Ultra 訂閱用戶使用。團隊在節目中拿了一個類比來解釋它的定位：如果 Flow 是讓一般人能用 Veo 影片生成模型的產品介面，那 Project Genie 就是讓一般人能用 Genie 3 世界模型的產品介面。

操作流程不複雜。使用者在介面上輸入一段文字描述，比如「一片珊瑚礁，有一隻鯊魚，背景是一艘沈船」，系統會先呼叫 Google 的圖片生成模型 Imagen（節目中稱為 Nano Banana Pro）產出一張 2D 的概念畫面。這一步的價值在於，它讓你在跳進 3D 世界之前，先確認場景大方向對不對。信任測試者的回饋顯示，這個「先看 2D 再跳 3D」的過渡步驟非常關鍵，因為從一張平面圖片變成你可以走進去的立體世界，那個瞬間的衝擊力是最強的。如果你連 2D 畫面都不滿意，就不用浪費算力去生成整個世界。

確認畫面之後，按下「Generate World」，後台會把你的描述擴展成更詳細的世界設定，然後 Genie 3 開始即時生成你可以操控角色在其中移動的環境。在節目中的即時示範裡，主持人 Logan Kilpatrick 看到珊瑚礁世界生成後，立刻嚷著要游到沈船裡面去探索。角色真的穿過了水域，游近了那艘船。整個過程沒有載入畫面、沒有等待時間，就是一幀接著一幀地即時生成。

## 把玩具恐龍放進虛擬世界

節目中最讓人印象深刻的示範，不是那個珊瑚礁，而是一隻玩具恐龍。

團隊帶了一隻實體的玩具恐龍「Bob」到錄影現場，用手機拍了一張照片，然後把照片上傳到 Project Genie。接著他們在提示詞裡寫了「這個房間和照片裡的玩具」，系統就以這張照片為基礎，生成了一個圖書館場景，而那隻恐龍成了你可以操控的角色。

這個示範的厲害之處在於它展現了一種全新的「照片體驗方式」。你拍一張照片，不是拿來看的，是拿來走進去的。節目中 Bob 在圖書館裡跳上了桌子、爬上了椅子，甚至穿過窗戶跑到了外面的大馬路上。主持人笑說外面居然是一條高速公路，現實中根本不是這樣，但模型根據場景合理推測了窗外可能的景象。

更耐人尋味的是風格轉換的能力。因為輸入的照片是實景拍攝，生成的世界也自然呈現寫實風格。但 Diego Rivas 提到，你完全可以在提示詞裡指定「漫畫風」或「漫畫世界」，模型會根據指定的風格來渲染整個環境。這代表同一張照片，可以變成截然不同視覺風格的互動世界。

## 模型隱式學會了物理法則

Project Genie 生成的世界不只是好看而已，它會回應你的動作。

在節目的示範中，角色在水中移動時，水面會產生飛濺的效果；角色撞到一顆球，球會滾動。這些看起來理所當然的物理反應，其實是模型自己「學」出來的。Diego Rivas 解釋，Genie 3 並沒有被明確寫入物理引擎的程式碼，而是在訓練過程中，模型隱式地學會了「這個世界是怎麼運作的」。當你撞到東西，東西會動；當你在水裡移動，水會有反應。模型嘗試在每一幀都生成一個「盡可能寫實的世界演進」。

這件事情為什麼困難？因為世界模型面對的問題比影片生成難上許多。影片生成模型在產出一段影片時，可以回頭修改前面和後面的幀，讓整體看起來一致。你可以想像它像是一個畫家在畫布上先打草稿，然後左修一點、右調一點，最後讓整幅畫看起來協調。但世界模型沒有這種自由。它必須在不知道使用者下一步會做什麼的情況下，即時生成下一幀，而且這一幀必須跟過去所有幀保持一致，同時還要正確回應使用者的即時操作。這是一個同時受到「過去狀態」和「當下輸入」雙重限制的生成問題，難度比單向的影片生成高出不少。

團隊中的研究人員也提到，這項技術的物理模擬能力不只對娛樂有價值。一個能夠學會「世界如何運作」的模型，在機器人學、自動駕駛模擬、建築設計等領域都可能有實際應用。這不只是一個讓你玩的玩具，它可能是未來 AI 理解物理世界的一塊基石。

## 為什麼只有 60 秒？算力成本才是真正的關卡

目前 Project Genie 每次探索的時間上限是 60 秒。這個限制不是因為技術做不到更久。Diego Rivas 在節目中坦言，模型其實可以跑更長的時間，團隊內部也有跑過好幾分鐘的示範。但他們最終選擇了 60 秒作為初始上限，原因有兩個層面。

第一個是體驗品質的考量。Genie 3 生成的世界會隨著時間推移，逐漸失去動態感。一開始水會流動、物件會反應，但跑久了之後，世界會變得越來越「靜態」，動態元素慢慢消失。這是目前模型的一個已知限制，團隊正在持續改善。與其讓使用者在後半段看到一個逐漸失去生命力的世界，不如讓他們在最精彩的 60 秒結束後，去探索另一個全新的世界。研究團隊的判斷是，兩個各 60 秒的精彩世界，比一個 120 秒但後半段了無生氣的世界，能帶給使用者更好的體驗。

第二個是更現實的成本問題。每一個運行中的世界，都需要專屬的 GPU 算力在背後即時運算。這不像文字生成那樣可以批次處理，而是每個使用者在探索的每一秒，都在持續消耗運算資源。如果把時間放長到五分鐘甚至更久，同樣的基礎設施能服務的使用者人數就會大幅減少。Diego Rivas 提到，目前模型生成世界的速度已經追上了使用者消費內容的速度，速度本身不是瓶頸，真正的瓶頸是讓更多人能同時使用的服務成本。

不過這個限制不是永久的。團隊表示會根據使用者回饋調整，而且不同類型的世界可能需要不同的時間。如果是一個滑雪下坡的場景，兩分鐘可能很精彩，因為場景持續在變化；但如果是在一間圖書館裡閒逛，一分鐘可能就夠了。

## 從 Genie 2 到 Genie 3：一年之內的跳躍

把時間尺度拉長一點看，Project Genie 今天能做到的事情，放在一年多前根本是奢望。

Genie 2 在 2024 年底亮相時，能做到的是 10 秒鐘的低解析度體驗，而且不是即時的。它只能處理特定類型的視覺環境，離寫實感還有很大的距離。到了 2025 年 8 月，Google 發布了 Genie 3 的展示影片，當時一分鐘的即時寫實互動還被團隊視為一個「挑戰性目標」（stretch goal），沒人確定做不做得到。

結果不只做到了，而且做到之後使用者的反應是「一分鐘不夠長」。研究團隊的人在節目裡苦笑著說，這其實是進步最好的證明。當你的技術從「這不可能」變成「這不夠」的時候，代表你已經跨過了某個門檻。

從 Genie 3 發表到 Project Genie 正式上線這段期間，團隊做的主要工作不是改進模型能力，而是在搞基礎設施和服務架構。他們需要把一個實驗室裡的研究原型，變成一個能讓大量使用者同時使用的產品。這牽涉到算力分配、成本最佳化、服務穩定性等工程問題。同時他們也跑了一輪信任測試者計畫，從創意工作者到教育工作者等不同背景的使用者身上，收集真實的使用回饋。

這段「從 demo 到產品」的旅程本身就是一個值得注意的訊號。很多 AI 研究成果停留在論文和展示影片的階段，能不能走到讓一般人實際使用的產品，是完全不同層次的挑戰。Google 選擇先用 Ultra 訂閱制度來控制使用者規模，用可控的用量來累積營運經驗，這是一個務實的策略。

## 畫廊、Remix 與使用者的想像力瓶頸

面對一個全新類型的生成工具，使用者碰到的第一個問題往往不是技術限制，而是「我不知道要做什麼」。

主持人 Logan Kilpatrick 自己就承認了這個尷尬。他說第一次打開 Project Genie 時，面對空白的提示輸入框完全不知道要打什麼。這個工具明明很強大，但你就是不知道從哪裡開始。這跟當初 ChatGPT 剛推出時很多人盯著對話框發呆是同一種感覺。

Creative Lab 團隊為此設計了一個「畫廊」功能，在 Project Genie 裡面放了一系列預先設計好的世界，讓使用者可以直接跳進去體驗，也可以拿來當作靈感的起點。如果你看到一個世界覺得不錯，但想改一些東西，可以點「Remix」按鈕，對現有的世界做修改。比如把場景裡的藍色球改成紅色球，系統會在原本的提示詞上加入你的修改，再透過 Imagen 產出新的概念圖，然後生成改版的世界。

研究團隊對此抱著一個開放的期待。他們回想 Veo 3 推出時，社群使用者創造出了許多團隊自己從未想過的用法。他們希望 Project Genie 也會出現類似的情況，讓使用者的創意去定義這項技術的可能性。

比較值得關注的是使用者回饋中浮現的一個模式：人們進入世界之後，會自然而然地想要根據場景做「合理的事」。看到門，就想開門；看到東西，就想拿起來；看到角色，就想互動。這種「情境感知互動」的需求，指出了世界模型下一階段的發展方向。目前 Project Genie 主要支援的是導航和基本碰撞，離真正的情境互動還有一段距離，但這條路的方向已經很清楚了。

## 我的觀察：第四種生成，第一道成本牆

回頭看這兩年 AI 生成式技術的演進，有一條清楚的路徑：先是文字，然後圖片，然後影片。每一次跨越都讓 AI 的輸出更接近人類感知世界的方式。Project Genie 代表的是第四種形態，可互動的世界。它跟前三種的根本差異不在技術複雜度，而在於「主導權」的翻轉。文字、圖片、影片，都是 AI 生成、你來看；世界模型是 AI 生成、你來玩。使用者第一次從被動的觀賞者變成主動的參與者，這個轉變的意義可能比畫質從 480p 升到 4K 更加深遠。

但這篇節目訪談裡最讓我在意的，不是技術有多厲害，而是那個 60 秒的限制。表面上看，60 秒是一個體驗設計的選擇；往深處想，它暴露的是世界模型面對的核心瓶頸：算力成本。文字生成是一次性的運算，圖片生成也是，影片稍微貴一點但還是一次算完。世界模型完全不同，它是持續性的運算，使用者在裡面的每一秒都在燒 GPU。這意味著世界模型的商業模式跟其他生成式 AI 有本質性的差異。你不是付一次錢買一個輸出，你是在「租用」一個持續運轉的世界。這個經濟模型要怎麼跑得通，可能比技術本身更值得觀察。

最後，玩具恐龍 Bob 的示範看似只是節目效果，但它指向的想像空間不小。如果你可以拍一張照片然後走進去，那旅遊照片可以變成可重新探索的場景，建築設計圖可以變成客戶能走動的空間體驗，歷史照片可以變成學生身歷其境的教材。這不只是遊戲或娛樂的事，它可能是「介面」本身的演化。我們跟數位內容互動的方式，有可能從「看螢幕」變成「走進螢幕」。聽起來像科幻小說，但 Google 已經把第一個版本放進瀏覽器裡了。
