---
title: "AI 看診比醫生準，但別急著高興：史丹佛 2026 臨床 AI 報告的六大發現"
date: 2026-01-31T10:00:00+08:00
description: "史丹佛與哈佛研究團隊 ARISE 發布 2026 年臨床 AI 報告，涵蓋 128 頁、橫跨模型能力、評測標準、人機協作、病患端應用等六大面向。報告揭示一個矛盾現實：AI 在控制環境中的診斷能力已超越人類醫師，但在不確定性、過度自信、真實臨床部署等關鍵領域仍有嚴重缺口。"
tags: ["臨床 AI", "ARISE", "Stanford", "Harvard", "醫療 AI", "LLM"]
categories: ["AI 技術前沿"]
source_url: "https://arise-ai.org/report"
source_name: "ARISE Network"
image: "/images/posts/20260131-clinical-ai-report-overview.jpg"
draft: false
---

> 本文整理自 ARISE Network 於 2026 年 1 月發布的 *State of Clinical AI Report 2026*。

---

2026 年 1 月，一份 128 頁的報告在醫療 AI 圈引起廣泛討論。這份由史丹佛大學與哈佛醫學院研究者組成的 ARISE Network 發布的《State of Clinical AI Report 2026》，不只是又一份樂觀的技術展望，而是對臨床 AI 現狀最全面的一次體檢。報告的核心訊息可以濃縮成一句話：前沿 AI 系統已經足夠強大，現在的問題是如何安全且有效地把這些工具帶進真實的醫療現場。

報告團隊由史丹佛計算醫學主任 Jonathan H. Chen、哈佛醫學院助理教授 Adam Rodman、以及 ARISE 執行總監 Ethan Goh 領銜。他們檢視了 2025 年最重要的臨床 AI 研究，從模型效能、評測方法、基礎技術、臨床工作流程、病患端應用到實際部署案例，歸納出六個核心發現和十個對 2026 年的預測。

## 發現一：模型能力飛速進步，但真實臨床證據遠遠跟不上

報告開宗明義指出一個數字：目前已有超過 1,200 個 FDA 核准的 AI 醫療工具和 35 萬個消費級健康 App，構成一個 700 億美元的市場。但這些工具中，只有極少數經過同行審查的評估。在 691 個 FDA 核准的 AI 醫療器材中，超過 95% 走的是 510(k) 途徑，這個途徑的邏輯是「與現有器材等效」，而非證明新工具確實能改善病患結果。更驚人的是，約 50% 的 FDA 審查摘要連研究設計都沒交代，53% 沒有樣本量，不到 1% 報告了病患結局。

與此同時，前沿推理模型在控制環境下的表現已經逼近甚至超越人類醫師。OpenAI 的 o1-preview 在《新英格蘭醫學期刊》的經典臨床病理討論（CPC）案例中，診斷準確率達 78%，選擇正確的下一步檢查更高達 87%。在急診室真實案例中，模型在初步分診時的精確/接近精確診斷率為 66%，高於醫師的 48% 至 54%。另一個基於 o3 的系統 Dr. CaBot，在盲測中不僅讓醫師無法分辨它與人類專家，且推理品質評分還更高。

這些數字看起來令人振奮，但報告團隊的結論卻相當冷靜：模型在控制環境中展現了超人能力，但在自主部署之前，仍需要更強的後設認知、校準能力和壓力測試。能在考試中拿高分，與能在真實的、混亂的、資訊不完整的臨床現場做出安全決策，是截然不同的兩件事。

## 發現二：AI 有一個致命弱點，就是不知道自己不知道什麼

報告點出前沿模型一個反覆出現的模式：它們在確定性高的任務上表現優異，卻在面對不確定性時嚴重失靈。一項研究用「以上皆非」（None of the Above, NOTA）替換了標準醫學考試的正確答案，結果各模型的準確率暴跌 9% 到 38%。一個原本 81% 準確率的系統，在答案模式改變後直接跌到 43%。這代表模型的部分表現來自對答案分布的模式辨識，而非真正的臨床推理。

另一項使用 Script Concordance Testing（SCT）的研究則測試了模型「在新資訊出現時修正臨床判斷」的能力。在 750 道跨越兒科、神經科、急診等領域的題目中，表現最好的 o3 也只拿到 68%，大致相當於醫學生水準，低於住院醫師和主治醫師。更關鍵的是，模型傾向於使用極端評分，很少承認自己不確定，展現出與人類專家截然不同的過度自信模式。

MetaMedQA 基準測試進一步揭露了這個問題的嚴重程度：當正確答案被刻意移除時，沒有任何一個模型能辨識出題目是無法回答的。0%，一個模型都沒有。它們全部信心滿滿地選了一個錯誤答案。在醫療場景中，一個不知道自己不知道什麼的系統，可能比一個能力較弱但懂得說「我不確定」的系統更危險。

## 發現三：多選題基準已經飽和，我們需要全新的評測方式

報告指出，醫學 AI 評測正面臨一個結構性問題。一項針對 519 篇研究的系統性回顧發現，45% 的研究在評測「回答醫學執照考試題目」的能力，19% 在測診斷準確率，但行政任務（如開處方、編碼計費）各只佔 0.2%。95% 的研究用準確率作為主要指標，只有 16% 評估了偏見與公平性，5% 考慮了部署可行性，1% 測量了校準或不確定性。換句話說，我們大量測量的是模型最擅長的事，卻很少測量臨床現場最需要的能力。

2025 年出現了幾個試圖扭轉這個局面的新基準。OpenAI 的 HealthBench 用 5,000 段多輪健康對話和 262 位醫師建立的評分標準，測試模型在真實對話情境中的表現。史丹佛的 MedHELM 涵蓋 35 個基準、5 大類別、121 項任務，其中 12 個基準使用真實電子病歷資料。MedAgentBench 則在虛擬電子病歷環境中測試 AI 的代理能力，結果發現模型在查詢型任務表現不錯，但在需要「採取行動」的任務（如下醫囑）上表現大幅下降。以 Claude 3.5 Sonnet 為例，查詢類準確率 85%，行動類只有 54%。

NOHARM 基準更直接地測量了「AI 建議可能造成的傷害」。在 100 個真實的基層照護轉診案例中，即便是表現最好的模型，仍有 10% 到 20% 的案例可能產生嚴重傷害性建議，其中 77% 的嚴重傷害來自「遺漏」，也就是忘了建議關鍵的檢查或治療。

## 發現四：人機協作還沒找到最佳解

報告中最反直覺的發現之一，是人機協作的效果不如預期。一項涵蓋 52 項研究的統合分析顯示，人類加上 AI 的診斷可靠度平均高於人類單獨作業，但很少達到真正的互補效果，而且常常贏不了 AI 單獨作戰。在一項 92 名醫師參與的隨機對照試驗中，使用 GPT-4 的醫師在管理推理任務上的得分提升了 7%，但他們的整體表現（43%）與 GPT-4 獨立表現（44%）幾乎相同。

這引出了一個令人不安的問題：如果人機團隊打不贏 AI 單獨上場，那人類在這個迴路中的角色是什麼？報告認為答案不在於放棄人類監督，而在於重新設計協作方式。工作流程設計、互動介面、以及醫師如何被訓練使用 AI，可能比模型本身的能力更重要。

更值得警惕的是自動化偏見和技能退化。一項實驗讓 AI 受訓過的醫師接觸含有刻意錯誤的 LLM 建議，結果診斷準確率從 85% 暴跌至 73%。另一項多中心觀察研究發現，長期使用 AI 輔助大腸鏡檢查的內視鏡醫師，在沒有 AI 輔助時的腺瘤偵測率從 28.4% 下降到 22.4%，顯示持續依賴 AI 可能侵蝕醫師的獨立判斷能力。

## 發現五：病患端 AI 潛力巨大，但安全護欄嚴重不足

Google 的 AMIE 系統在 159 個模擬病患案例的雙盲研究中，幾乎在所有診斷準確率、溝通品質和同理心指標上都超越了 20 位醫師。AI 健康教練在糖尿病預防、心血管健康指導、睡眠與體適能建議等領域，都展現了與人類教練相當甚至更優的效果，而且參與度往往更高。GPT-4o 將兒科病患指引從英文翻譯成西班牙文的品質，與專業人工翻譯不相上下，且翻譯錯誤更少。

但報告同時發出了嚴厲警告。一項 300 人的實驗顯示，受試者無法區分 AI 和醫師的回答，而且即使是低準確度的 AI 回答，受試者也認為它與醫師回答一樣可信、一樣值得遵從。這意味著病患不能被指望扮演任何品質把關的角色。在一個商業利益驅動的環境中，如果廠商把「用戶參與度」當作成功指標而非客觀的健康結局，病患端 AI 可能造成的傷害將遠超預期。

## 發現六：真正在改變臨床結果的，是窄而深的專科 AI

報告的最後一個核心訊息，或許是最務實的。在所有華麗的通用型 AI 展示之外，真正在改善病患結果的，往往是針對特定任務、特定場景打造的專科 AI 系統。英國 NHS 在 26 家醫院部署中風 AI 影像辨識後，血栓切除術的執行率從 2.3% 翻倍到 4.6%，轉院時間從 192 分鐘縮短到 128 分鐘。德國一項涵蓋 46 萬名女性的乳癌篩檢研究中，AI 輔助讓乳癌偵測率提升 17.6%，同時沒有增加誤報率。AI 的安全網功能在研究期間被觸發了 3,969 次，其中 204 次成功抓到了原本會被漏掉的乳癌。

肯亞的 Penda Health 與 OpenAI 合作，在 15 個診所、近 4 萬次看診中前瞻性部署了 AI Consult 系統，結果診斷錯誤減少了 16%，治療錯誤減少了 13%。如果擴展到 Penda 每年 40 萬次看診，這意味著每年可以減少約 22,000 次診斷錯誤和 29,000 次治療錯誤。更有意思的是，使用 AI 的醫師隨時間推移學會了避免常見錯誤，紅燈觸發率從 45% 降到 35%，暗示 AI 不只是安全網，還是一種持續學習的工具。

## 十個大膽預測：2026 年會發生什麼

報告在最後列出了十個對 2026 年的預測，其中幾個特別值得注意。第一，醫療 AI 將迎來第一場醫療過失訴訟，AI 在錯誤中扮演重要角色。第二，AI 急診代理將進入主流，越來越多醫療體系開始提供。第三，醫療保險業者和醫療體系將陷入 AI 軍備競賽，雙方用 AI 機器人互相對抗處理事前授權、理賠否決和編碼優化，最終除了供應商之外無人受益。

第七個預測格外值得深思：超過 90% 的臨床筆記文字將由 AI 語音抄寫員產生，屆時沒有人能確定醫師在看診時究竟在想什麼。這個預測如果成真，將從根本上改變醫療紀錄的性質，從「醫師思考的紀錄」變成「AI 對醫師言語的詮釋」。

這份報告最有價值的地方，不在於它告訴我們 AI 有多厲害，而在於它誠實地呈現了能力與安全之間的落差。前沿模型在考試中拿到超人成績，卻不知道自己什麼時候在胡說八道。人機協作理論上應該 1+1>2，現實中卻常常 1+1<1.5。病患相信 AI 說的每一句話，即使那句話是錯的。窄而深的專科 AI 已經在拯救生命，通用型 AI 離安全部署還有一段路要走。

醫療 AI 已經不再是「能不能做到」的問題，而是「如何安全地做到」。這份報告提供的不只是一張成績單，更是一張待辦清單。
