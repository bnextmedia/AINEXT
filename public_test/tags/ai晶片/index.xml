<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI晶片 on AINEXT</title>
    <link>https://bnextmedia.github.io/AINEXT/tags/ai%E6%99%B6%E7%89%87/</link>
    <description>Recent content in AI晶片 on AINEXT</description>
    <generator>Hugo</generator>
    <language>zh-TW</language>
    <lastBuildDate>Tue, 06 Jan 2026 12:00:00 +0800</lastBuildDate>
    <atom:link href="https://bnextmedia.github.io/AINEXT/tags/ai%E6%99%B6%E7%89%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Groq 攜手 NVIDIA：Chamath 親解「Pre-fill 與 Decode」的架構之爭</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20260106-groq-nvidia-prefill-decode/</link>
      <pubDate>Tue, 06 Jan 2026 12:00:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20260106-groq-nvidia-prefill-decode/</guid>
      <description>&lt;p&gt;這可能是近期 AI 硬體圈最令人震驚的消息之一：NVIDIA 宣佈與 AI 晶片新創 Groq 達成戰略合作。&lt;/p&gt;&#xA;&lt;p&gt;這個消息之所以反直覺，是因為 Groq 長期以來都被視為 NVIDIA 的挑戰者。Groq 創辦人 Jonathan Ross 曾是 Google TPU 的發明者，這家公司主打的 LPU（Language Processing Unit）架構，正是為了打破 GPU 在大型語言模型（LLM）推論上的壟斷而生。然而，這場原本被視為「大衛對抗歌利亞」的戰爭，卻在 2025 年底演變成了一場價值 200 億美元的聯手。&lt;/p&gt;&#xA;&lt;p&gt;為什麼 NVIDIA 執行長黃仁勳（Jensen Huang）會願意「擁抱」競爭對手？All-In Podcast 主持人、同時也是 Groq 早期投資人的 Chamath Palihapitiya，在最新一集節目中揭露了這場合作背後的技術邏輯。這不僅是一次商業上的合縱連橫，更揭示了 LLM 運算架構正在經歷一場根本性的典範轉移——從單一架構通吃，走向「Pre-fill（預填充）」與「Decode（解碼）」的分工時代。&lt;/p&gt;&#xA;&lt;h2 id=&#34;llm-的兩張面孔閱讀與寫作&#34;&gt;LLM 的兩張面孔：閱讀與寫作&lt;/h2&gt;&#xA;&lt;p&gt;要理解這次合作的意義，首先得理解大型語言模型是如何思考的。Chamath 在節目中引用了一組關鍵概念：Pre-fill（預填充）與 Decode（解碼）。這兩個術語聽起來生硬，但它們精準地描述了 AI 處理任務的兩個截然不同的階段。&lt;/p&gt;&#xA;&lt;p&gt;所謂「Pre-fill」，就是模型的「閱讀階段」。當你把一長串 Prompt（提示詞）丟給 ChatGPT 時，模型必須一次性讀取所有的文字，計算字與字之間的關聯。這個過程是高度平行化的，需要巨大的算力來同時處理龐大的矩陣運算。這正是 NVIDIA GPU 的主場——GPU 的設計初衷就是為了處理這種大規模並行任務，它能像推土機一樣，暴力且高效地碾過這些數據。隨著 Context Window（上下文視窗）越來越大，Pre-fill 的運算需求也呈指數級上升，這讓 NVIDIA 的優勢更加不可撼動。&lt;/p&gt;&#xA;&lt;p&gt;然而，當模型讀完題目，開始「寫作」時，情況就變了。這就是「Decode」階段。在這個階段，模型必須一個字、一個字（token by token）地生成答案。每生成一個字，它都必須回頭看之前生成的所有內容，以確保上下文連貫。&lt;/p&gt;&#xA;&lt;p&gt;這時，運算的瓶頸不再是「算力」，而是「記憶體頻寬」。因為每生成一個字，資料就必須在晶片的運算單元（Logic）和記憶體（HBM）之間來回搬運一次。這就像是你每寫一個字，都得從書桌跑到圖書館查一次字典，然後再跑回來寫下一個字。無論你的寫字速度（算力）有多快，你的產出速度最終會被「跑圖書館」（記憶體傳輸）的時間給卡住。這就是為什麼我們有時會覺得 AI 回答時會「卡頓」或「像打字機一樣慢」的物理原因。&lt;/p&gt;&#xA;&lt;h2 id=&#34;蓋大樓的比喻為什麼-gpu-在-decode-階段效率低&#34;&gt;蓋大樓的比喻：為什麼 GPU 在 Decode 階段效率低？&lt;/h2&gt;&#xA;&lt;p&gt;Chamath 用了一個生動的建築比喻來解釋這個瓶頸。想像你在一棟摩天大樓裡，如果你要從 A 點移動到 B 點（完成一次運算），在 GPU 的架構下，你必須先搭電梯上到 10 樓，處理完後再搭電梯回到一樓，然後走到隔壁棟，再搭電梯上去。這個「搭電梯」的過程，就是資料在 HBM（高頻寬記憶體）與運算單元之間傳輸的過程。&lt;/p&gt;</description>
    </item>
    <item>
      <title>客製化晶片來了，Nvidia 的好日子要結束了嗎？</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20260106-custom-silicon-nvidia-groq/</link>
      <pubDate>Tue, 06 Jan 2026 10:30:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20260106-custom-silicon-nvidia-groq/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Deepwater Asset Management 旗下《DeepTech》Podcast 2025 年 9 月播出的單集。&#xA;🎬 YouTube：&lt;a href=&#34;https://www.youtube.com/watch?v=NsQozkZmrXA&#34;&gt;收看連結&lt;/a&gt;&#xA;🎧 Spotify：&lt;a href=&#34;https://open.spotify.com/episode/54P6sV9SekO2nPjtHiiJIN&#34;&gt;收聽連結&lt;/a&gt;&#xA;🎧 Apple Podcast：&lt;a href=&#34;https://podcasts.apple.com/us/podcast/deeptech-ep5-upon-further-review-were-still-early-in-ai/id1721973292?i=1000726489600&#34;&gt;收聽連結&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;OpenAI 和晶片設計公司 Broadcom 簽下了一份價值超過 100 億美元的合約，要開發專為 OpenAI 設計的客製化 AI 晶片。幾乎在同一時間，Nvidia 宣布以 200 億美元收購 AI 推論晶片新創公司 Groq 的技術和團隊。&lt;/p&gt;&#xA;&lt;p&gt;這兩則新聞放在一起看，透露出一個重要訊號：AI 晶片市場正在發生結構性的變化。&lt;/p&gt;&#xA;&lt;p&gt;投資管理公司 Deepwater Asset Management 的合夥人 Doug Clinton 在《DeepTech》Podcast 中提出了一個大膽的判斷：AI 產業的敘事正在從「GPU 需求」轉向「客製化晶片需求」。這個轉變對 Nvidia 來說，可能是好消息，也可能是壞消息——取決於你用什麼時間尺度來看。&lt;/p&gt;&#xA;&lt;h2 id=&#34;為什麼市值超過-1000-億美元的-ai-公司都想自己做晶片&#34;&gt;為什麼市值超過 1000 億美元的 AI 公司都想自己做晶片？&lt;/h2&gt;&#xA;&lt;p&gt;Doug Clinton 在節目中提出了一個明確的判斷：任何市值超過 1000 億美元的 AI 公司，最終都必須發展自己的客製化晶片。這不是選擇題，而是生存問題。&lt;/p&gt;&#xA;&lt;p&gt;理由很直接：成本和效率。&lt;/p&gt;&#xA;&lt;p&gt;Nvidia 的 GPU 是「通用型」晶片，設計目標是能夠處理各種不同的運算任務。這種通用性是它的優勢，讓 Nvidia 可以賣給各種不同的客戶。但通用性也意味著妥協——對於任何單一特定任務，通用型晶片都不會是「最佳化」的選擇。&lt;/p&gt;&#xA;&lt;p&gt;客製化晶片（Broadcom 稱之為 XPU）則不同。它是專門為特定模型、特定用途設計的。當你知道這顆晶片只需要跑某一種模型時，你可以把所有不需要的功能都拿掉，把所有資源都集中在你需要的功能上。結果就是：更低的功耗、更高的效率、更低的單位運算成本。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
