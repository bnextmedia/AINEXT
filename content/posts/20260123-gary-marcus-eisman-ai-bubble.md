---
title: "《大賣空》主角對談 AI 批評者：為什麼 OpenAI 可能是下一個 WeWork？"
date: 2026-01-23T10:00:00+08:00
description: "認知科學家 Gary Marcus 與《大賣空》原型 Steve Eisman 對談，拆解 LLM 的根本問題、Scaling 撞牆的現實、以及 AI 產業泡沫的投資風險。當做空次貸的人開始問 AI 的問題，華爾街該緊張了。"
tags: ["Gary Marcus", "Steve Eisman", "OpenAI", "LLM", "AI 泡沫", "Podcast"]
categories: ["AI 產業動態"]
source_url: "https://www.youtube.com/watch?v=aI7XknJJC5Q"
source_name: "The Real Eisman Playbook"
draft: false
---

> 本文整理自 The Real Eisman Playbook 第 42 集，2026 年 1 月播出。

{{< youtube aI7XknJJC5Q >}}

{{< spotify "episode/0sK8Dh9ovBkjlbjrnEzt26" >}}

{{< apple-podcast "us/podcast/gary-marcus-on-the-massive-problems-facing-ai-llm/id1818671690?i=1000745770914" >}}

---

## 當做空次貸的人開始問 AI 的問題

Steve Eisman 不是普通的財經 Podcast 主持人。

2008 年金融海嘯前，他是少數看穿次貸泡沫的人之一，大舉做空，賺進數億美元。他的故事被寫進 Michael Lewis 的《大賣空》（The Big Short），後來改編成同名電影。在電影裡，Steve Carell 飾演的角色 Mark Baum，原型就是他。

所以當 Eisman 找來 Gary Marcus——這位從 2012 年就唱衰深度學習的認知科學家——談 AI 產業的結構性問題時，這場對話的份量就不一樣了。

這不是學術辯論，而是一個曾經看穿金融泡沫的人，試圖理解另一個可能正在形成的泡沫。

## LLM 的本質：類固醇版自動完成

Gary Marcus 的核心論點很簡單：大型語言模型（LLM）本質上就是「類固醇版的自動完成」。

它們做的事情跟 iPhone 上的自動校正一樣——預測下一個字。差別只在於，LLM 被餵了整個網際網路的資料，所以它能「記住」幾乎所有人類問過的問題和答案。

Marcus 用一個詞來形容：「美化過的記憶機器」（glorified memorization machines）。

這解釋了為什麼 LLM 常常能回答問題——因為網路上有人問過，也有人答過。但這也解釋了為什麼它們會「幻覺」——把不相關的資訊片段錯誤地拼湊在一起。

Marcus 舉了一個例子：他的朋友 Harry Shearer（《辛普森家庭》Mr. Burns 配音員、也演過《楚門的世界》）被 ChatGPT 描述成「英國配音員」。問題是，Shearer 出生在洛杉磯，這在維基百科上寫得清清楚楚。

LLM 為什麼會犯這種錯？因為它把「配音員」、「喜劇演員」這些標籤跟「英國」的統計關聯（Ricky Gervais、John Cleese 等人）混在一起，產生了看似合理但完全錯誤的輸出。

更糟的是，這些錯誤往往包裝得很漂亮。文法完美、語氣自信，讓人很容易相信。Marcus 稱之為「看起來不錯效應」（looks good to me effect），這導致了一個新現象：「工作泥漿」（Work Slop）——表面上很專業，但充滿錯誤的報告。

## Scaling 撞牆：GPT-5 的失望

過去幾年，AI 產業的主流信念是「Scaling」——只要把模型做得更大、餵更多資料、用更多 GPU，就能持續進步，最終達到通用人工智慧（AGI）。

Marcus 稱之為「萬億磅寶寶謬誤」：你的寶寶出生時 8 磅，一個月後 16 磅，不代表上大學時會是一萬億磅。

這個謬誤在 2025 年 8 月被戳破。

Sam Altman 花了好幾年暗示 GPT-5 會是革命性的突破，甚至在 2025 年 1 月的部落格寫道「我們現在知道如何打造傳統意義上的 AGI」。結果 GPT-5 發布後，社群只花了一個小時就發現——它還是會幻覺，還是有同樣的問題，只是「稍微好一點」。

Marcus 說，從 GPT-1 到 GPT-4，每一代的進步都是肉眼可見的，你不需要任何測量工具，玩半小時就知道明顯變好了。但 GPT-5 相較於 GPT-4 呢？你得跑正式的 benchmark 才能看出差異，這就是邊際效益遞減的定義。

更有說服力的是 Ilya Sutskever 的轉向。這位 OpenAI 共同創辦人、2012 年 GPU 深度學習論文的共同作者，在離開 OpenAI 後公開表示：「我們需要回到研究的時代。」

當 Marcus 和 Sutskever 這兩個從對立端出發的人，最後得出相同結論，產業應該要認真聽了。

## 沒有護城河：LLM 的商品化宿命

如果 Scaling 是唯一的遊戲規則，那勝者只會是資源最多的人。

Marcus 早在 2023 年就預測：LLM 領域會出現「頂端堆疊」現象——大家用同樣的方法、同樣的架構，最後產品會趨於一致，然後進入價格戰。

這正是現在發生的事。過去一年，每個 token 的價格下跌了 100 倍，Google 追上來了，Anthropic 追上來了，連中國都追上來了。更要命的是，Google 不只追上來，還拿下了 Apple 的合作案，而且 Google 甚至不需要 NVIDIA——它有自己的 TPU（張量處理器），在這場軍備競賽中佔盡優勢。

Eisman 直接問：「如果 Scaling 是唯一的路，誰能比 Google 花更多錢？沒有人。」

Marcus 同意：「這就是為什麼我說 OpenAI 可能是 AI 界的 WeWork。」

## OpenAI 的 WeWork 命運

Marcus 的分析很直接：OpenAI 的營收是幾十億美元，但每個月虧損 30 億美元。他們剛拿到 400 億美元的融資，聽起來很多，但這只夠燒一年左右。下一輪融資需要 1000 億美元，而全世界能開這張支票的人不超過 5 個。

如果這 5 個人中有一個說不呢？兩個呢？三個呢？

更現實的問題是：為什麼要投 OpenAI 而不是 Google？Google 有穩定的現金流、有自己的晶片、而且技術已經追上了。對投資人來說，這個選擇並不困難。

Marcus 預測 OpenAI 最後可能被 Microsoft 吸收，就像 WeWork 最後的命運一樣。到時候人們會回頭問：「當初怎麼會給它那麼高的估值？」

## 真正需要的是什麼：世界模型

如果純 LLM 走不通，那下一步是什麼？

Marcus 的答案是「世界模型」（World Model）。

這個概念來自傳統 AI：你的軟體裡需要有一個對外在世界的表徵。GPS 導航系統知道道路怎麼連接、交通狀況如何，這就是世界模型。

LLM 試圖跳過這一步。它不建立世界模型，只做統計預測。結果就是：它不知道 Harry Shearer 出生在洛杉磯，因為它沒有一個可以「查詢」的知識庫，只有一堆統計關聯。

Marcus 指出，各大公司其實已經悄悄開始整合符號式 AI——比如讓 LLM 呼叫 Python 程式碼來做運算。這些符號工具跑在 CPU 上，不是 GPU 上。這代表什麼？對 NVIDIA 來說，這不是好消息。

但這些都只是補丁。真正的解方是讓 AI 能夠「歸納」世界模型——從資料中理解因果關係、實體、和它們之間的關係。這是個困難的研究問題，不會在短期內解決，但這才是應該投資的方向，而不是繼續把錢砸進 Scaling 的無底洞。

---

## 我的觀察

Marcus 的論點不新。他從 2012 年就在講同樣的話，被 OpenAI 內部當成笑柄，Sam Altman 還在 Twitter 上叫他「酸民」。

但這場對談的意義不在於 Marcus 說了什麼，而在於誰在問這些問題。

Steve Eisman 是那種會花幾年時間研究一個產業、找出結構性問題、然後下重注的人。他不是為了流量做節目，他是在做功課。當這種人開始認真研究 AI 泡沫敘事，華爾街應該要注意了。

對台灣讀者來說，這場對談的啟示很實際：

第一，不要把 LLM 當成魔法。它是有用的工具，但能力邊界比行銷話術說的窄很多，拿來輔助寫作、整理資料、腦力激盪都很好，但如果你的商業模式建立在「LLM 會越來越強，最後取代人類」這個假設上，可能需要重新評估了。

第二，跟風投入 LLM 應用開發要謹慎。如果你的產品只是在 ChatGPT API 上包一層皮，你不會有護城河，因為連 OpenAI 自己都沒有護城河。

第三，關注真正的技術突破，而不是 benchmark 刷分。當產業開始談「推理模型」、「世界模型」、「神經符號整合」時，那才是值得關注的方向，純 Scaling 的故事已經講完了。

Marcus 在節目尾聲說：「AI 真的會改變世界，但不是現在，不是用這個技術。」這句話聽起來像唱衰，但其實是最務實的態度——承認現有技術的極限，才能把資源投到真正有潛力的方向。

WeWork 的故事我們都知道怎麼收場，AI 產業會不會重演？沒人知道。但至少，現在有更多人開始問對的問題了。
