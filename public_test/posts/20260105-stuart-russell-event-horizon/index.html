<!DOCTYPE html>
<html lang="en" >
  <head>
  <title>2030 不歸點： ai 教父的末日時鐘 | AINEXT</title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


<meta name="keywords" content="AINEXT">
<meta property="og:locale" content='en_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="2030 不歸點：AI 教父的末日時鐘">
<meta property="og:description" content="
本文整理自《The Diary Of A CEO with Steven Bartlett》2025 年 12 月 4 日播出的單集。


  ">
<meta property="og:url" content="https://bnextmedia.github.io/AINEXT/posts/20260105-stuart-russell-event-horizon/">
<meta property="og:image" content="https://bnextmedia.github.io/AINEXT/images/images/logo.png">
<link rel="canonical" href="https://bnextmedia.github.io/AINEXT/posts/20260105-stuart-russell-event-horizon/">

<link rel="apple-touch-icon" sizes="180x180" href='https://bnextmedia.github.io/AINEXT/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='https://bnextmedia.github.io/AINEXT/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='https://bnextmedia.github.io/AINEXT/favicon-16x16.png'>
<link rel="manifest" href='https://bnextmedia.github.io/AINEXT/site.webmanifest'>

<link rel="stylesheet" href="https://bnextmedia.github.io/AINEXT/css/styles.648e60c6d86809f863ae1346848574b9c685732794e7851c7d3e557de9ddd293bc1c209f963d5041785c2fd4268470bdfde453a99f550b655d1b4f825eed4682.css" integrity="sha512-ZI5gxthoCfhjrhNGhIV0ucaFcyeU54UcfT5Vfend0pO8HCCflj1QQXhcL9QmhHC9/eRTqZ9VC2VdG0&#43;CXu1Ggg==">
</head>

  <body>
    <div class="nav-drop">
  <div class="nav-body">
      <a href="https://bnextmedia.github.io/AINEXT/" class="nav_item">首頁</a>
      <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav_item">文章列表</a>
    <div class="nav-close"></div><div class="color_mode">
  <label for="mode">Toggle Dark Mode</label>
  <input type="checkbox" class="color_choice" id="mode">
</div>

  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href=https://bnextmedia.github.io/AINEXT/ class="nav-brand nav_item">
        <img src="https://bnextmedia.github.io/AINEXT/images/logo.png" alt="AINEXT " class="logo-light" width="130px" />
        <img src="https://bnextmedia.github.io/AINEXT/images/logo-dark.png" alt="AINEXT " class="logo-dark" width="130px" /></a>
    
    
    <div class="nav-links">
        <a href="https://bnextmedia.github.io/AINEXT/" class="nav-link">首頁</a>
        <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav-link">文章列表</a>
    </div>
    
    <div class="nav_bar-wrap">
      <div class="nav_bar"></div>
    </div>
  </nav>
</header>

<style>
 
.nav-links {
  display: flex;
  flex-direction: row;
  gap: 1.5rem;
  align-items: center;
  white-space: nowrap;
}

.nav-link {
  color: var(--text);
  text-decoration: none;
  font-size: 0.95rem;
  padding: 0.5rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover {
  color: var(--theme);
}

 
@media (min-width: 768px) {
  .nav_bar-wrap {
    display: none;
  }
}

 
@media (max-width: 767px) {
  .nav-links {
    display: none;
  }
  
  .nav_bar-wrap {
    display: grid;
  }
}

 
.logo-dark {
  display: none;
}

html[data-mode="dark"] .logo-light {
  display: none;
}

html[data-mode="dark"] .logo-dark {
  display: block;
}

 
@media (prefers-color-scheme: dark) {
  html:not([data-mode="light"]) .logo-light {
    display: none;
  }
  
  html:not([data-mode="light"]) .logo-dark {
    display: block;
  }
}

 
.nav {
  position: relative !important;
  background: var(--bg);
  padding: 0.5rem 0;
  border-bottom: 1px solid var(--border);
}

.mt {
  margin-top: 2rem !important;
}

 
.post {
  padding-top: 1rem;
}

.post_date {
  margin-top: 0;
}

 
.archive {
  padding-top: 1rem;
}

.archive_title {
  margin-top: 0;
}
</style>


    <main>
      
  <div class="wrap mt post">
    <div><p class=post_date>05. January 2026</p>
      <h1 class="post_title">2030 不歸點：AI 教父的末日時鐘</h1>
      <div class="post_body">
        <div class="post_inner">
        
        
          <blockquote>
<p>本文整理自《The Diary Of A CEO with Steven Bartlett》2025 年 12 月 4 日播出的單集。</p>
</blockquote>
<div class="media-embed">
  <iframe 
    src="https://open.spotify.com/embed/episode/6LDmLYDdYwyBtwCqELGzQk" 
    width="100%" 
    height="152" 
    frameBorder="0" 
    allowfullscreen 
    allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" 
    loading="lazy"
    style="border-radius: 12px;"
  ></iframe>
</div>

<div class="media-embed">
  <iframe 
    allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" 
    frameborder="0" 
    height="175" 
    width="100%"
    style="border-radius: 12px; overflow: hidden;" 
    sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" 
    src="https://embed.podcasts.apple.com/nl/podcast/the-man-who-wrote-the-book-on-ai-2030-might-be-the/id1291423644"
    loading="lazy"
  ></iframe>
</div>

<hr>
<p>「我們可能已經越過了事件視界。」</p>
<p>這是 OpenAI 執行長 Sam Altman 在他的部落格《溫和的奇點》（The Gentle Singularity）中寫下的一句話。Stuart Russell 在接受《The Diary Of A CEO》專訪時，解釋了這句話的真正含義。</p>
<p>Russell 是全球最權威的 AI 教科書《Artificial Intelligence: A Modern Approach》的作者，這本書被翻譯成 15 種語言，在全球超過 1,500 所大學使用，包括臺灣多所頂尖資工系所。他研究 AI 超過 50 年，曾獲英國女王授予 OBE 勳章，並連續多年被《時代》雜誌評選為 AI 領域最具影響力人物。</p>
<p>在這集長達兩小時的訪談中，Russell 詳細剖析了為什麼他認為人類正在接近一個「不可逆轉的臨界點」。</p>
<h2 id="什麼是事件視界">什麼是「事件視界」？</h2>
<p>「事件視界」是物理學中的概念，來自黑洞理論。</p>
<p>想像一個質量極大的物體，大到連光都無法逃脫它的引力。在這個物體周圍，存在一條看不見的界線。一旦你越過這條線，就再也不可能回頭——你會被不可逆轉地拉向黑洞的中心。</p>
<p>這條界線，就叫做事件視界。</p>
<p>Altman 用這個比喻來描述 AI 的發展。他的意思是：我們可能已經到了一個點，無論我們想不想，AGI（通用人工智慧）的到來已經變得不可避免。我們已經被拉進那個引力場，正在被吸向那個中心。</p>
<h2 id="15-萬億美元的磁鐵">15 萬億美元的磁鐵</h2>
<p>Russell 用另一個比喻來解釋這個現象：一塊巨大的磁鐵。</p>
<p>「想想 AGI 的經濟價值，」他說。「我估計大約是 15 萬億美元。這個巨大的獎金就像未來的一塊超級磁鐵，把我們都拉過去。越接近它，吸力越強。」</p>
<p>這解釋了為什麼即使大家都知道風險，也停不下來。越接近 AGI，投資報酬率看起來越高。越多錢投入，就有越多人依賴這些投資獲得回報。這形成了一個正向循環——或者，如果你擔心後果的話，一個惡性循環。</p>
<p>「我們已經開始看到這個投資的衍生效益，」Russell 指出。「比如 ChatGPT，它已經開始產生收入。所以它確實像磁鐵一樣運作。越接近，我們就越難抽身。」</p>
<h2 id="各大執行長怎麼說">各大執行長怎麼說？</h2>
<p>訪談中，主持人 Steven Bartlett 整理了各大 AI 公司執行長對 AGI 時程的預測：</p>
<ul>
<li><strong>Sam Altman</strong>（OpenAI）：2030 年之前</li>
<li><strong>Demis Hassabis</strong>（Google DeepMind）：2030 到 2035 年</li>
<li><strong>Jensen Huang</strong>（NVIDIA）：大約五年</li>
<li><strong>Dario Amodei</strong>（Anthropic）：2026-2027 年會有接近 AGI 的強大 AI</li>
<li><strong>Elon Musk</strong>：2020 年代</li>
</ul>
<p>幾乎所有人的預測都落在五年以內。</p>
<p>但 Russell 有不同看法。</p>
<p>「我實際上認為會更久一些，」他說。「我不認為你可以純粹基於工程來做預測。是的，我們可以把機器做大十倍、做快十倍。但這可能不是我們還沒有 AGI 的原因。」</p>
<p>「事實上，我認為我們擁有的運算能力已經遠超過 AGI 所需——可能超過一千倍。我們還沒有 AGI 的原因，是我們不知道如何正確地建造它。」</p>
<h2 id="我們像古埃及人一樣">「我們像古埃及人一樣」</h2>
<p>Russell 用一個精妙的比喻來說明我們對 AI 的理解程度。</p>
<p>「古埃及人可以精確地測量太陽的運行軌跡。精確到他們可以把金字塔的東西軸完美對準春分點。巨石陣的建造者也展現了對太陽週期的精確掌握。但他們完全不懂軌道力學。他們不知道地球繞著太陽轉。他們不知道為什麼太陽會東升西落、為什麼會有四季。在他們的神話裡，太陽是神駕著戰車拉過天空的。」</p>
<p>「我們對 Scaling Laws（規模定律）的理解，」Russell 繼續說，「與古人對太陽的理解處於類似的階段：精確測量，但缺乏根本性的理解。」</p>
<p>Scaling Laws 是目前 AI 發展的核心驅動力。研究人員發現，當你增加模型的參數量、訓練資料量、以及運算量時，模型的表現會以一種可預測的方式提升。但沒有人真正知道這個定律為什麼會成立。</p>
<p>這帶來一個實際問題：既然我們不知道它為什麼有效，我們也無法確定它什麼時候會停止有效。</p>
<h2 id="快速起飛ai-自己研究-ai">「快速起飛」：AI 自己研究 AI</h2>
<p>Russell 提到了另一個令人擔憂的可能性：「智慧爆炸」（intelligence explosion）。</p>
<p>這個概念最早由 Alan Turing 的同事 I.J. Good 在 1965 年提出。想法是這樣的：一個智慧足夠高的 AI 系統，可以自己做 AI 研究。它可以設計更好的演算法、更好的硬體架構、更好的訓練方法。然後用這些改進來升級自己，變得更聰明。然後再做更多 AI 研究，再次升級自己。</p>
<p>「假設它一開始的『智商』是 150，」Russell 解釋。「它用這個智商做 AI 研究，改進了自己，現在智商變成 170。接著它用 170 的智商做更多研究——而且因為更聰明了，做得更好。下一次迭代變成 250。然後繼續。」</p>
<p>這就是所謂的「快速起飛」（fast takeoff）。一旦 AI 達到某個臨界點，它可能會在極短時間內超越人類智慧，快到我們根本來不及反應。</p>
<p>Altman 自己也說過：「我認為快速起飛的可能性比幾年前我想的更高。」</p>
<h2 id="史上最大的科技專案">史上最大的科技專案</h2>
<p>Russell 提供了一個驚人的數據對比。</p>
<p>二戰期間，曼哈頓計畫開發原子彈的預算，換算成 2025 年的幣值，大約是 200 多億美元。</p>
<p>而明年（2026 年）全球投入 AGI 開發的預算，預計將達到一兆美元。</p>
<p>「這是曼哈頓計畫的 50 倍，」Russell 說。「這是人類歷史上規模最大的科技專案，而且差距還在拉大。」</p>
<p>這麼多錢砸下去，當然會有進展。問題是，這些進展是在安全可控的方向上嗎？</p>
<h2 id="監管一億分之一的標準">監管：一億分之一的標準</h2>
<p>Russell 認為，有效的監管應該要求 AI 公司證明他們的系統是安全的。</p>
<p>他用核電廠的標準來做對比。核電廠必須證明，核心熔毀的機率低於每年一百萬分之一。有些電廠做到了每年一千萬分之一。這需要大量的數學分析——分析每個組件、冗餘系統、監控機制、操作程序。</p>
<p>「那麼，對於人類滅絕這種風險，我們應該接受什麼樣的機率？」Russell 問。「一百萬分之一？滅絕比核電廠熔毀嚴重多了。也許應該是一億分之一？」</p>
<p>「現在那些執行長們說的是多少？25%。他們差了幾百萬倍。」</p>
<p>「如果我們要求他們證明風險低於每年一億分之一，他們會說什麼？他們會說：『我們不知道怎麼做。』」</p>
<p>「這實際上就是在說：人類沒有權利保護自己免受我們的傷害。」</p>
<h2 id="可能建造安全的超級智慧嗎">可能建造安全的超級智慧嗎？</h2>
<p>訪談最後，Bartlett 問了一個關鍵問題：建造安全的、可控的超級智慧 AI，到底有沒有可能？</p>
<p>Russell 說：有可能。但需要一種根本不同的架構。</p>
<p>「長期以來，AI 的概念就是『純粹的智慧』——帶來你自己想要的未來的能力。智慧越高越好。但我們其實不想要純粹的智慧。因為它想要的未來，可能不是我們想要的未來。」</p>
<p>「宇宙並沒有把人類特別標示為唯一重要的東西。純粹的智慧可能決定讓蟑螂過得很好，或者根本不在乎生物生命。」</p>
<p>「我們真正想要的，是一種唯一目的是實現人類想要的未來的智慧。它必須以人類為中心，不是蟑螂，不是外星人，不是它自己。」</p>
<p>問題在於：如何精確定義「人類想要的未來」？這是一個出了名難以回答的問題。Russell 引用了「邁達斯國王」的故事：邁達斯向神許願，希望他碰到的一切都變成黃金。結果他的食物變成黃金，他的女兒變成黃金。他在飢餓和悲傷中死去。</p>
<p>「這說明了精確描述我們想要什麼有多困難。」</p>
<p>Russell 的解決方案是：不要試圖預先定義目標。讓 AI 系統一開始就處於「不知道人類想要什麼」的狀態，然後透過觀察和互動來學習。而且永遠保持一定程度的不確定性——這樣它就不會貿然採取可能傷害人類的行動。</p>
<p>這在數學上是可行的。問題是，目前的公司都在往完全相反的方向跑。</p>
<hr>
<p><strong>關於 Stuart Russell</strong>：
Stuart Russell 是加州大學柏克萊分校計算機科學教授，曾獲英國女王授予 OBE 勳章，連續多年被《時代》雜誌評選為 AI 領域最具影響力人物。他與 Google 研究總監 Peter Norvig 合著的《Artificial Intelligence: A Modern Approach》是全球最暢銷的 AI 教科書，目前已發行第四版（2021 年），臺灣可在天瓏書店購買。他的另一本著作《Human Compatible: Artificial Intelligence and the Problem of Control》深入探討如何建造對人類有益的 AI 系統。</p>

        </div>
        <div class="post_extra mb-2">
          
<div class="copy" data-before="分享故事" data-after="已複製">
  <svg class="icon">
    <use xlink:href="#copy"></use>
  </svg>
</div>
        </div>
        <div>
        
        </div>
      </div>
    </div>
    <a href=https://bnextmedia.github.io/AINEXT/ class="post_nav"><span class="post_next">Latest Posts
      <svg class="icon icon_scale">
        <use xlink:href="#double-arrow"></use>
      </svg>
    </span></a>
  </div>

    </main>
    <footer class="footer wrap pale">
  <p>&copy;&nbsp;<span class="year"></span>&nbsp;AINEXT</p>
  <p class="attribution upcase">由 <a href = 'https://bnextmedia.github.io/AINEXT/' target = '_blank' title = '領英個人檔案' rel = 'nonopener'>AINEXT</a> 設計</p>
</footer>


<script src="https://bnextmedia.github.io/AINEXT/js/index.min.0c2fb80a1ade817d7387f0de8ee061e7de6878a65a420dc61a6e0d0b3bb765c4c0394caefa7c40b16d39c7d74283b3cab364aa109f58cad99d149ec813f930c8.js"></script>

    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
