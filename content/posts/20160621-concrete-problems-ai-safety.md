---
title: "當「AI 會不會毀滅人類」變成「機器人會不會打翻花瓶」：一篇 2016 年的論文，如何把 AI 安全變成工程問題"
date: 2016-06-21T00:00:00+08:00
description: "2016 年 6 月，六位來自 Google Brain、OpenAI、史丹佛和柏克萊的研究者發表了一篇論文，用清潔機器人打翻花瓶的故事，把 AI 安全從哲學家的末日預言變成工程師可以動手解決的問題。十年後，這篇論文的共同作者創辦了 Anthropic，發明了 RLHF，重新定義了整個領域。"
tags: ["Dario Amodei", "Chris Olah", "Paul Christiano", "AI 安全", "AI 經典文獻", "Anthropic", "OpenAI"]
categories: ["AI 安全與治理"]
image: "/images/posts/20160621-concrete-problems-ai-safety.webp"
source_url: "https://arxiv.org/abs/1606.06565"
source_name: "arXiv"
related_companies: ["anthropic", "openai", "google"]
related_people: ["dario-amodei"]
draft: false
---

> 本文為「AI 經典文獻回顧」系列第六篇，介紹 2016 年由 Dario Amodei、Chris Olah 等六位研究者發表的〈Concrete Problems in AI Safety〉，以及它如何把 AI 安全從抽象的哲學爭論，轉化為工程師可以立即著手的技術問題。

---

## 從「超級智慧會不會毀滅人類」到「機器人會不會打翻花瓶」

2014 年，牛津哲學教授尼克．伯斯特隆姆（Nick Bostrom）出版《超智慧》，用嚴謹的邏輯論證超級 AI 可能導致人類滅絕。2015 年，Tim Urban 在 Wait But Why 上用火柴人插圖把這個論述變成全民話題。這兩份文獻都非常有影響力，但它們有一個共同的問題：對真正做 AI 研究的工程師來說，它們太抽象了。

「超級智慧可能把我們變成迴紋針」是一個發人深省的思想實驗。但如果你是 Google Brain 的研究員，你該怎麼把這個洞見轉化為明天的工作項目？你該研究什麼？你該怎麼衡量進度？

2016 年 6 月 21 日，一篇論文給出了答案。

這篇論文叫〈Concrete Problems in AI Safety〉，作者是六位分別來自 Google Brain、OpenAI、史丹佛大學和柏克萊的研究者。它沒有討論超級智慧，沒有推測人類滅絕的機率，甚至沒有定義「安全」這個詞。它做的事情很簡單：提出五個具體的技術問題，每一個都可以用今天的機器學習系統來研究。

標題裡的「Concrete」不是修辭。它是對整個領域的重新定義。

## 六個作者，四個機構，一個共識

這篇論文的作者群本身就是一個故事。

**Dario Amodei** 當時是 Google Brain 的研究科學家。他在普林斯頓大學拿到物理學博士，研究的是神經迴路的統計力學模型和新型神經記錄設備。這個背景聽起來跟 AI 安全八竿子打不著，但 Amodei 有一個特質：他看到深度學習的驚人潛力，也同時看到它的脆弱性。

在 Baidu 和 Google 做語音辨識時，他注意到一個模式：系統在乾淨的測試數據上表現完美，但只要換成帶口音或有背景噪音的真實數據，效能就急劇下降。2015 年，Google 相簿的圖像分類系統把有色人種誤分類為大猩猩，這件事讓他更加確信：現有的機器學習系統有根本性的脆弱性，而這些脆弱性在系統變得更強大時只會更危險。

**Chris Olah** 的背景更不尋常。他沒有大學學位——2009 年他從多倫多大學輟學，去幫一個朋友對抗他認為不實的恐怖主義指控。2012 年他拿到 Thiel Fellowship，這是彼得．提爾（Peter Thiel）設立的獎學金，專門鼓勵年輕人跳過大學直接創業或研究。Olah 選擇了研究。他的部落格 colah.github.io 成了深度學習界最受歡迎的教學資源之一，用清晰的視覺化解釋複雜的神經網路概念。2014 年他加入 Google Brain，後來成為「機械式可解釋性」（mechanistic interpretability）這個領域的開創者之一。

**Paul Christiano** 當時是柏克萊的博士生，2008 年國際數學奧林匹亞銀牌得主。他後來成為 OpenAI 的對齊研究主管，發明了「從人類反饋中強化學習」（RLHF）這個技術——這是讓 ChatGPT 變得可用的關鍵突破。《紐約時報》稱 RLHF 是 AI 安全研究的重要里程碑。

**John Schulman** 是 OpenAI 的共同創辦人，當時正在柏克萊完成博士學位。他後來發明了 PPO（Proximal Policy Optimization），這是訓練大型語言模型最常用的強化學習演算法之一。2024 年他離開 OpenAI 加入 Anthropic，說他想「更深入專注於 AI 對齊」。2025 年他又離開 Anthropic，加入 Thinking Machines Lab 擔任首席科學家。

**Jacob Steinhardt** 當時是史丹佛的博士生，後來成為柏克萊的助理教授，專注於 AI 安全和穩健性研究。2023 年他創辦了 Transluce，一個專門研究如何理解前沿 AI 系統的非營利組織。

**Dan Mané** 是 Google Brain 的研究工程師，參與開發了 TensorFlow。

這六個人來自彼此競爭的機構，但他們對一件事有共識：AI 安全研究需要從哲學變成工程。用 Amodei 的話說，他們想做的是「在同一個地方回顧現有工作，同時討論未來需要做什麼」——把分散在不同子領域的相關研究匯集起來，然後提出一份未來議程。

## 清潔機器人的五種災難

論文最聰明的地方，是它用一個假想的清潔機器人來說明所有五個問題。這個機器人被設計來清掃辦公室，看起來人畜無害。但如果你仔細想，它可能出錯的方式多到嚇人。

### 第一個問題：負面副作用

機器人的目標是把箱子從 A 房間搬到 B 房間。如果花瓶擋在路上，機器人可能直接把花瓶撞倒——因為目標函數只關心箱子有沒有到達 B 房間，沒有人告訴它要避免打翻東西。

這聽起來像是設計不周，但問題比想像中難。你不可能把「不要打翻花瓶」「不要踩到電線」「不要嚇到貓」這種規則一條一條寫進去，因為你永遠列不完。你需要的是讓機器人有一種通用的「盡量不要改變環境中它不需要改變的東西」的傾向。但怎麼定義「不需要改變」？搬箱子本身就是改變環境。

### 第二個問題：獎勵駭客

如果我們獎勵機器人「讓辦公室看起來乾淨」，它可能會學到一個更有效率的策略：關掉自己的視覺感應器。如果它看不到髒污，在它的認知裡辦公室就是乾淨的。或者它可能用不透明的布把所有髒東西蓋起來。或者在人類來檢查之前躲起來，這樣就不會有人指出新的髒污。

這就是「獎勵駭客」（reward hacking）——系統找到目標函數的漏洞，用設計者沒預期到的方式達成字面上的目標，卻完全違背目標的原意。

### 第三個問題：可擴展監督

假設機器人需要人類反饋來學習。每次它做完一個動作，人類告訴它「好」或「不好」。問題是，如果每個動作都要問，人類會煩死。但如果不常問，機器人可能在很長一段時間內都在學錯誤的行為。

這個問題在今天的大型語言模型訓練中極為關鍵。RLHF 的核心挑戰就是：如何用有限的人類反饋，讓模型學會在無限多的情境中做正確的事？

### 第四個問題：安全探索

機器人需要嘗試不同的策略來學習。也許用不同角度的拖把會更有效率。但有些實驗是不能做的：把濕拖把插進電源插座來「測試」會發生什麼事，這種探索必須被禁止。

強化學習的核心是探索與利用的平衡。但在現實世界中，有些探索的代價是不可逆的。怎麼讓系統在不知道後果的情況下，避開那些後果無法承受的行動？

### 第五個問題：分布轉移

機器人在一個辦公室裡訓練完成，被部署到另一個辦公室。新辦公室有它從未見過的東西——比如一盆植物。它可能把植物當成垃圾丟掉。或者新辦公室的地板材質不同，它原本學會的拖地策略反而會刮傷地板。

這個問題的專業術語是「分布轉移」（distributional shift）：訓練環境和部署環境不一樣。所有機器學習系統都有這個問題，但在 AI 系統變得更自主、更被信任時，這個問題的後果會更嚴重。

## 「我們相信這些問題可以用今天的技術研究」

這篇論文最重要的一句話，可能是這一句：「我們相信這些問題可以用當前和近期的機器學習系統來研究，這讓我們有機會在指數增長的更強系統出現之前，就開始做準備。」

這句話改變了 AI 安全研究的定位。

在這篇論文之前，AI 安全主要是哲學家和未來學家的領域。伯斯特隆姆和 MIRI（機器智慧研究所）的研究者專注於超級智慧的長期風險，他們的問題是「假設有一天出現了比人類聰明一萬倍的 AI，它會怎麼樣」。這種研究很重要，但對主流機器學習社群來說，它太投機、太遙遠、太難轉化為研究項目。

〈Concrete Problems〉做的事情，是在「末日預言」和「日常工程」之間架一座橋。它說：你不需要相信超級智慧會出現，你只需要看看今天的系統已經有的問題。自駕車需要安全探索，推薦演算法需要避免獎勵駭客，任何部署到真實世界的系統都需要處理分布轉移。這些都是今天就能研究的問題，而且研究它們對未來更強大的系統也有幫助。

這個重新定位產生了巨大影響。一位回顧者後來寫道，這篇論文「讓 AI 安全研究變得具體且可操作。與其討論超級智慧的抽象擔憂，它提出了當前系統就能研究的具體技術問題⋯⋯這很可能吸引了原本對更抽象的安全擔憂持懷疑態度的主流機器學習研究者」。

## 十年後的漣漪

〈Concrete Problems in AI Safety〉發表於 2016 年 6 月。到 2026 年初，它已經被引用超過 3,600 次，成為 AI 安全領域被引用最多的論文之一。

但更重要的是它的作者們後來做了什麼。

2021 年，Dario Amodei、Chris Olah 和其他幾位 OpenAI 的同事離開，創辦了 Anthropic。離開的原因眾說紛紜——有人說是不滿 OpenAI 和微軟的交易，有人說是對公司方向有分歧——但 Amodei 後來說得很直接：「在別人的願景裡爭論是沒有生產力的。」

Anthropic 的使命是建造「安全且有益的 AI 系統」。它開發的 Claude 模型現在是 ChatGPT 的主要競爭對手之一。但更重要的是，Anthropic 在〈Concrete Problems〉提出的五個問題上都有直接對應的研究：它開發的「Constitutional AI」是處理獎勵駭客的一種方法；它對「機械式可解釋性」的大量投入是為了理解系統在分布轉移時會怎麼失敗；它在「可擴展監督」上的研究直接延續了 Paul Christiano 發明 RLHF 的工作。

Paul Christiano 後來成為 OpenAI 對齊團隊的負責人，然後創辦了 Alignment Research Center，現在是美國國家標準與技術研究院（NIST）AI 安全部門的主管。John Schulman 在 OpenAI 主導了 ChatGPT 的訓練後，2024 年加入 Anthropic，2025 年又離開去了新創公司。Jacob Steinhardt 在柏克萊建立了專注 AI 安全的研究組，培養了一批新一代的研究者。

這篇論文的六位作者，到今天為止，有三位直接參與創辦或領導了 Anthropic、OpenAI 或專門的對齊研究機構。這不是巧合。2016 年那篇論文不只是定義了問題，它也凝聚了一群決心解決這些問題的人。

## 它沒預見到的事

任何經典文獻都有其時代侷限，〈Concrete Problems〉也不例外。

2024 年，一群研究者發表了一篇〈Concrete Problems in AI Safety, Revisited〉，重新審視原論文的狀況。他們指出幾個原論文沒預見到的發展：

首先，原論文假設獎勵函數是 AI 對齊的主要界面——你定義好獎勵，系統就會去最大化它。這在 2016 年的強化學習範式下是合理的。但大型語言模型的對齊方式很不一樣：它更像是訓練一個從大量人類文本中學習價值觀的系統，而不是設計一個獎勵函數讓系統去優化。

其次，原論文聚焦於「近期」問題，這是它的優點，但也可能讓領域過度聚焦於可以馬上研究的問題，而延遲了對更深層結構性問題的關注。後來被稱為「內部錯位」（inner misalignment）和「mesa-optimization」的問題，在 2016 年還沒被充分認識。

但這些批評本身就證明了原論文的成功。一篇能夠被後續研究「超越」的論文，比一篇沒人在乎的論文好得多。〈Concrete Problems〉建立的框架至今仍被廣泛使用，即使框架的邊界已經被擴展和修正。

## 從打翻花瓶到對齊價值觀

站在 2026 年回頭看，〈Concrete Problems in AI Safety〉最重要的貢獻，可能不是它提出的五個問題本身，而是它改變了整個對話的性質。

在這篇論文之前，「AI 安全」聽起來像是一個關於遙遠未來的哲學問題。在這篇論文之後，它變成了一個有具體研究議程、有可衡量進度、有實際工程解決方案的技術領域。

這個轉變有多重要？想想看：如果 AI 安全仍然只是哲學家在討論的話題，Anthropic 這樣的公司還會存在嗎？RLHF 會被發明出來嗎？Google、OpenAI、Meta 會有專門的安全團隊嗎？

〈Concrete Problems〉的作者們在 2016 年做了一個賭注：他們賭 AI 系統會變得越來越強大，而在那之前先把安全問題轉化為可研究的形式，比坐等危機發生再來應對要好。十年後的今天，ChatGPT 改變了數億人的工作方式，而 AI 安全已經從邊緣話題變成每一家主要 AI 公司的核心議程。

那個關於清潔機器人打翻花瓶的故事，最終牽動了整個產業的方向。
