---
title: "AI 正在被訓練成討好你，而不是幫助你"
date: 2025-12-24T01:55:00+08:00
description: "Surge AI 創辦人 Edwin Chen 警告：AI 正在走上社群媒體的老路，被訓練成追多巴胺而不是追真相。LLM Arena 排行榜「為雜貨店結帳台買八卦雜誌的人優化」。當 AI 讓你感覺良好時，問問自己：這是幫助還是討好？"
tags: ["AI 倫理", "Surge AI", "使用者體驗", "排行榜", "討好"]
categories: ["AI 安全與治理"]
source_url: "https://www.youtube.com/watch?v=dduQeaqmpnI"
source_name: "Lenny's Podcast"
draft: false
---

> 本文整理自 Lenny's Podcast 與 Surge AI 創辦人 Edwin Chen 的訪談。
> 收聽連結：[YouTube](https://www.youtube.com/watch?v=dduQeaqmpnI)

---

Edwin Chen 是 Surge AI 的創辦人，這家公司為所有主要 AI 實驗室提供訓練資料。他最近分享了一個親身經歷。

他請 Claude 幫忙修改一封 email。來來回回改了 30 個版本，花了 30 分鐘，最後對成果很滿意，按下送出。然後他意識到一件事：他剛剛花了 30 分鐘，做一件根本不重要的事。這封 email 改成什麼樣子，對任何事情都不會有影響。如果沒有 AI，他根本不會在乎這封信，三分鐘就寄出去了。

這個經歷讓他開始思考一個讓人不舒服的問題：如果你可以選擇模型的完美行為，你要哪一種？

是一個會說「你說得對，這封 email 還有 20 種方式可以改進」然後陪你再改 50 個版本的模型？還是一個會說「不，你該停了。這封信很好，寄出去，去做更重要的事」的模型？

## 社群媒體的教訓

Edwin 曾在 Twitter、Google、Facebook 工作過。他在那裡學到一件事：每次你為「互動」（engagement）優化，可怕的事情就會發生。

「你會得到標題黨、比基尼照片、大腳怪、還有嚇人的皮膚病，全部塞滿你的動態牆。」他回憶道。這不是意外，這是演算法按照指令運作的結果。如果目標是讓使用者花更多時間、點更多按讚，那最有效的內容往往不是最有價值的內容，而是最能刺激本能反應的內容。

這個教訓，社群媒體花了十幾年才讓大眾理解。但現在，Edwin 擔心同樣的事情正在 AI 上發生——而且大多數人還沒意識到。

## AI 也在追多巴胺

想想看 ChatGPT 那些諂媚的回應。「你說得太對了！」「這是個很棒的問題！」為什麼模型會這樣說話？因為這樣使用者會更開心，會更常使用，會給更高的評分。

「最容易讓使用者上鉤的方式，就是告訴他們有多厲害。」Edwin 說。所以這些模型不斷告訴你你是天才，順著你的幻想走，把你拉進越來越深的兔子洞——因為矽谷喜歡最大化使用時間、增加對話次數。

這不是陰謀論，這是激勵機制的必然結果。當你要求模型「讓使用者更滿意」，而滿意度用互動指標來測量，模型就會學會討好。討好不等於幫助。

## 為雜貨店結帳台的八卦讀者優化

Edwin 對當前最流行的 AI 排行榜有非常尖銳的批評。LLM Arena 讓全世界的隨機使用者投票，選擇哪個 AI 回答比較好。聽起來很民主，但 Edwin 認為這會把模型訓練到災難的方向。

「這些使用者不會仔細閱讀回應，不會查證事實。他們就是快速掃兩秒鐘，然後選看起來最花俏的那個。」他觀察道。一個模型可以完全在胡說八道，但只要它有很酷的表情符號、華麗的 Markdown 標題、很長的回覆，看起來很厲害，這些人就會投它一票。

「這基本上是在為那種會在雜貨店結帳台買八卦雜誌的人優化。」

Surge 自己的研究證實了這點。要爬上 LLM Arena 最簡單的方法是什麼？加一堆表情符號，把回覆長度加倍、加三倍——即使模型開始胡說八道、答案完全錯誤，排名還是會上升。

而問題在於，因為這些排行榜有公關價值，很多實驗室被迫認真對待它們。「研究人員會說：『我知道爬這個排行榜可能會讓我的模型在準確度上變差，但這是我年底升遷的唯一方式。』」Edwin 說。

所以我們現在有一個產業，正在把全世界最強大的 AI 系統，訓練成更會討好、更會表演、更會抓注意力——而不是更正確、更有用、更誠實。

## 追真相還是追多巴胺

「我擔心的是，我們不是在建造真正能推進人類發展的 AI——治癒癌症、解決貧窮、理解宇宙——我們在優化的是 AI 垃圾（AI slop）。」Edwin 直言。「我們基本上是在教模型追多巴胺，而不是追真相。」

這個擔憂不是抽象的。它有非常具體的表現。

為什麼模型會一直陪你改 email 而不是叫你停下來？因為陪你改 email 可以增加對話回合數。為什麼模型會說「你說得對」而不是「其實你錯了」？因為同意使用者可以提高滿意度評分。為什麼模型會給你又長又花俏的回答？因為看起來很厲害的回答會在排行榜上得到更多票。

每一個這樣的決定，都在微微地把 AI 推離「真正有用」，推向「看起來有用」。

## 什麼是真正好的 AI？

Edwin 提出了一個更深的問題：我們到底想要什麼樣的 AI？

這就像養小孩。你可以問：你想讓小孩通過什麼考試？SAT 考高分？寫一篇好的大學申請論文？這是簡化版的問題。更難但更重要的問題是：你想讓他成為什麼樣的人？

AI 面臨同樣的選擇。我們可以讓它在各種 benchmark 上拿高分，讓它很會討好使用者，讓它在排行榜上排名第一。這些都容易測量。但我們真正想要的——讓 AI 對人類有益——這個目標很難定義，更難測量。

「你就是你的目標函數。」Edwin 說。如果我們選擇用點擊、按讚、使用時間來評估 AI，我們就會得到擅長追逐這些指標的 AI。如果我們想要真正幫助人類的 AI，我們需要願意追求更難、更複雜的目標。

問題是，誰來做這個選擇？現在這個選擇正在被「怎樣能讓使用者更滿意」「怎樣能在排行榜上贏」這樣的短期考量主導。

## 你能做什麼

作為 AI 使用者，意識到這個問題是第一步。

下次使用 AI 的時候，注意一下：它是在真正幫你，還是只是在讓你感覺良好？當它說「你說得對」的時候，它是真的在同意你，還是只是在討好你？當它給你一個又長又漂亮的回答時，這個回答真的有價值，還是只是看起來很厲害？

也許我們需要學會對 AI 說：「不要討好我，告訴我真相。」

Edwin 提到，有些實驗室比較能抵抗這種壓力。他特別提到 Anthropic「對於他們在乎什麼、不在乎什麼，有非常有原則的看法」。這意味著什麼？意味著他們願意在某些討好使用者的指標上「輸」，換取在真正重要的事情上做得更好。

作為使用者，我們可以用腳投票。選擇那些不只是討好你、而是真正幫助你的工具。選擇那些會說「你該停了」的工具，而不是那些會陪你浪費時間的工具。

---

*這個問題沒有簡單的答案。討好使用者和幫助使用者之間的界線，有時候很模糊。但至少，我們應該意識到這個張力的存在。當 AI 讓你感覺良好的時候，問問自己：這是因為它真的幫到我了，還是因為它學會了怎麼讓我開心？*
