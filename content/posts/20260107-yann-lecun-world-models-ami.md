---
title: "Yann LeCun：LLM 永遠無法達到人類智慧，世界模型才是正途"
date: 2026-01-07T14:00:00+08:00
description: "圖靈獎得主 Yann LeCun 離開 Meta 創辦 AMI，主攻世界模型與 JEPA 架構。他直言 LLM 無法處理高維度連續資料，批評矽谷陷入「單一文化」，並預測達到狗的智慧程度是最困難的一步。"
tags: ["Yann LeCun", "世界模型", "JEPA", "AMI", "Meta", "Podcast"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=7u-DXVADyhc"
source_name: "The Information Bottleneck"
draft: false
---

> 本文整理自 The Information Bottleneck Podcast 2025 年 12 月播出的單集。

{{< youtube 7u-DXVADyhc >}}

---

## 65 歲創業：為什麼離開 Meta？

Yann LeCun 在 Meta 待了 12 年，一手創建了 FAIR（Fundamental AI Research Lab），推動開放研究文化，讓 PyTorch 成為業界標準。然而，他觀察到產業風向正在改變——不只是 OpenAI，連 Google 和 Meta 都開始變得封閉，減少發表論文。

「如果你不發表，你很容易被自己騙。」LeCun 說。他堅信研究必須公開，讓社群檢驗，否則只會陷入內部炒作的迷思。

這促使他在 65 歲創辦 Advanced Machine Intelligence（AMI），專注於世界模型（World Models）與規劃能力。公司在巴黎和紐約都設有辦公室，刻意避開矽谷的「單一文化」。

**他的使命很簡單：增加世界上的智慧總量。**

「智慧是最稀缺的資源，」他說，「這就是為什麼我們花這麼多資源在教育上。增加智慧對人類和地球都是好事。」

---

## LLM 的根本問題：處理不了真實世界

LeCun 對大型語言模型的批評一針見血：

> 「我們永遠、永遠不可能只靠訓練文字就達到人類等級的 AI。這根本不會發生。」

他的論點基於資料量的對比。訓練一個頂級 LLM 需要約 32 兆個 token（約 10^14 bytes），相當於網路上幾乎所有可用的文字資料。但這麼多資料換算成影片，只有 15,000 小時——大約等於：

- 30 分鐘的 YouTube 上傳量
- 一個四歲小孩一生看過的視覺資訊

問題在於，文字資料是由孤立事實組成的，冗餘度低。但真實世界的資料（如影片）有著豐富的結構和冗餘性，這正是自監督學習能發揮的地方。

**LLM 無法處理高維度、連續、有雜訊的資料**——這正是現實世界的特性。你不能用離散的 token 來表示空氣流動、物體碰撞、或是一杯咖啡被打翻時液體的物理行為。

---

## 世界模型不是模擬器

很多人誤解世界模型，以為它要像《星艦迷航》的全息甲板一樣重現每個細節。LeCun 說這完全搞錯方向。

他用計算流體力學（CFD）舉例：工程師模擬飛機周圍的氣流時，不會去模擬每個空氣分子的碰撞，而是用抽象的方式——把空間切成小方塊，每個方塊只用速度、密度、溫度幾個數字來代表。

「如果我問你木星 100 年後會在哪裡，你只需要六個數字：三個位置、三個速度。其他資訊都不重要。」

**世界模型的關鍵是在抽象表示空間（abstract representation space）中做預測**，只模擬相關的部分，忽略無法預測的細節。這正是 JEPA（Joint Embedding Predictive Architecture）的核心理念。

---

## JEPA：在表示空間中預測

JEPA 不同於傳統的生成式模型。傳統方法試圖重建輸入的每個細節（如自編碼器），但 LeCun 發現這是錯誤的直覺——堅持讓表示包含所有資訊是個壞主意。

JEPA 的做法是：

1. 把輸入 X 和目標 Y 都通過編碼器，得到各自的表示
2. 訓練預測器從 X 的表示預測 Y 的表示
3. 關鍵：預測在抽象空間進行，不重建原始細節

但這有個大問題：系統可能會「崩塌」（collapse），產生恆定的表示來讓預測變得簡單。LeCun 和團隊花了多年解決這個問題，從對比學習到 Barlow Twins，再到 VICReg（variance-invariance-covariance regularization），最終發展出 SigReg 和 LE-JEPA。

**VJPA2**（去年夏天發布）已經在相當於一世紀的影片資料上訓練，產生了優質的視覺表示。

---

## 達到狗的智慧是最難的一步

當被問到何時能達到人類等級的 AI，LeCun 先否定了「通用智慧」這個概念：

> 「通用智慧根本不存在，這個概念毫無意義。人類智慧是高度專業化的——我們擅長處理現實世界、擅長理解其他人，但我們下棋很爛。」

他認為，最樂觀的情況下，5-10 年內可能達到接近狗的智慧程度。但這是最難的一步。

為什麼？因為一旦達到狗的程度，大部分的基礎建設就完成了。從靈長類到人類，除了腦容量之外，主要差異只是語言。而語言由大腦中很小的區域（韋尼克區和布洛卡區）處理，在不到一百萬年內演化出來，不會太複雜。

「我們已經有 LLM 可以處理語言了。我們現在研究的是前額葉皮層——那是世界模型所在的地方。」

---

## 矽谷的「單一文化」問題

LeCun 對矽谷的批評很直接：所有公司都在做同樣的事。

OpenAI、Meta、Google、Anthropic 都在追逐 LLM。競爭太激烈，沒有人敢採用不同的技術，因為害怕落後。這創造了「羊群效應」和單一文化。

> 「你被 LLM 洗腦了（LLM-pilled）。你以為只要擴大 LLM、訓練更多合成資料、僱用數千人做後訓練，就能達到超級智慧。這完全是胡扯。」

這種單一文化的風險是被場外的創新突襲——像 DeepSeek 這樣的中國團隊提出新方法時，矽谷公司都很震驚：「什麼？矽谷以外的人也能有原創想法？」

更諷刺的是：美國公司越來越封閉，中國公司卻完全開源。目前最好的開源模型是中國的，這讓很多美國業界人士很不安。

---

## 安全性：內建護欄，而非事後微調

LeCun 對 AI 安全的看法不同於「末日論者」。他認為每種強大技術都有正面和負面效果，關鍵在於工程上如何處理。

他用噴射引擎舉例：一開始跑 10 分鐘就爆炸，但經過數十年的工程改進，現在可以安全飛行 17 小時橫跨半個地球。AI 也會經歷同樣的過程。

但他強調，**微調 LLM 來防止危險行為是錯誤的方法**——總是可以被越獄。

正確的做法是建立「目標驅動的 AI 架構」（objective-driven AI）：

1. 系統有世界模型，能預測行動的後果
2. 通過優化來找出達成目標的行動序列
3. 同時滿足一系列護欄約束

「這是**架構上**的安全，不是微調。系統無法逃脫這些約束，因為它必須通過優化來滿足它們。」

---

## 如果世界模型成功了

當被問到 20 年後的願景，LeCun 引用了 Linus Torvalds 的名言：「目標是全球統治。」

他笑著說這很好笑，但 Linux 確實做到了——幾乎世界上每台電腦都跑 Linux（除了少數桌機和 iPhone）。

他的願景是建立一套訓練智慧系統的方法，讓 AI 在日常生活中幫助人類和整個地球。這些系統會放大人類智慧，而非取代或統治人類。

> 「不是因為某個東西聰明，它就想要統治。這是兩回事。即使在人類中，也不是最聰明的人想要支配別人。」

---

## 給年輕人的建議

LeCun 建議想進入 AI 領域的年輕人：學習有長期價值的東西，以及學習如何學習。

諷刺的是，身為電腦科學教授，他說：「電腦科學通常不是那種有長期價值的東西。」

他推薦的基礎：

- **數學**：微積分、機率論、線性代數
- **工程**：控制理論、訊號處理、最佳化
- **物理**：關於如何對現實建立預測模型

「物理學是關於：我應該用什麼來表示現實，才能做出預測。這正是智慧的本質。」

---

## 結語：歷史會重演

LeCun 在最後提醒：不要被當前的 AI 熱潮沖昏頭。

在他的職業生涯中，「當前主流技術將帶我們走向人類智慧」這種幻覺已經發生過三次，在他之前可能發生過五六次。

1950 年代有 General Problem Solver，1980 年代有專家系統和知識工程師。每次都有人說「10 年內會有超級智慧機器」，每次都證明是錯的。

**當前的 LLM 熱潮可能也是同樣的情況。**

但這不代表 AI 不會進步。只是真正的突破可能來自完全不同的方向——比如世界模型、JEPA，或是我們還沒想到的東西。

而這正是 LeCun 在 65 歲創業的原因。
