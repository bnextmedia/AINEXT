---
title: "NVIDIA 的全棧帝國：CES 2026 揭示黃仁勳的終極野心"
date: 2026-01-07T12:00:00+08:00
description: "CES 2026 上，NVIDIA 用一整天的活動展示了驚人的野心：從晶片到系統到模型到應用，從自駕車到機器人到企業 AI，黃仁勳正在建構一個前所未有的全棧帝國。這不只是產品發布，而是一場關於 AI 產業未來控制權的宣言。"
tags: ["NVIDIA", "Jensen Huang", "CES 2026", "Vera Rubin", "Alpamayo", "Physical AI", "Cosmos", "Snowflake", "Mercedes-Benz", "AI Agent"]
categories: ["科技巨頭觀察"]
source_url: "https://www.youtube.com/watch?v=0NBILspM4c4"
source_name: "NVIDIA"
draft: false
---

> 本文整理自 NVIDIA 於 CES 2026（2026 年 1 月 5 日）發表的完整活動，包含開場 Panel 討論與黃仁勳主題演講。

{{< youtube 0NBILspM4c4 >}}

---

## 開場白：這不只是產品發布

3,000 人擠滿拉斯維加斯 Fountain Blue 劇院，另外 3,000 人在場外透過螢幕觀看，全球數百萬人同步收看直播。這是 CES 2026 最受矚目的一場活動，但黃仁勳要展示的，遠不只是幾顆新晶片。

當你看完這整場活動，你會發現一件事：**NVIDIA 已經不是一家賣 GPU 的公司了。** 它正在系統性地吃下 AI 產業鏈的每一層——從最底層的晶片，到最上層的應用程式，中間經過基礎設施、模型、開發工具，全部都要。

更令人不安的是，它的策略不是封閉壟斷，而是「開放壟斷」。所有模型都開源，所有工具都免費，所有架構都公開——但每一層都跑在 NVIDIA 的硬體上。這是一種比微軟 Windows 更聰明的生態鎖定。

讓我從頭說起。

---

## 第一部分：Panel 討論——NVIDIA 生態圈的集體現身

主題演講前，NVIDIA 安排了一場精心策劃的 Panel 討論。表面上是邀請產業專家聊 AI 趨勢，實際上是讓整個 NVIDIA 生態圈集體現身，展示這家公司的觸角已經伸到多遠。

### 1.1 AI 基礎設施：為什麼這次不是泡沫

Bank of America 半導體分析師 Vivek Arya 開場就點出一個關鍵問題：過去三年，超過 8000 億美元砸進 AI 基礎設施，光是 2026 年預估就有 6000 億美元。每個人都在問：這是泡沫嗎？

Arya 給出三個理由，說明為什麼這次不同：

**第一，無縫採用（Seamless Adoption）**。ChatGPT 上線那天，全球 50 億手機和電腦用戶立刻可以使用。不像過去的光纖建設，基礎設施蓋好了，終端用戶還沒準備好。這次是基礎設施追著需求跑。

**第二，高利用率**。還記得「暗光纖」（Dark Fiber）這個詞嗎？當年光纖鋪滿地底，卻沒人在用。但現在，Arya 說：「你聽不到『暗運算』（Dark Compute）這個詞。所有部署的運算基礎設施都被充分利用，連六、七年前的老舊 GPU 都跑滿了。」

**第三，資金來源不同**。上一波是資金不足的新創在燒錢，這一波是現金流充沛的大公司在投資。他們的資本支出只占營運現金流的三分之二，自由現金流依然為正。

這段話表面上在分析產業，實際上在說：**NVIDIA 的需求是真實的，而且會持續。**

### 1.2 Snowflake：資料平台如何成為 NVIDIA 附庸

Snowflake CEO Sridhar Ramaswamy 上台，談的是他們與 NVIDIA 的合作。但聽完之後，你會發現 Snowflake 在 AI 領域的每一步都離不開 NVIDIA：

- Snowflake Intelligence（他們的 AI 產品）跑在 NVIDIA 晶片上
- 過去訓練的大型基礎模型用 NVIDIA 晶片
- 正在開發的嵌入模型與 NVIDIA 深度合作
- 未來 GPU 加速資料運算也要靠 NVIDIA

Ramaswamy 還透露了一個細節：**他自己的手機上就跑著一個資料 Agent**，隨時可以問「這個客戶的狀況如何？關係是上升還是下降？」這種即時存取企業資料的能力，正是 AI Agent 的殺手級應用。

但更有趣的是他談到的企業採用障礙：

> 「實務問題永遠存在。資料所有權、資料主權、特定地區的 GPU 微短缺（micro shortages）......我們經常遇到德國客戶沒有足夠的德國境內算力，必須協商能不能把流量送到瑞典。」

這段話揭示了一個現實：**NVIDIA GPU 的短缺已經細化到區域層級**。不是全球缺貨，是德國缺、瑞典不缺。這種「微短缺」反而強化了 NVIDIA 的議價地位。

### 1.3 開放模型 vs 封閉模型：開發者心智的爭奪戰

Ramaswamy 對開放模型有一段精闘的分析：

> 「前沿模型在關鍵應用上——像 Agent 工具呼叫、程式碼 Agent——還是遠遠領先。但當事情成熟、人們想要大規模運行時，開放模型就變得越來越重要。」
>
> 「我們都需要記住：開放模型對開發者心智有巨大影響。只有 OpenAI 那幾千名工程師能接觸到最新最強的模型，這是一個很小的環境。但我們反覆看到，開放平台能吸引成千上萬、甚至數百萬開發者投入，這可以形成正向循環。」
>
> 「這就是為什麼連 OpenAI 這種封閉公司也會釋出開放模型——因為他們想成為開發者生態圈的一部分。」

這段話解釋了為什麼 NVIDIA 要大力投入開放模型。不是因為善心，是因為**開發者在哪裡，生態圈就在哪裡，硬體需求就在哪裡**。

### 1.4 醫療 AI：Abridge 的臨床革命

Abridge 創辦人 Shiv 帶來了醫療 AI 的第一線觀察。他引用了一個驚人的數據：

> 「幾年前《一般內科醫學期刊》有篇文章指出，醫生需要一天 30 小時才能完成所有工作。」

目前醫生的工作分配是 80% 文書作業、20% 面對面臨床推理。Abridge 的目標是用 AI 翻轉這個比例。

Shiv 強調，醫療 AI 的成功不在技術，在於**融入工作流程**：

> 「我們必須把自己塞進工作流程，而不是讓工作流程來配合我們。這不只是勾選隱私和安全的方框，還要考慮延遲、要考慮醫療紀錄的所有產出物。」

他們的做法是蒸餾（distillation）、微調（fine-tuning）、後訓練（post-training）。現在 Abridge 每年將觸及 8000 萬到 1 億條生命，每天從編輯中學習，持續變得更好。

關於信任，Shiv 說得很直接：

> 「信任是基本門檻。它是透明度、可靠性、可信度的組合。透明度包括可審計性、與醫療系統分享我們的指標。可信度則是發表研究、與研究者合作進行隨機對照試驗來評估實際影響。」

他們的信條是：**省時間、讓人更專注彼此、省錢**。最終目標是第三階段——救命，幫助臨床醫生做出更好的決策。

### 1.5 程式碼審查 AI：CodeRabbit 的信任層

CodeRabbit 創辦人 Harjot 代表的是 AI 程式碼領域。他的定位很清楚：

> 「程式碼生成的量大幅增加，CodeRabbit 用生成式 AI 來審查這些程式碼。我們提供一個關鍵的信任層，介於你的程式碼 Agent 和生產環境之間，讓組織能對進入生產環境的東西設定護欄。」

這家公司已經被數萬個組織、數十萬開發者使用。Harjot 觀察到一個有趣的現象：

> 「生成式 AI 降低了進入軟體開發的門檻。以前的瓶頸是『我不會 TypeScript、不會 Rust』，現在這個門檻大幅降低了。同時，生成式 AI 並不會讓 10 倍工程師變成 100 倍。它把開源中的優秀範例帶給普通開發者或初學者，讓他們的程式碼『夠好』。」

換句話說：**AI 縮小了高手和新手的差距，但沒有把天花板提高。** 這解釋了為什麼審查變得更重要——產出更多了，但品質參差不齊。

### 1.6 Agent 可靠性：Human-in-the-Loop 的必要性

當被問到「Agent 可靠性是否已經解決」時，Shiv 和 Harjot 都給出了務實的答案。

Shiv 區分了兩種情境：

> 「在醫療領域，有高頻率、低風險的工作流程，Agent 可以在適當的護欄下在背景執行。但也有需要人類參與的任務，特別是涉及化療、重要診斷或治療決策時，必須有人類做最後一哩。」

Harjot 則指出程式碼領域的現狀：

> 「我們看到強烈產品市場契合的使用案例，都是非常互動式的——人類在迴圈中，持續與 Claude Code 或其他 Agent 對話。挑戰在於，一旦進入背景 Agent 可以長時間運行的模式，可靠性就非常低。」
>
> 「你會發現，十個開發者裡，可能只有兩三個真的把 Cursor 或 Claude Code 用得很好，其他人還在摸索正確的提示技巧。」

這是一個重要的現實檢查：**Agent 的炒作與實際可靠性之間還有很大落差。**

### 1.7 開源模型的成本考量

關於開源模型是否已經追上閉源模型，Harjot 的觀察是：

> 「12 個月前，開源模型還很早期。但現在我們看到開源模型開始取代我們部分工作負載，像是摘要和程式碼庫搜尋。這些可以用更小、更划算的開源模型很好地完成。」
>
> 「『你只能用最好的程式碼模型，不管多貴多慢』的敘事已經改變了。開源確實在追上。但頂級模型還是頂級模型——像是程式碼審查這種推理密集的使用案例，我們還是會用 Opus 或 GPT-5.2 這類模型的最高設定。」

Shiv 補充了垂直領域的觀點：

> 「在醫療這種垂直產業的應用層，擁有更多控制權會帶來很大的差異。LLM 對垂直任務來說還沒有商品化——你需要在上面做很多工作才能創造差異化。」

### 1.8 Mercedes-Benz：Level 3 自駕的真實挑戰

Mercedes-Benz CEO Ola Källenius 帶來了自駕車的第一手經驗。Mercedes 是最早取得美國 Level 3 自駕認證的車廠之一。

Källenius 強調，Level 3 的突破不只是工程問題，更是法律問題：

> 「Level 3 的重大突破不只是工程任務，也是法律任務——因為電腦真的接管了。這是 Level 2（人類負責）和 Level 3 及以上（電腦負責）的差別。」

他也坦承「長尾問題」的挑戰：

> 「讓工程團隊做到 99% 的展示相對容易。但其他所有可能發生的事情的長尾，那才是更大的挑戰。這就是為什麼安全是關鍵。你可以快速但馬虎，但那樣你承擔的風險可能超出你的預期。」

Mercedes 的策略是：不急著搶第一，但要確保推出的東西夠穩健。他們剛在舊金山和矽谷測試了與 NVIDIA 合作的 Level 2++ 系統：

> 「我昨天開了一個多小時，從舊金山穿過相當繁忙的週日交通，上高速公路、下高速公路、回到市區，全程無中斷。感覺就像車子在軌道上行駛。」

這套系統將在今年稍晚於美國上市，使用 NVIDIA 技術。

### 1.9 Skilled AI：通用機器人大腦

Skilled AI 創辦人兼卡內基美隆大學機器人學教授 Deepak Pathak，帶來了機器人領域最前沿的思考。

他的一句話定義：**「任何機器人、任何任務、一個大腦。」**

為什麼要追求這種通用性？Pathak 解釋：

> 「想到語言模型，你會想到大腦。想到機器人，你會想到硬體。但過去 70 年，儘管有那麼多令人印象深刻的機器人展示，每年都感覺機器人要搶走工作、要進入家庭了，卻從未發生。原因是缺少一個通用大腦。」
>
> 「硬體有那麼多種——工廠用的、家用的、狗型的、人形的。如果能跨機器人身體通用，就能利用任何來源的資料，創造持續推動進步的資料飛輪。」

關於機器人領域的資料稀缺問題，Pathak 的解法是：

> 「沒有魔法子彈。不像數位 AI，網路上有資料。機器人沒有。所以我們必須從某處啟動。我們從人類影片啟動——觀看人類做事，可以是第一人稱視角，也可以是 YouTube 或 Flickr 上的第三人稱視角。」
>
> 「但光看不夠。如果夠的話，我看了那麼多費德勒的比賽，應該打得跟他一樣好了。你必須練習、嘗試、失敗。但在真實世界失敗幾百萬次是不可能的，這就是模擬的用處。」
>
> 「真實人類影片加模擬，這是我們啟動的方式。然後創建基礎層，再針對各種任務微調。比如在 Mercedes 工廠的某個場景，我們可以收集遙控資料來微調。」

他也呼應了語言模型的經驗：**先通用，再專精。** 在 LLM 出現之前，每個應用都有專門的系統，但都很爛。LLM 的發現是：先做通用模型，再微調。機器人也應該走同樣的路。

### 1.10 工廠機器人：百年來最大的生產力躍進

Källenius 對工廠機器人的前景非常興奮：

> 「聽 Deepak 講，我很興奮，因為我知道機器人領域正在發生的事，將會是我們工廠幾十年甚至一百年來最大的生產力躍進。」
>
> 「你可以想像，一個製造業工人在做某些任務，旁邊有他的機器人夥伴。這個機器人夥伴是會思考的機器，你可以跟它說話、給它指令，它可以幫你完成任務。」

他們已經在工廠進行試驗，預計三到五年內，走進 Mercedes 工廠會看到完全不同的景象。

同時，透過 NVIDIA Omniverse，他們在虛擬世界建造工廠，在灌注任何混凝土之前，就能在虛擬世界完整生產汽車，用 AI 工具除錯。這讓建造製造設施更快、更便宜。

Pathak 補充了一個重要觀點：

> 「在工廠裡，如果我要選一個任務垂直深入，有幾百個任務可選。每個任務可能只有三四個人在做，但每個任務都跟其他任務有點不同。這是機器人本質上是通用問題的另一個論點。」

---

## 第二部分：黃仁勳主題演講——全棧帝國的藍圖

Panel 討論結束，黃仁勳登場。接下來兩小時，他系統性地展示了 NVIDIA 的全棧布局。

### 2.1 雙重平台轉移：AI 正在重塑一切

黃仁勳開場就定調：

> 「每 10 到 15 年，電腦產業就會重置。新的平台轉移發生。從大型主機到 PC、PC 到網路、網路到雲端、雲端到行動裝置。每一次，應用程式都會以新平台為目標。」
>
> 「但這一次，有兩個平台轉移同時發生。第一，應用程式現在建構在 AI 之上。人們一開始以為 AI 是應用程式，但其實 AI 是應用程式，而且你還要在 AI 上面建構應用程式。」
>
> 「第二，軟體的運行和開發方式徹底改變了。整個電腦產業的軟體堆疊正在被重新發明。你不再『程式設計』軟體，而是『訓練』軟體。不再跑在 CPU 上，而是跑在 GPU 上。」

這意味著過去十年約 10 兆美元的運算投資，現在都要「現代化」到新架構。這解釋了為什麼有數千億美元的創投資金湧入，為什麼各產業的研發預算正在從傳統方法轉向 AI 方法。

### 2.2 2025 年四大突破

黃仁勳回顧了過去一年的關鍵進展：

**Scaling Laws 持續有效**：從 2015 年的 BERT、2017 年的 Transformer、2022 年的 ChatGPT，到 2023 年 O1 推理模型引入的「測試時擴展」（test-time scaling），每個階段都需要更多算力。預訓練、後訓練（強化學習）、推理時思考，三個階段的運算需求都在爆炸。

**Agentic Systems 興起**：具備推理、研究、使用工具、規劃未來、模擬結果能力的 AI Agent。黃仁勳特別點名 Cursor：「徹底改變了我們在 NVIDIA 做軟體開發的方式。」

**Physical AI 成為焦點**：理解物理世界常識的 AI——物體恆存、因果關係、摩擦力、重力、慣性。這些對幼兒是常識，但對 AI 完全未知。同時還有「AI 物理學」——理解物理定律的 AI。

**開放模型達到前沿**：DeepSeek R1 的出現激活了整個開放模型運動。雖然仍落後封閉模型約六個月，但每六個月就有新模型出現。下載量爆炸，因為新創、大公司、研究者、學生、每個國家都想參與 AI 革命。

### 2.3 NVIDIA 的開放模型帝國

這可能是最令人意外的部分。NVIDIA 投入數十億美元建造自己的 DGX 超級電腦，不是為了賣雲端服務，而是為了開發開放模型。

黃仁勳展示了 NVIDIA 在各領域的開放模型：

| 領域 | 模型 | 功能 |
|------|------|------|
| 數位生物學 | La Proteina | 蛋白質合成與生成 |
| | OpenFold3 | 蛋白質結構理解 |
| | Evo 2 | 多蛋白質理解與生成，細胞表示的開端 |
| 物理模擬 | Earth 2 | 理解物理定律的 AI |
| | ForecastNet | 天氣預測 |
| | CoreDiv | 物理模擬 |
| 語言模型 | Nemotron | 混合 Transformer-SSM 架構，可快速思考或深度推理 |
| 世界模型 | Cosmos | 理解世界運作方式的開放基礎模型，對齊語言 |
| 機器人 | Groot | 人形機器人系統，整合關節、移動、行走 |
| 自駕車 | Alpamayo | 會思考、會推理的自駕 AI |

關鍵是：**不只開源模型，還開源訓練資料。** 黃仁勳說：「只有這樣，你才能真正信任模型是怎麼來的。」

他們還提供完整的生命週期管理工具（NEMO 系列），從資料處理、生成、訓練、評估、護欄到部署，全部開源。

這些模型不只是開源，還登上各種排行榜榜首——智慧、PDF 解析、語音辨識、語意搜尋。NVIDIA 現在是 AI 模型領域貢獻最多的公司之一。

### 2.4 Agentic AI 架構：未來應用程式的藍圖

黃仁勳詳細解釋了 AI Agent 的架構革命：

> 「ChatGPT 剛出來時，人們說它會幻覺。當然會幻覺——它能記住過去的一切，但無法記住未來和現在。所以它需要做研究、查資料。」
>
> 「推理的能力——判斷是否需要做研究、是否需要使用工具、如何把問題拆解成步驟——每個步驟都是 AI 知道怎麼做的事，組合起來就能完成從未被訓練過的任務。這是推理的美妙之處。」

他特別提到 Perplexity 的創新：**同時使用多個模型。** 一個 AI 可以呼叫世界上所有優秀的 AI 來解決問題的不同部分。這讓 AI 天生就是多模態（理解語音、圖像、文字、影片、3D、蛋白質）、多模型（使用最適合任務的模型）、多雲（模型分散在各處）、混合雲（有時在邊緣、有時在企業、有時在醫院）。

現場展示了一個簡單的個人助理 Demo：用 DGX Spark（家用 AI 電腦）搭配 Hugging Face 的 Ricci 機器人，建立一個能管理日曆、電子郵件、待辦事項的助理。幾分鐘內就能組合前沿模型 API、本地開放模型、意圖路由器、機器人控制、語音合成。

黃仁勳說：「這在幾年前是完全不可想像的，現在是完全微不足道的。」

這個架構已經整合到企業 SaaS 平台：

- **Palantir**：整個 AI 和資料處理平台由 NVIDIA 加速
- **ServiceNow**：全球領先的客服和員工服務平台
- **Snowflake**：頂級雲端資料平台
- **CodeRabbit**：NVIDIA 內部大量使用
- **CrowdStrike**：用 AI 偵測 AI 威脅
- **NetApp**：資料平台加上語意 AI 和 Agent 系統

重點是：**Agentic 系統將成為這些平台的使用者介面。** 不再是 Excel 表格或命令列，而是像跟人對話一樣與平台互動。

### 2.5 Physical AI：三台電腦的架構

Physical AI 是黃仁勳談了好幾年的主題，NVIDIA 已經投入八年。核心問題是：如何讓 AI 從螢幕走進真實世界？

這需要教 AI 物理世界的常識：物體恆存（看開再看回來，東西還在）、因果關係（推一下會倒）、摩擦力、重力、慣性（重卡車需要更長時間煞車）、滾動的球會繼續滾。這些對小孩是常識，但 AI 必須學習。

更重要的是，AI 必須能**模擬其行動的後果**。它怎麼知道自己的動作是否正確？必須在環境中模擬物理世界對其行動的反應。

這需要三台電腦：

1. **訓練電腦**：訓練 AI 模型
2. **推理電腦**：在車輛、機器人、工廠等邊緣設備運行
3. **模擬電腦**：這是 NVIDIA 最舒適的領域，模擬是 Physical AI 一切的基礎

三台電腦上運行多個軟體堆疊：
- **Omniverse**：數位孿生、物理模擬世界
- **Cosmos**：世界基礎模型（不是語言的基礎模型，是世界的基礎模型），對齊語言
- **Groot**：人形機器人模型
- **Alpamayo**：自駕車模型

### 2.6 Cosmos：把運算變成資料

Physical AI 最大的挑戰是資料。網路上有大量文字，但沒有大量機器人資料。影片很多，但不足以涵蓋所需的多樣性和互動類型。

解法是**合成資料生成**——用模擬來創造訓練資料。NVIDIA Cosmos 就是這個魔法的核心。

Cosmos 是什麼？

- 預訓練於網路規模的影片、真實駕駛和機器人資料、3D 模擬
- 學習了世界的統一表示，能對齊語言、圖像、3D 和動作
- 執行 Physical AI 技能：生成、推理、軌跡預測

它能做什麼？

- 從單張圖像生成逼真影片
- 從 3D 場景描述生成物理一致的動作
- 從駕駛遙測和感測器日誌生成環視影片
- 從規劃模擬器生成多攝影機環境
- 從場景提示生成邊緣案例
- 運行互動式閉迴圈模擬：當動作發生，世界回應

最關鍵的功能：**Cosmos 推理。** 它能分析邊緣場景，拆解成熟悉的物理互動，推理接下來可能發生什麼。

黃仁勳說：「Cosmos 把運算變成資料，訓練自駕車處理長尾問題，訓練機器人適應每種場景。」

Cosmos 已經被下載數百萬次，全球都在使用，為 Physical AI 新時代做準備。

### 2.7 Alpamayo：會思考的自駕車 AI

今日發布的 Alpamayo 是「世界第一個會思考、會推理的自駕車 AI」。

它的特點：

- **端到端訓練**：從攝影機輸入直接到方向盤、煞車、油門輸出
- **多重資料來源**：人類示範駕駛 + Cosmos 生成的合成資料 + 數十萬個仔細標註的範例
- **會解釋行動**：不只執行，還會說明「我要做什麼」和「為什麼」
- **推理能力**：在每個場景都會推理即將採取的行動

為什麼推理能力重要？因為自駕的「長尾問題」——不可能收集每個國家、每種情境、每個可能發生的事情。但如果能把未見過的場景分解成一堆熟悉的子場景，AI 就能推理出解決方案。

現場播放了一段 Alpamayo 的駕駛影片：全程一鏡到底、無人手介入，從「導航到目的地」到「你已抵達」，中間穿越各種交通情境，每一步都在畫面上顯示 AI 的推理過程。

### 2.8 八年的垂直整合

黃仁勳回顧了 NVIDIA 在自駕車領域的八年旅程：

> 「我們很早就推論，深度學習和 AI 將重新發明整個運算堆疊。如果我們要理解如何導航自己、如何引導產業走向這個新未來，我們必須學會建造整個堆疊。」

AI 是五層蛋糕：

1. **最底層**：土地、電力、外殼（在機器人的情況下是車體）
2. **第二層**：晶片（GPU、網路晶片、CPU）
3. **第三層**：基礎設施（Omniverse、Cosmos）
4. **第四層**：模型（Alpamayo）
5. **第五層**：應用程式（Mercedes-Benz）

NVIDIA 與 Mercedes-Benz 合作五年，建造了整個堆疊。這是 NVIDIA 第一個完整的垂直整合案例。

時程：
- 2026 Q1：美國上路
- 2026 Q2：歐洲
- 2026 Q3-Q4：亞洲

而且會持續更新，推出 Alpamayo 的新版本。

### 2.9 雙重冗餘的安全架構

Alpamayo 雖然端到端訓練，技能驚人，但沒人能保證永遠完美安全。所以 NVIDIA 建立了雙重軟體堆疊：

1. **Alpamayo 堆疊**：端到端訓練，驚人的技能
2. **傳統 AV 堆疊**：完全可追溯，花了六七年建造

兩個堆疊互相鏡像。一個策略和安全評估器決定：如果我對這個情境非常有信心，讓 Alpamayo 處理；如果不太確定，切回傳統 AV 堆疊。

黃仁勳說：「我們是世界上唯一一輛同時運行兩個 AV 堆疊的車。所有安全系統都應該有多樣性和冗餘性。」

這套系統剛獲得 NCAP 評為「世界最安全的車」。每一行程式碼、晶片、系統都經過安全認證。

### 2.10 機器人的時代

自駕車之後，下一個時代是機器人。黃仁勳邀請了一群機器人上台：四足機器人、人形機器人、送餐機器人、手術機器人、工業機器人......

這些機器人都：
- 內建 Jetson 電腦
- 在 Omniverse 的 Isaac Sim 和 Isaac Lab 模擬器中訓練
- 使用同樣的 Physical AI 技術

現場展示了機器人在模擬器中學習的過程——這就是機器人學會成為機器人的方式。

合作夥伴包括：Neurobot、Agibot、LG、Caterpillar（最大的機器人）、Surf Robot（送餐到你家，連接 Uber Eats）、Agility、Boston Dynamics、手術機器人、Franca 的操作機器人、Universal Robotics......

### 2.11 晶片設計革命：Cadence、Synopsys、Siemens

NVIDIA 的技術現在成熟到可以反過來革命化創造它的產業——晶片設計。

宣布與三大巨頭的整合：

**Cadence**：
- CUDA-X 整合到所有模擬和求解器
- NVIDIA Physical AI 用於不同的物理工廠和工廠模擬
- AI 物理學整合
- 領先物理設計（佈局佈線）和仿真驗證

**Synopsys**：
- 領先邏輯設計和 IP
- 同樣整合 NVIDIA 技術
- 未來會有 Agentic 晶片設計師和系統設計師，就像現在有 Agentic 軟體工程師一樣

**Siemens**（今日宣布）：
- 整合 CUDA-X、Physical AI、Agentic AI、Nemo、Nemotron、Omniverse
- 涵蓋 EDA、CAE、數位孿生工具和平台
- 從設計模擬到生產營運的完整工業生命週期

黃仁勳的願景：機器人會在電腦中被設計、在電腦中被製造、在電腦中被測試，「遠在你需要對抗重力之前」。製造工廠本身也會是巨大的機器人。

### 2.12 Vera Rubin：下一代 AI 超級電腦

以發現暗物質的天文學家 Vera Rubin 命名，這是 NVIDIA 的下一代 AI 超級電腦。

為什麼需要它？AI 運算需求正在爆炸：
- 模型每年增大 10 倍
- Token 生成量每年增加 5 倍（因為 AI 現在會「思考」）
- 強化學習引入後訓練，運算量再爆炸
- 成本每年下降 10 倍（代表競爭有多激烈，大家都在搶下一個前沿）

**問題是**：摩爾定律放緩，電晶體數量只增加 1.6 倍。如果不做極致協同設計（extreme co-design），不可能跟上這種速度。

所以 NVIDIA 打破了自己的規則（通常每代只改一兩顆晶片），這次**重新設計了全部六顆晶片**：

| 晶片 | 關鍵突破 |
|------|----------|
| **Vera CPU** | 88 核心、176 執行緒（空間多執行緒），每瓦效能是業界最強 CPU 的 2 倍 |
| **Rubin GPU** | 浮點運算是 Blackwell 的 5 倍，但電晶體只多 1.6 倍 |
| **NVFP4 Tensor Core** | 革命性處理單元，動態調整精度和結構，不是簡單的 FP4 數字 |
| **NVLink 6 Switch** | 400Gbps SerDes，史上最快；單機架 240TB/s 頻寬（全球網路的 2 倍） |
| **ConnectX 9** | 世界最好的 NIC，可程式化 RDMA，與 Vera CPU 協同設計 |
| **BlueField 4** | 虛擬化、安全、網路卸載，加上革命性的 KV Cache 管理 |
| **Spectrum X** | 矽光子 AI 乙太網路交換器，512 埠 × 200Gbps，雷射直接連接晶片 |

### 2.13 機架革命

舊的 NGX 機架：43 條線纜、6 條管線、組裝要 2 小時、80% 液冷
新的 Vera Rubin 機架：0 條線纜、2 條管線、組裝只要 5 分鐘、100% 液冷

單一機架規格：
- 144 顆 Rubin GPU（每個 NVLink 72 節點有 72 顆，共 2 個節點）
- 2200 億個電晶體
- 重約 2 噸（因為忘了把水排掉，運來時是 2.5 噸）
- 背面有 2 英里的銅纜（5000 條銅纜），讓 NVLink 脊柱以 400Gbps 連接

### 2.14 BlueField 4 與 KV Cache 革命

這是一個新類別的儲存系統，解決 AI 領域的重大痛點。

問題是什麼？AI 推理時，每生成一個 Token，GPU 就要讀取整個模型和整個工作記憶體（KV Cache），產生一個 Token，再存回去。隨著對話變長、模型變大、使用者變多，這個 KV Cache 會爆炸性成長。

原本放在 HBM 記憶體裡，不夠了。去年用 Grace CPU 的快速記憶體擴展，還是不夠。傳統解法是用北南網路傳到儲存系統，但大量 AI 同時運行時，網路會成為瓶頸。

解法：**BlueField 4 + 機架內 KV Cache 儲存**

在機架內放入 KV Cache 儲存節點：
- 每個 BlueField 4 後面有 150TB 記憶體
- 分配給每個 GPU 後，每個 GPU 額外獲得 16TB 上下文記憶體
- 用同樣的東西向流量、同樣的 200Gbps 資料率
- 比原本節點內的 1TB 擴展了 16 倍

這對 AI 實驗室和雲端服務商來說是救命的——KV Cache 流量造成的網路壓力是他們最大的痛點之一。

### 2.15 效能數據與系統特性

**訓練吞吐量**（10 兆參數模型，DeepSeek++ 規模，100 兆 Token，1 個月）：
- Blackwell（綠色）：需要較多系統
- Rubin：吞吐量高得多，只需四分之一的系統數量

**工廠吞吐量**（每瓦效能，直接關係到資料中心營收）：
- Blackwell 比 Hopper 高 10 倍
- Rubin 再高 10 倍

**Token 成本**：
- Rubin 約是 Blackwell 的十分之一

**其他系統特性**：

| 特性 | 說明 |
|------|------|
| 能源效率 | 功耗翻倍，但冷卻水維持 45°C（不需冷水機），省下約 6% 全球資料中心電力 |
| 機密運算 | 全系統加密——傳輸中、靜態、運算中。每條匯流排（PCIe、NVLink、CPU-GPU）都加密 |
| 電力平滑 | AI 工作負載會瞬間飆升 25% 電流（all-reduce 運算），現在有全系統電力平滑，不用過度配置 |

**宣布**：Vera Rubin 已進入量產。

---

## 第三部分：觀點評論——開放壟斷的終極形態

看完這整場活動，讓我分享幾個觀察。

### 3.1 「開放」是最聰明的壟斷策略

NVIDIA 把所有模型開源、所有工具免費、所有架構公開。這看起來很慷慨，但仔細想想：

- 開源模型吸引開發者 → 開發者在 NVIDIA 硬體上開發 → 生態鎖定
- 免費工具降低門檻 → 更多人進入 AI 領域 → 更多 GPU 需求
- 公開架構讓合作夥伴能整合 → 整合越深，切換成本越高

這跟微軟當年的策略異曲同工：Windows 本身不是利潤中心，但它鎖定了整個 PC 生態圈。NVIDIA 的 CUDA 也是同樣邏輯，現在更進一步——連模型、工具、架構都開放，但硬體依賴越來越深。

### 3.2 垂直整合的深度前所未見

NVIDIA 現在的觸角：

- **晶片層**：GPU、CPU、NIC、DPU、Switch，全部自己設計
- **系統層**：整機、機架、冷卻、電源，全部自己規格
- **基礎設施層**：Omniverse（模擬）、Cosmos（世界模型）、NEMO（工具鏈）
- **模型層**：語言、視覺、物理、生物、機器人、自駕，全領域
- **應用層**：與 Palantir、ServiceNow、Snowflake 等深度整合

這不是賣 GPU 的公司，這是要吃下整個 AI 產業鏈的公司。

### 3.3 競爭者的困境

AMD 在同一個 CES 發布了 MI-455X 和 Helios 機架。但問題是：

- NVIDIA 有 CUDA 生態鎖定（十幾年的開發者累積）
- NVIDIA 有完整的軟體堆疊（從訓練到推理到模擬）
- NVIDIA 有開放模型吸引開發者
- NVIDIA 有垂直整合的系統優勢

AMD 賣的是「晶片」，NVIDIA 賣的是「平台」。這是完全不同層次的競爭。

Intel、Google TPU、Amazon Trainium 面臨同樣的困境。硬體規格可以追上，但生態圈很難複製。

### 3.4 Physical AI 是下一個戰場

這場演講最重要的訊息可能是：**NVIDIA 已經在 Physical AI 領域建立了完整的護城河。**

Cosmos 是世界模型，Omniverse 是模擬平台，Alpamayo 和 Groot 是應用模型。訓練、推理、模擬三台電腦的架構已經定義好了。

當機器人、自駕車、工業自動化開始大規模部署，這套架構很可能成為事實標準。就像 CUDA 在 AI 訓練領域的地位一樣。

### 3.5 台灣的角色

值得注意的是，NVIDIA 的硬體製造高度依賴台灣：

- 晶片：台積電製造
- 系統：鴻海、廣達、緯創等組裝
- 供應鏈：數千家台灣零組件廠商

這既是機會也是風險。機會是台灣廠商深度參與 AI 革命；風險是過度依賴單一客戶（NVIDIA 佔台積電先進製程產能的很大比例）。

---

## 完整發布清單

| 類別 | 名稱 | 說明 |
|------|------|------|
| **世界模型** | Cosmos | 開放的世界基礎模型，理解世界運作方式，對齊語言 |
| **自駕 AI** | Alpamayo | 首款會思考、會推理的自駕 AI，端到端訓練，今日開源 |
| **機器人** | Groot | 人形機器人模型，整合關節、移動、行走 |
| **語言模型** | Nemotron 3 | 混合 Transformer-SSM 架構，可快速或深度推理 |
| **生物模型** | La Proteina, OpenFold3, Evo 2 | 蛋白質合成、結構理解、細胞表示 |
| **物理模型** | Earth 2, ForecastNet, CoreDiv | 物理定律理解、天氣預測 |
| **CPU** | Vera | 88 核心、176 執行緒，每瓦效能業界最強 2 倍 |
| **GPU** | Rubin | 5 倍 Blackwell 浮點運算 |
| **Tensor Core** | NVFP4 | 動態精度調整的革命性處理單元 |
| **NIC** | ConnectX 9 | 可程式化 RDMA，與 Vera 協同設計 |
| **DPU** | BlueField 4 | 虛擬化、安全、KV Cache 管理 |
| **Switch** | NVLink 6 | 400Gbps SerDes，單機架 240TB/s |
| **乙太網路** | Spectrum X (矽光子) | 512 埠 × 200Gbps，雷射直連晶片 |
| **超級電腦** | Vera Rubin | 下一代 AI 超級電腦，已進入量產 |
| **合作夥伴** | Siemens | CUDA-X、Physical AI、Omniverse 整合到工業平台 |
| | Cadence, Synopsys | AI 整合到 EDA 工具 |
| | Mercedes-Benz | Alpamayo 自駕系統，2026 Q1 美國上路 |
| | Palantir, ServiceNow, Snowflake | 企業 AI 平台整合 |

---

## 結語：帝國的輪廓

黃仁勳用一句話結束演講：

> 「我們的工作是創造整個堆疊，讓你們能為世界創造令人難以置信的應用。」

這句話聽起來謙虛，但背後的意涵是：**NVIDIA 要成為 AI 時代的基礎設施供應商，就像電力公司、自來水公司一樣不可或缺。**

不同的是，電力公司只賣電，NVIDIA 不只賣算力，還定義了你怎麼用這些算力。從晶片到系統到模型到工具到架構，全部由 NVIDIA 定義，全部開放給你用——只要你跑在 NVIDIA 硬體上。

這是一種新型態的壟斷：**開放的壟斷、生態的壟斷、標準的壟斷。**

CES 2026 的這一天，我們見證的不只是產品發布，而是一個帝國的輪廓逐漸清晰。
