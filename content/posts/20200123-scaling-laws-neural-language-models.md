---
title: "一篇物理學家寫的論文，如何給了矽谷砸千億美元的數學底氣"
date: 2020-01-23T00:00:00+08:00
description: "2020 年 1 月，OpenAI 的 Jared Kaplan 等人發表了 Scaling Laws 論文，用冪律方程式證明語言模型的表現與參數量、資料量、計算量之間存在可預測的數學關係。這篇沒有提出新架構的論文，卻從根本上改變了 AI 產業分配數千億美元資源的邏輯。"
tags: ["Scaling Laws", "Jared Kaplan", "OpenAI", "Chinchilla", "AI 經典文獻"]
categories: ["AI 技術前沿"]
image: "/images/posts/20200123-scaling-laws-neural-language-models.webp"
source_url: "https://arxiv.org/abs/2001.08361"
source_name: "arXiv"
related_companies: ["openai", "anthropic", "google-deepmind"]
draft: false
---

> 本文為「AI 經典文獻回顧」系列第十篇（上），介紹 Kaplan et al. 於 2020 年 1 月發表的論文《Scaling Laws for Neural Language Models》。這篇論文用物理學的方法量化了「越大越好」的直覺，為 AI 產業的大規模投資提供了數學基礎。下篇見〈一篇論文，拆出了 Anthropic：Scaling Laws 背後的人與路線之爭〉。

![封面圖](/images/posts/20200123-scaling-laws-neural-language-models.webp)

---

## 一篇物理學家寫的論文，如何給了矽谷砸千億美元的數學底氣

2020 年 1 月 23 日，一篇論文悄悄出現在 arXiv 預印本伺服器上。標題是《Scaling Laws for Neural Language Models》，來自 OpenAI，作者列了十個人。這篇論文沒有提出任何新的模型架構，沒有打破任何基準測試紀錄，甚至沒有發布任何可以下載的程式碼。它做的事情聽起來平淡無奇：測量。

研究者們系統性地訓練了數百個不同大小的語言模型，從一千多個參數到十幾億個參數，改變訓練資料量、改變計算預算，然後把結果畫成圖。他們發現，模型的表現和這三個變數之間存在一種極為規律的數學關係：冪律（power law）。不是大致上有關聯，而是在對數座標上幾乎畫出一條直線那種程度的規律。

這個發現的意義在當時並不完全清楚。但五年後回頭看，這篇論文可能是 AI 產業史上最昂貴的一張圖表。因為它告訴了全世界一件事：如果你想讓 AI 更聰明，有一個可預測的方程式告訴你需要多大的模型、多少數據、多少算力。剩下的只是錢的問題。從 GPT-3 到 GPT-4，從微軟的百億美元投資到 Stargate 計畫的千億美元藍圖，每一筆巨額支出的背後，都站著這篇論文提供的數學信心。

## 2020 年 1 月：GPT-2 已出，GPT-3 未至

要理解這篇論文為什麼重要，得先回到它出現的那個時間點。

2020 年 1 月的 AI 領域正處在一個微妙的階段。GPT-2 已經在 2019 年 2 月發布，用 15 億個參數證明了語言模型可以生成品質驚人的文字，甚至讓 OpenAI 一度以「太危險」為由拒絕釋出完整版本。但 GPT-3 還沒問世，它要到同年 6 月才會以 1,750 億參數的規模震驚世界。

在那個時間窗口裡，整個 AI 社群面臨一個根本性的問題：我們該繼續把模型做大嗎？直覺上，更大的模型表現更好，但「更好」是什麼意思？好多少？好到什麼程度會停下來？把模型從 15 億參數擴大到 1,500 億參數，需要的算力和金錢是天文數字。如果表現只提升一點點，這筆投資就是浪費。如果表現提升很多但遲早會撞到天花板，那你至少需要知道天花板在哪裡。

在 Kaplan 的論文之前，這些問題的答案基本上是靠直覺。研究者根據經驗和手感來決定模型該多大、資料該餵多少、訓練該跑多久。這不是說沒有人注意到規模和表現之間的關係。2017 年，Hestness 等人就做過一些初步的觀察。我們在這個系列的第一篇介紹過，Google 的三位研究者早在 2009 年就主張「海量數據比精巧演算法更有效」。理察．薩頓（Richard Sutton）在 2019 年的〈苦澀的教訓〉中更是把「算力勝過人類巧思」提升到了歷史法則的高度。但這些都是定性的觀察——「越大越好」、「越多越好」。

Kaplan 的論文做了一件之前沒人做過的事：把「越大越好」量化成精確的數學方程式。

## 一個理論物理學家眼中的 AI

這篇論文的第一作者是 Jared Kaplan，而他不是一個典型的 AI 研究者。

Kaplan 是約翰霍普金斯大學的理論物理學教授。他的學術背景是弦理論和量子場論，博士論文研究的是反德西特空間中的全像對偶（AdS/CFT correspondence），這是理論物理學中最抽象的數學結構之一。他在哈佛讀博士期間，跟未來的 Fields 獎候選人和諾貝爾獎得主們一起做高能物理研究。

一個研究弦理論的物理學家怎麼跑去研究語言模型？答案藏在方法論裡。理論物理學家做研究有一個根深蒂固的習慣：測量系統在不同尺度下的行為，然後找出支配這些行為的數學規律。這在物理學裡叫做「標度律」（scaling laws），它源自統計力學和重正化群理論。水在接近沸點時的行為、磁鐵在接近居里溫度時的表現、甚至地震的頻率和震級之間的關係，都遵循冪律。物理學家花了幾十年研究這些冪律的數學結構，發展出一整套分析工具。

Kaplan 把這套工具帶進了 AI。他做的事情，本質上和一個物理學家在實驗室裡測量相變行為沒什麼不同：改變一個參數、測量結果、畫圖、找規律。只是他的「實驗室」是 OpenAI 的 GPU 叢集，他的「實驗」是訓練數百個不同大小的語言模型。

這個物理學的視角至關重要。在 Kaplan 之前，AI 研究者通常從工程的角度看問題：我有這些資源，怎麼訓練出最好的模型？Kaplan 的問題不一樣。他問的是：這個系統的基本行為法則是什麼？就像牛頓不是問「這顆蘋果會不會掉下來」，而是問「所有物體之間的引力遵循什麼數學關係」。

## 三條冪律線：Scaling Laws 的核心發現

論文的核心發現可以濃縮成三條在對數座標上近乎筆直的線。

第一條線描述的是模型參數量（N）和表現的關係。當你固定訓練資料量和計算預算，只增加模型的參數量，模型的交叉熵損失（cross-entropy loss，一種衡量語言模型預測準確度的指標）會以一種非常規律的方式下降。在對數座標上，這個關係幾乎是一條直線，斜率大約是 -0.076。翻譯成人話就是：每當你把模型大小乘以十倍，損失函數下降一個固定的幅度。

第二條線描述的是訓練資料量（D）和表現的關係。固定模型大小，增加訓練資料——從數億個 token 到數百億個 token——損失同樣以冪律方式下降，斜率大約是 -0.095。

第三條線描述的是計算量（C）和表現的關係。固定其他條件，增加用於訓練的浮點運算次數（FLOPs），損失再次以冪律下降，斜率大約是 -0.050。

這三個冪律關係各自獨立成立，而且在他們測試的所有範圍內——跨越了好幾個數量級——都沒有明顯的彎折或飽和跡象。換句話說，在他們能測到的範圍裡，越大就是越好，而且好的幅度是可以精確預測的。

但真正讓人興奮的不只是這三條線的存在，而是它們帶來的一個實用結論。Kaplan 的團隊發現，當你有固定的計算預算時，最佳策略是把大部分資源投入讓模型變大，而不是讓模型在更多數據上訓練更久。他們的經驗法則是：模型大小應該「盡可能大」，然後用「相對少量但足夠」的數據來訓練。具體來說，他們建議計算預算增加十倍時，模型參數量應該增加大約 5.5 倍，而訓練數據量只需增加大約 1.8 倍。

這個結論直接影響了接下來幾年 AI 產業的資源配置邏輯。

## 一張改變產業邏輯的圖表

在這篇論文之前，AI 產業在資源配置上基本上是摸著石頭過河。你有一千萬美元的算力預算，該訓練一個大模型跑少一點的步數，還是一個小一點的模型跑多一點的步數？該收集更多訓練資料，還是把錢花在租更多 GPU 上？每個團隊根據自己的經驗做判斷，答案南轅北轍。

Scaling Laws 把這個問題從「靠直覺」變成了「算數學」。你可以拿出計算器，輸入你的預算，然後得到一個相當精確的預測：用這些資源能訓練出多好的模型。更重要的是，你可以反過來推算：如果我想讓模型表現提升到某個水準，需要投入多少資源。

這對投資決策的衝擊非常直接。當 OpenAI 去找微軟要十億美元的投資時，他們不再只是說「我們相信更大的模型會更聰明」。他們可以攤開一張圖表，指著那條冪律曲線說：「根據我們的數學模型，用這些資源我們可以訓練一個比 GPT-2 大一百倍的模型，它的表現會提升到這個水準。」這不是信仰，這是工程計算。

GPT-3 在 2020 年 6 月發表，它的規模——1,750 億個參數——幾乎可以確定是受到了 Scaling Laws 的指導。論文發表於 1 月，GPT-3 發表於 6 月，這五個月裡 OpenAI 把 Kaplan 的理論變成了實踐。GPT-3 的表現確實大致落在冪律曲線的預測範圍內，這是對整個 Scaling Laws 框架最壯觀的驗證。

從那之後，Scaling Laws 成了 AI 產業的通用語言。每一個新的大型語言模型的訓練計畫，都會先用 Scaling Laws 來估算預期表現和資源需求。每一輪投資談判，都會引用冪律曲線來論證投資回報。這篇論文不只是改變了研究方向，它改變了整個產業分配資源的邏輯。

## 但他們錯了一件重要的事

然而，Kaplan 的論文有一個關鍵結論後來被證明是錯的。

還記得他們說「計算預算應該主要用來加大模型，而不是加多數據」嗎？兩年後，Google DeepMind 的一群研究者用一篇叫做《Training Compute-Optimal Large Language Models》的論文（圈內人稱之為「Chinchilla 論文」）直接挑戰了這個結論。

Chinchilla 團隊做了一件更系統的事。他們訓練了超過 400 個語言模型，涵蓋了從七千萬到一百六十億個參數的範圍，每個模型用一到四組不同大小的數據集來訓練。他們的結論讓很多人嚇了一跳：模型大小和訓練數據量應該以大致相同的比例擴增。具體來說，每個模型參數大約需要 20 個訓練 token。

這意味著什麼？如果你有一個 700 億參數的模型，按照 Chinchilla 的建議，你需要大約 1.4 兆個 token 的訓練數據。而當時很多大型語言模型，包括 DeepMind 自己的 Gopher（2,800 億參數但只用了 3,000 億 token 訓練），都嚴重「吃不飽」——參數太多、數據太少。Chinchilla 用 700 億參數和 1.4 兆 token 的搭配，打敗了比它大四倍的 Gopher。

為什麼 Kaplan 團隊會得出偏向「模型優先」的結論？一個可能的原因是他們的實驗範圍不夠大。他們測試的最大模型只有十幾億參數，這在 2020 年已經算大，但跟 GPT-3 的千億參數相比微不足道。在較小的範圍裡，加大模型確實比加多數據更有效率，但當你把尺度拉大，數據的重要性就追上來了。這是一個在物理學裡很常見的現象：一個看似穩固的規律，在你測量的尺度改變之後可能需要修正。

Chinchilla 的修正對產業的影響同樣巨大。它讓整個 AI 社群意識到，光是堆模型大小不夠，你需要同步增加高品質的訓練數據。這直接催生了對訓練數據的「軍備競賽」：Common Crawl 的清洗和篩選技術、合成數據的研究、以及後來關於數據版權的法律爭議，都可以追溯到 Chinchilla 揭示的這個「數據不足」問題。

## 從 Kaplan 到 Chinchilla 再到 2026：Scaling Laws 的演進

如果把 Scaling Laws 的思想史畫成一條線，它的演進大致是這樣的。

2009 年，Google 的三位研究者在〈數據的不合理有效性〉中提出了最初的直覺：數據比演算法重要。2012 年，AlexNet 用實際結果驗證了這個直覺：深度神經網路加上大規模數據，可以碾壓精心設計的傳統方法。2019 年，薩頓的〈苦澀的教訓〉把這個觀察推到了哲學高度：算力和通用方法才是 AI 進步的根本驅動力。

然後 Kaplan 在 2020 年做了關鍵的一步：他把所有這些定性觀察量化了。「越大越好」不再是一句口號，而是一個帶有具體指數和係數的數學方程式。你可以用它來做預算、做計劃、做投資決策。Chinchilla 在 2022 年修正了方程式中的係數，但沒有否定框架本身。

到了 2024 年和 2025 年，Scaling Laws 遇到了新的挑戰。有研究者注意到，在某些基準測試上，光靠增加預訓練的規模，改善幅度開始趨緩。高品質文本數據被認為可能在可預見的未來耗盡。OpenAI 的 Ilya Sutskever 在離開公司時甚至暗示，「scaling 的時代」可能正在結束。

但故事沒有在這裡停下來。2024 年底，OpenAI 推出 o1 系列模型，開啟了所謂「推理時計算」（test-time compute）的新方向。過去的 Scaling Laws 關注的是訓練階段：投入更多資源去訓練，得到更好的模型。推理時計算則是讓模型在使用階段投入更多計算：遇到難題時，讓它花更多時間「思考」，嘗試不同解法，自我驗證。研究者很快發現，推理時計算同樣遵循某種形式的冪律關係。算力的戰場從訓練轉移到了推理，但 Scaling Laws 的基本精神——投入更多計算資源就能得到更好的結果——依然成立。

到了 2026 年 2 月，AI 領域的共識大致是：Scaling Laws 沒有「失效」，但它的形態正在演變。原始的、單純靠加大模型和增加訓練數據的路線，確實在遇到瓶頸。但如果把 Scaling Laws 理解為一個更廣義的概念——系統性地增加計算資源可以帶來可預測的表現提升——那它在推理時計算、合成數據生成、多模態模型等新方向上都還在發揮作用。

## 物理學的遺產

站在 2026 年回望，Kaplan 這篇論文最深遠的影響可能不是那幾個具體的冪律指數——那些數字後來確實被修正了。它最深遠的影響是一種思維方式的轉變。

在 Kaplan 之前，AI 研究更像手藝：大量的直覺、經驗、反覆試錯。在 Kaplan 之後，AI 的一部分開始像物理學：測量、建模、預測、驗證。你不需要真正理解為什麼神經網路會遵循冪律（到今天也沒人完全理解），你只需要知道它確實遵循，然後用這個知識來做決策。

這跟我們在上一篇聊到的〈苦澀的教訓〉形成了一個有趣的對比。薩頓用七十年的 AI 歷史告訴我們一個哲學層面的教訓：別迷信人類巧思，要相信算力。Kaplan 則用數學告訴我們一個工程層面的工具：你可以精確計算需要多少算力。苦澀的教訓說「往那個方向走」，Scaling Laws 說「走那個方向要帶多少糧草」。

但這個工具有一個前提：冪律曲線會繼續成立。就像古埃及人能精確預測太陽的軌跡但不知道為什麼，我們能精確預測模型的表現但不知道冪律的底層原因。如果某天這條曲線突然彎折了，我們甚至不知道該怎麼解釋。

這就是 Scaling Laws 最迷人、也最令人不安的地方。它給了 AI 產業前所未有的信心，讓數千億美元的投資有了數學基礎。但這個基礎本身是一個我們精確測量卻不真正理解的經驗規律。Kaplan 用物理學家的眼睛看見了這條規律，他的物理學訓練也讓他很清楚：一個沒有理論解釋的經驗規律，隨時可能在你沒預料到的尺度上失效。

這篇論文和整個 Scaling Laws 故事後續的人物線——Kaplan 和 Dario Amodei 一起離開 OpenAI 創辦 Anthropic、GPT-3 論文的共同第一作者 Tom Brown 也加入 Anthropic、OpenAI 內部圍繞 scaling 路線的路線之爭——我們在下一篇會詳細展開。
