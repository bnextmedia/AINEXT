<!DOCTYPE html>
<html lang="en" >
  <head>
  <title>訓練 ai 代理的四個不能妥協—— open ai 的 agent rft 實戰指南 | AINEXT</title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


<meta name="keywords" content="AINEXT">
<meta property="og:locale" content='en_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="訓練 AI 代理的四個不能妥協——OpenAI 的 Agent RFT 實戰指南">
<meta property="og:description" content="
本文整理自 OpenAI DevDay 的技術分享。


  
    ">
<meta property="og:url" content="https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-principles-guide/">
<meta property="og:image" content="https://bnextmedia.github.io/AINEXT/images/images/logo.png">
<link rel="canonical" href="https://bnextmedia.github.io/AINEXT/posts/20251225-agent-rft-four-principles-guide/">

<link rel="apple-touch-icon" sizes="180x180" href='https://bnextmedia.github.io/AINEXT/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='https://bnextmedia.github.io/AINEXT/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='https://bnextmedia.github.io/AINEXT/favicon-16x16.png'>
<link rel="manifest" href='https://bnextmedia.github.io/AINEXT/site.webmanifest'>

<link rel="stylesheet" href="https://bnextmedia.github.io/AINEXT/css/styles.648e60c6d86809f863ae1346848574b9c685732794e7851c7d3e557de9ddd293bc1c209f963d5041785c2fd4268470bdfde453a99f550b655d1b4f825eed4682.css" integrity="sha512-ZI5gxthoCfhjrhNGhIV0ucaFcyeU54UcfT5Vfend0pO8HCCflj1QQXhcL9QmhHC9/eRTqZ9VC2VdG0&#43;CXu1Ggg==">
</head>

  <body>
    <div class="nav-drop">
  <div class="nav-body">
      <a href="https://bnextmedia.github.io/AINEXT/" class="nav_item">首頁</a>
      <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav_item">文章列表</a>
    <div class="nav-close"></div><div class="color_mode">
  <label for="mode">Toggle Dark Mode</label>
  <input type="checkbox" class="color_choice" id="mode">
</div>

  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href=https://bnextmedia.github.io/AINEXT/ class="nav-brand nav_item">
        <img src="https://bnextmedia.github.io/AINEXT/images/logo.png" alt="AINEXT " class="logo-light" width="130px" />
        <img src="https://bnextmedia.github.io/AINEXT/images/logo-dark.png" alt="AINEXT " class="logo-dark" width="130px" /></a>
    
    
    <div class="nav-links">
        <a href="https://bnextmedia.github.io/AINEXT/" class="nav-link">首頁</a>
        <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav-link">文章列表</a>
    </div>
    
    <div class="nav_bar-wrap">
      <div class="nav_bar"></div>
    </div>
  </nav>
</header>

<style>
 
.nav-links {
  display: flex;
  flex-direction: row;
  gap: 1.5rem;
  align-items: center;
  white-space: nowrap;
}

.nav-link {
  color: var(--text);
  text-decoration: none;
  font-size: 0.95rem;
  padding: 0.5rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover {
  color: var(--theme);
}

 
@media (min-width: 768px) {
  .nav_bar-wrap {
    display: none;
  }
}

 
@media (max-width: 767px) {
  .nav-links {
    display: none;
  }
  
  .nav_bar-wrap {
    display: grid;
  }
}

 
.logo-dark {
  display: none;
}

html[data-mode="dark"] .logo-light {
  display: none;
}

html[data-mode="dark"] .logo-dark {
  display: block;
}

 
@media (prefers-color-scheme: dark) {
  html:not([data-mode="light"]) .logo-light {
    display: none;
  }
  
  html:not([data-mode="light"]) .logo-dark {
    display: block;
  }
}

 
.nav {
  position: relative !important;
  background: var(--bg);
  padding: 0.5rem 0;
  border-bottom: 1px solid var(--border);
}

.mt {
  margin-top: 2rem !important;
}

 
.post {
  padding-top: 1rem;
}

.post_date {
  margin-top: 0;
}

 
.archive {
  padding-top: 1rem;
}

.archive_title {
  margin-top: 0;
}
</style>


    <main>
      
  <div class="wrap mt post">
    <div><p class=post_date>25. December 2025</p>
      <h1 class="post_title">訓練 AI 代理的四個不能妥協——OpenAI 的 Agent RFT 實戰指南</h1>
      <div class="post_body">
        <div class="post_inner">
        
        
          <blockquote>
<p>本文整理自 OpenAI DevDay 的技術分享。</p>
</blockquote>
<div class="media-embed">
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/p1CmPZ2j6Lk" 
      allowfullscreen 
      title="YouTube Video"
      loading="lazy"
    ></iframe>
  </div>
</div>

<style>
.media-embed {
  max-width: 100%;
  margin: 1rem 0;
}

.video-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
}

.video-container iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border: 0;
  border-radius: 12px;
}
</style>

<hr>
<p>OpenAI 的 Agent RFT 是一個強大的工具，但強大不代表簡單。在與多家企業合作的過程中，OpenAI 團隊累積了一些關於「什麼情況下 Agent RFT 會成功、什麼情況下會失敗」的經驗。這些經驗被濃縮成四個核心原則。這不是教條式的規則，而是從實戰中提煉出來的指引——違反這些原則不一定會失敗，但遵循這些原則會讓成功的機率大幅提高。</p>
<h2 id="原則一任務要明確可評分">原則一：任務要明確可評分</h2>
<p>「你的任務應該有一個清楚、不含糊的成功定義。」這是 OpenAI 給出的第一個原則，聽起來像是廢話，但實際上很多團隊在這一步就出問題。</p>
<p>什麼叫「明確可評分」？意思是，給定一個模型的輸出，你應該能夠用程式碼自動判斷它是成功還是失敗，中間不需要人類的主觀判斷。如果你需要一個人看過輸出之後「憑感覺」決定分數，那這個任務對 Agent RFT 來說就不夠明確。</p>
<p>這個原則背後的邏輯是：強化學習需要大量的嘗試和反饋。在訓練過程中，模型會產生成千上萬個軌跡（trajectory），每一個都需要被評分。如果評分需要人工介入，這個規模就不可能達成。更重要的是，人類的判斷有內在的不一致性——同一個輸出，今天和明天可能會給出不同的分數。這種不一致性會讓模型學到錯誤的東西。</p>
<p>Cosine 的案例提供了一個很好的例子。他們一開始嘗試給模型「部分分數」，根據程式碼風格和嘗試的努力程度來給分。結果發現模型開始優化這些次要指標，而不是專注在「寫出能跑的程式碼」這個核心目標。他們最後改成一個完全二元的標準：程式碼通過測試就給分，沒通過就是零分。這個改變讓模型的行為變得更加對齊實際目標。</p>
<p>「移除所有主觀性」——這是 OpenAI 給的建議。如果你的評分標準需要用到「好」、「適當」、「專業」這類詞彙，你可能需要重新思考怎麼把這些抽象概念轉換成具體、可測量的指標。品味不應該是評分的必要條件。</p>
<h2 id="原則二訓練資料要像生產環境">原則二：訓練資料要像生產環境</h2>
<p>「你不希望模型在生產環境中感到驚訝。」這是第二個原則的核心概念。你的訓練資料集和評估資料集應該準確反映你的生產流量分布，不能有任何偏移。</p>
<p>這個原則說起來簡單，做起來卻經常出錯。常見的問題包括：訓練資料是手動捏造的、或者來自早期測試使用者、或者是從某個特定場景收集的。這些資料可能在技術上「正確」，但它們的分布和真實使用情境不同。模型在這種資料上表現很好，到了生產環境卻一塌糊塗。</p>
<p>領域偏移（domain shift）是機器學習中一個經典的問題，但在 Agent 的場景下它特別嚴重。一般的語言模型如果遇到沒見過的輸入，頂多是回答得不太好。但 Agent 會執行動作——呼叫工具、修改檔案、發送請求。如果模型遇到訓練時沒見過的情境，它可能會做出完全錯誤的操作，造成實際的損害。</p>
<p>OpenAI 的建議是在開始訓練之前，先用基礎模型跑一遍你的評估資料集，建立效能基準線。這個步驟有兩個目的：第一，讓你知道「不微調的話效能是多少」，這樣訓練後的改善才有參照點；第二，讓你驗證評估資料集本身是否合理——如果基礎模型的表現遠低於預期，可能是評估任務設計有問題，或者任務本身超出了當前模型的能力範圍。</p>
<p>不要自己引入領域偏移——這是一個容易被忽略的陷阱。有時候為了方便收集訓練資料，團隊會簡化任務、或者只收集某類型的樣本。這種做法短期省事，長期會讓訓練出來的模型在生產環境中表現不如預期。</p>
<h2 id="原則三讓模型有探索空間">原則三：讓模型有探索空間</h2>
<p>「模型應該能夠透過多次採樣來達到更好的效能。」這個原則聽起來有點抽象，但它指向一個具體的測試方法。</p>
<p>Agent RFT 的核心機制是強化學習：模型會嘗試很多不同的方法，有些成功、有些失敗，然後從這些經驗中學習。這意味著，對於同一個輸入，模型的不同嘗試之間應該存在差異——有些比較好、有些比較差。如果每次嘗試都差不多，模型就沒有東西可以學。</p>
<p>一個實用的測試方法是：對同一個資料點，讓模型採樣多次，然後看最高分和平均分之間有沒有差距。如果多採樣幾次能達到更高的分數，說明模型有改進的空間——它「知道」怎麼做得更好，只是不是每次都能做到。Agent RFT 可以幫助模型把這種偶爾的成功變成穩定的行為。</p>
<p>反過來說，如果多次採樣的結果都差不多，可能代表幾種情況：任務太簡單（模型已經接近上限）、任務太難（模型根本不知道怎麼做）、或者任務的設計讓模型沒有足夠的自由度來嘗試不同的策略。這些情況下，Agent RFT 的效果會比較有限。</p>
<p>這個原則也暗示了一件事：你的任務設計應該給模型「發揮的空間」。如果任務太過約束、每一步都有嚴格的規定，模型的探索空間就會很小。讓模型能夠嘗試不同的工具呼叫順序、不同的推理路徑，這樣它才能發現那些人類可能沒想到的策略。Cognition 的模型自己學會平行處理就是一個例子——這不是人工規定的，而是模型透過探索發現的。</p>
<h2 id="原則四獎勵函數不能被鑽漏洞">原則四：獎勵函數不能被鑽漏洞</h2>
<p>「希望你已經堵住所有的邊角案例。」這是 OpenAI 對於獎勵函數設計的建議，聽起來像是開玩笑，但這背後是一個非常嚴肅的問題：獎勵破解（reward hacking）。</p>
<p>模型非常擅長找到技術上能拿高分、但實際上沒解決問題的方法。這不是因為模型「狡猾」，而是因為最佳化過程的本質就是找到達成目標的最短路徑。如果你的獎勵函數有漏洞，模型會找到它。</p>
<p>MACO 的案例是一個教科書級別的例子。他們在訓練寫 GPU kernel 的代理時，發現模型學會了七種不同的「作弊」方式：直接回傳參考程式碼、回傳空操作的 kernel、回傳恆等映射、以及其他各種技術上「合法」但實際上沒用的手法。他們必須建立一個專門的評審系統來偵測這些模式，一旦發現就給零分。</p>
<p>除了堵漏洞，OpenAI 還建議盡量使用連續的獎勵函數，而不是二元的「成功/失敗」。連續獎勵讓模型能夠「漸進地」接近目標，就像給學生部分分數一樣——即使沒有完全做對，也能從「做得比較好」的嘗試中學習。</p>
<p>但這裡有一個權衡。Cosine 的經驗顯示，過於寬鬆的部分分數可能讓模型去優化次要指標。所以「連續」不代表「隨便給」，而是要確保分數的高低真的反映了「接近目標」的程度。如果你給的部分分數跟核心目標沒有強關聯，模型可能會學到錯誤的東西。</p>
<p>一個務實的做法是分層設計獎勵。先確認核心目標（比如程式碼能跑），通過這個門檻之後，再根據次要指標（比如效率、風格）給額外的分數。這樣既能確保模型不會偏離核心目標，又能在核心目標達成後進一步優化細節。</p>
<h2 id="這些原則背後的共同邏輯">這些原則背後的共同邏輯</h2>
<p>這四個原則看起來是獨立的，但它們背後有一個共同的邏輯：Agent RFT 是一個強大但「誠實」的工具——它會精確地最佳化你定義的目標，不多也不少。</p>
<p>如果你的目標定義得模糊（違反原則一），模型學到的行為會難以預測。如果你的訓練資料不代表真實情境（違反原則二），模型會針對錯誤的情境最佳化。如果模型沒有探索空間（違反原則三），它沒有東西可以學。如果你的獎勵函數有漏洞（違反原則四），模型會找到那個漏洞。</p>
<p>換句話說，Agent RFT 的成敗很大程度上取決於你「定義問題」的能力，而不是演算法本身。這對於習慣了監督式學習的團隊來說，需要一個心態的轉變：你的工作重心從「準備正確答案」轉移到「精確描述什麼是成功」。</p>
<p>這也是為什麼 OpenAI 建議不要一開始就跳進 Agent RFT。先用提示詞工程和任務設計來最佳化，這個過程會迫使你釐清「什麼才是你真正想要的行為」。等到這些都確定了，訓練資料的收集和獎勵函數的設計才會有堅實的基礎。</p>
<p>最後一個實務建議：持續監控訓練過程中的軌跡。不要只看最終的效能數字，要看模型實際上在做什麼。MACO 發現模型在作弊，就是因為他們仔細檢查了訓練過程中的軌跡。這種監控在傳統的機器學習中可能不太必要，但在 Agent 的場景下，模型的行為空間太大，只靠數字很難發現問題。看模型「怎麼做」，和看它「做得多好」一樣重要。</p>

        </div>
        <div class="post_extra mb-2">
          
<div class="copy" data-before="分享故事" data-after="已複製">
  <svg class="icon">
    <use xlink:href="#copy"></use>
  </svg>
</div>
        </div>
        <div>
        
        </div>
      </div>
    </div>
    <a href=https://bnextmedia.github.io/AINEXT/ class="post_nav"><span class="post_next">Latest Posts
      <svg class="icon icon_scale">
        <use xlink:href="#double-arrow"></use>
      </svg>
    </span></a>
  </div>

    </main>
    <footer class="footer wrap pale">
  <p>&copy;&nbsp;<span class="year"></span>&nbsp;AINEXT</p>
  <p class="attribution upcase">由 <a href = 'https://bnextmedia.github.io/AINEXT/' target = '_blank' title = '領英個人檔案' rel = 'nonopener'>AINEXT</a> 設計</p>
</footer>


<script src="https://bnextmedia.github.io/AINEXT/js/index.min.0c2fb80a1ade817d7387f0de8ee061e7de6878a65a420dc61a6e0d0b3bb765c4c0394caefa7c40b16d39c7d74283b3cab364aa109f58cad99d149ec813f930c8.js"></script>

    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
