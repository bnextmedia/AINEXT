---
title: "陳天奇：XGBoost、TVM 背後的男人，一個關於失敗、初心與長期主義的故事"
date: 2026-01-19T10:00:00+08:00
description: "XGBoost、TVM、MXNet——這些改變機器學習產業的開源工具，背後都有同一個名字：陳天奇。本文整理自 WhynotTV Podcast 深度訪談，看這位從浙江縣城走出來的工程師，如何用二十年時間，在失敗中學會接受失敗，在成功後努力保持初心。"
tags: ["陳天奇", "XGBoost", "TVM", "MXNet", "機器學習系統", "開源", "長期主義", "OctoML", "NVIDIA", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=jvqsvbntEFQ"
source_name: "WhynotTV Podcast"
draft: false
---

> 本文整理自 WhynotTV Podcast 2025 年 9 月播出的深度訪談。

{{< youtube jvqsvbntEFQ >}}

{{< spotify "episode/69NV0wbQ4Sbenfv39ivoB5" >}}

---

如果你曾經參加過 Kaggle 資料科學競賽，你大概用過 XGBoost——那個「幾乎什麼問題都能用、效果都很好」的神奇工具。如果你是 AI 工程師，你可能聽過 TVM，一個讓深度學習模型能跑在各種晶片上的編譯器。

這些工具的背後，都有一個共同的名字：**陳天奇**。

2024 年底，陳天奇共同創辦的 OctoML 被 NVIDIA 收購。從浙江省小縣城的高中生，到 CMU 教授，再到被全球最大 AI 晶片公司收購的創業者——這段二十年的旅程，不只是技術天才的成功故事，更是一堂關於「如何選擇重要問題」與「如何面對失敗」的課。

## 一、縣城少年的起點：沒有教練，只有網路

陳天奇的計算機之路，始於浙江省松陽縣的一所普通高中。

「我的高中是在一個小縣城的縣中，而且它現在不是那個縣最好的中學。」他在訪談中笑著說。當時學校沒有資訊競賽教練，他只能靠網路上的 Online Judge 平台和論壇自學 C 語言。

高二暑假，出於純粹的興趣，他決定挑戰一個「聽起來很酷」的東西——寫一個編譯器。那時候沒有 GitHub，沒有現成教程，他就在小本本上畫模組圖，然後一個暑假埋頭苦幹，真的寫出了一個能把 Pascal 轉成 C 的轉譯器。

「什麼東西都沒有現成的教程，自己在那邊，架構到寫，一開始 Windows，後來用了 Linux，自己管理這個項目。」

這段「出生牛犢不怕虎」的經歷，成為他日後敢於挑戰未知的底氣。

高三那年，他拿下省賽一等獎，考進上海交通大學。但和許多競賽保送生不同，他是靠高考考進去的——在自學編程、打競賽、寫編譯器的同時，他的高考成績依然是全年級頂尖。

## 二、交大 ACM 班：系統能力的根基

2006 年，陳天奇進入上海交大 ACM 班。

這是中國計算機教育的頂尖搖籃之一。但真正讓他印象深刻的，不是課堂教學，而是學長們自發設計、一代傳一代的「編譯原理大作業」——從零開始搭一個完整的編譯器。

「那個訓練放到現在北美高校的標準來看，我覺得都是一流的。」

ACM 班的創辦人俞勇老師有一句話，深深影響了陳天奇：**「低調做人，高調做事。」**

這句話的意思是：做事要有野心、要追求卓越，但同時要謙虛，知道成功從來不是一個人的功勞。

「很多時候你之所以能成一件事情，它的因素幾乎不全部來自於你自己。我接觸了很多人，他們都非常謙虛，知道你需要周圍的人一起合作完成一件事情。」

## 三、早期挫敗：用錯誤方法解決正確問題

本科畢業後，陳天奇留在交大讀研究生。這時候，深度學習的曙光剛剛出現。

他和實驗室夥伴看到了一個重要的問題——ImageNet 圖像分類。同時，他們相信一個當時很前沿的方法：限制玻爾茲曼機（Restricted Boltzmann Machine）。

「這個問題很重要，這個方法很前沿，我們就把兩個東西綁在一起做。」

他們甚至自己動手改裝硬體。當時實驗室的伺服器沒有 GPU，他們買了顯示卡卻發現機箱電源不夠，最後的解決方案是：**把電源放在機箱外面，用線接回去。**

就這樣，陳天奇花了整整兩年時間，手寫 CUDA 代碼，搭建深度學習系統。結果呢？

「最後什麼論文都沒有。」

兩年後，AlexNet 橫空出世，證明深度學習是對的——但用的是卷積神經網路，不是限制玻爾茲曼機。

**他挑對了問題，卻用錯了方法。**

這段失敗的經歷，成為他後來最重要的教訓之一：

> 「做科研的時候，如果你同時鎖定問題和方法，就很危險。你很難一次就挑對正確的問題和正確的方法。」

但他也說，正是這段失敗，讓他學會了接受失敗：「我做了兩年一件事情，最後 turns out 不成功，但我還是活得好好的。這讓我後來更敢放手去做事情。」

## 四、XGBoost：極致、社區、專注

2013 年，陳天奇來到華盛頓大學讀博士。

他的指導教授 Carlos Guestrin 問了一個有趣的問題：深度學習為什麼有效？是因為神經網路本身，還是因為「大數據 + 大模型」？

陳天奇決定用自己熟悉的樹模型來驗證這個假設。這就是 XGBoost 的起源。

XGBoost 是一種「梯度提升」演算法。用白話說，它就像是讓很多「弱小的判斷」組合起來，變成一個「強大的判斷」。想像你要預測房價，XGBoost 會先做一個粗略的猜測，然後看猜錯了多少，再做第二個猜測來修正錯誤，一直重複這個過程，直到預測越來越準。

為什麼 XGBoost 會成功？陳天奇總結了三個關鍵字：**極致、痛點、專注。**

「做一件事情要做到極致。」他想盡各種方法讓它跑得更快、更快、再更快。發布時，XGBoost 是世界上最快的梯度提升工具。

除了快，他還解決了一個真實世界的痛點。資料常常有缺失值——問卷調查裡，有人不填年齡、不填收入。傳統方法需要先「清理資料」，用平均值填補空缺。但陳天奇想：為什麼不讓模型自己學會處理缺失值？於是 XGBoost 內建了這個功能。

「Turns out 這個東西非常受歡迎，因為大家都不想自己去處理 missing value。」

最後是專注。「XGBoost 就做了一個演算法，把這一個演算法做好、做到極致。」他對比自己之前做過的專案：「我們之前做的 SVD Feature 是一個很大很全的工具，有二三十個設定選項。但對於小團隊來說，專注把一件事做好，才容易成功。」

今天，XGBoost 的論文被引用超過七萬次，在表格數據和時間序列分析上，依然是業界首選。

## 五、MXNet：團隊協作與用戶體驗的教訓

2015 年前後，深度學習框架百花齊放。陳天奇和一群來自不同學校的博士生——包括後來創辦「動手學深度學習」的李沐——決定合作做一個新框架：MXNet。

「我們當時做 MXNet 的時候，完全不擔心 TensorFlow 會對我們產生什麼競爭。」陳天奇回憶，「出生牛犢不怕虎，幹就是了。」

MXNet 有幾個領先的設計：Python 優先、自動調度、支援多卡並行。在很長一段時間裡，它在多 GPU 訓練上的效能是所有開源框架中最好的。後來 Amazon 選擇它作為官方深度學習框架。

但最終，MXNet 還是輸給了 PyTorch。為什麼？

「一定要把使用者體驗做到極致。」陳天奇坦承，「我們當時在使用者體驗和效能之間，覺得兩個都要。但其實應該先選擇使用者體驗。」

PyTorch 的設計讓使用者可以像寫普通 Python 程式一樣寫深度學習代碼，除錯非常直覺。相比之下，早期的 TensorFlow 和 MXNet 都需要先「定義計算圖」，再執行，除錯起來很痛苦。

這段經歷讓陳天奇學到一件事：**技術領先不代表產品成功，使用者體驗才是關鍵。**

## 六、TVM：在荒島上建城堡

MXNet 的經歷讓陳天奇看到另一個瓶頸：不管框架設計得多好，要支援各種不同的硬體（GPU、TPU、手機晶片），都需要大量的人力去手寫優化代碼。

「我去參加 GTC（NVIDIA 開發者大會）回來，發現還是要花這麼多工程力量去支撐這些東西。我就想：能不能讓這件事變得更簡單一點？」

這就是 TVM 的起源——一個「深度學習編譯器」。

用白話說，TVM 就像是一個「翻譯機」。你訓練好的 AI 模型是一種「語言」，不同的晶片（NVIDIA GPU、手機晶片、車用晶片）各自聽懂不同的「語言」。TVM 的工作就是自動把模型「翻譯」成各種晶片能高效執行的代碼。

這在當時是一個極度冒險的賭注。陳天奇形容它是「在荒島上建城堡」——沒有人知道這條路行不行得通。

從 2016 年 4 月開始規劃，到 2017 年 3 月第一個版本跑起來，整整 11 個月，他幾乎都在寫代碼。

「當時沒有想風險這個問題。當時只是覺得，這個東西我想做，而且它是解決機器學習瓶頸的關鍵，就去試試看。」

為了真正理解硬體，他甚至跨界設計了一套 NPU（神經網路處理器）的指令集。這讓他從「軟體工程師」變成了「軟硬體協同設計」的專家。

今天，TVM 已經成為機器學習編譯領域的標竿專案，被廣泛應用於各種邊緣裝置的 AI 部署。

## 七、創業與收購：OctoML 的商業洞察

2019 年，陳天奇和幾位夥伴基於 TVM 創辦了 OctoML。

「我加入 Octo 的原因是因為我希望開源社區能夠成功。」他說。這個動機和很多創業者不同——不是為了創業而做開源，而是為了讓開源更成功而創業。

OctoML 的商業模式經歷了幾次轉型。一開始是幫助企業把 AI 模型部署到各種硬體上；後來轉向提供大模型推論的 API 服務，和 Together AI、Fireworks 等公司競爭。

2024 年底，OctoML 被 NVIDIA 收購。

「在 NVIDIA 工作是一個不錯的機會。」陳天奇說，「NVIDIA 想要 grow on AI compiler 這個方向，而他們的產品線從資料中心 GPU 到邊緣運算都有，這個問題本身還是很大。」

創業五年，他學到最重要的一課是：

> 「所有東西你都必須要能夠 reinvent yourself。如果我們一直抱著原來的技術路線，繼續去優化 ResNet，最後我們會輸得很慘。」

他也更深刻理解了「技術以外的因素」：如何與團隊溝通、如何在現實約束下實現理想、如何找到產品市場契合點。

「這段經歷讓我更現實。我會知道怎麼樣能夠通過現實的方式，去更好地實現理想。」

## 八、長期主義：接受失敗，保持初心

訪談的最後，主持人問陳天奇：如果讓你對二十年前、十年前的自己說幾句話，你會說什麼？

他的回答出人意料：

> 「應該反過來。我會希望過去的我，對現在的我說——記住自己對自己的承諾，堅持自己的理想，往下走下去。」

不是現在的自己給過去建議，而是需要過去那個「出生牛犢不怕虎」的自己，來提醒現在的自己。

為什麼？

「很多時候，當你看到的東西多了之後，你會開始擔心。我們剛開始做 MXNet 的時候，完全不會擔心 TensorFlow 能對我們產生什麼競爭。但現在，有時候你會反覆問自己：這個專案到底能不能成功？」

他說，隨著資源和責任增加，保持初心反而變得更難：

「一旦你拿到資源，就代表著責任。你必須保證團隊可以成功，必須產出一些 impact 來 justify 你繼續做這件事。這就會產生一定的重量。但如果直接被這些東西裹挾而走，又不是一件好的狀態。」

那麼，什麼是他眼中的「成功」？

「在我看來，最重要的還是做問心無愧的事情。」

他舉了一個判斷標準：**「如果你選了這條路之後失敗了，你還會不會後悔？」**

如果不會後悔，那就做。

「很多時候你後悔的，不是有沒有獲得成功，而是這個東西是不是你理想中想要走的那條路。」

---

## 我的觀察

聽完這場近三小時的訪談，印象最深的不是那些響亮的專案名稱，而是陳天奇反覆提到的兩件事：**「接受失敗」**和**「保持初心」**。

聽起來像老生常談，但放在他的經歷裡，重量不一樣。

他不是沒有失敗過。研究生時期花兩年做深度學習，最後什麼論文都沒發。MXNet 一度領先，最後輸給 PyTorch。但這些失敗沒有讓他變得保守，反而讓他更敢冒險。

「我做了兩年一件事情，最後不成功，但我還是活得好好的。這讓我後來更敢放手去做事情。」

而「保持初心」的難，不在於年輕時沒有初心，而在於有了資源、有了成就之後，還能不能保持當年那種「出生牛犢不怕虎」的心態。

訪談最後，他說了一句話：

> 「接受失敗，並且能夠提醒未來的自己，繼續按照當時的初心努力，不要怕任何事情。」

聽起來像雞湯，但從一個真的失敗過、又真的爬起來過的人嘴裡說出來，重量不一樣。
