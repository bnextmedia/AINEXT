---
title: "從模仿到自己犯錯：Google 如何用一個模型拿下數學奧林匹亞金牌"
date: 2026-01-27T10:00:00+08:00
description: "Google DeepMind 新加坡團隊負責人 Yi Tay 分享 Gemini 拿下國際數學奧林匹亞金牌的幕後故事。他們放棄了原本表現不錯的專用系統，賭上一切用通用模型挑戰，背後是一個關於 AI 訓練方法的根本轉變。"
tags: ["Yi Tay", "Google DeepMind", "Gemini", "IMO", "強化學習", "Podcast"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=unUeI7e-iVs"
source_name: "Latent Space Podcast"
draft: false
---

> 本文整理自 Latent Space Podcast 2026 年 1 月播出的訪談。

{{< youtube unUeI7e-iVs >}}

{{< spotify "episode/0BQGYSkRTlGRj2wQKtaLj2" >}}

---

## 這個新加坡人，為什麼值得我們關注

Yi Tay 是少數在全球 AI 頂尖圈子佔有一席之地的東南亞研究者。他從新加坡南洋理工大學畢業後，2019 年加入 Google Brain，很快就成為 PaLM-2 的架構共同負責人，也發明了 UL2 預訓練方法，開創了「生成式檢索」這個研究領域。

2023 年，他做了一個大膽的決定：離開 Google 創業。他和幾位志同道合的研究者創辦了 Reka AI，用 20 人的團隊、6000 萬美金的資金，硬是做出了能和 GPT-4 競爭的多模態模型，一度衝到 LMSYS 排行榜第五名。

但一年半後，他又回到了 Google DeepMind。這次他沒有去矽谷，而是選擇在新加坡建立 Gemini 團隊，專注於推理和強化學習。2025 年，他帶領的團隊用 Gemini 模型拿下了國際數學奧林匹亞金牌。

這集 Podcast 是他回歸 Google 後第二次接受 Latent Space 專訪，聊的不只是技術突破，更是一個關於「模型該怎麼學習」的根本思考。

## 讓模型從自己的錯誤中學習

訪談一開始，主持人就問 Yi Tay：現在大家都在談「推理能力」，但這個詞到底是什麼意思？

Yi Tay 的回答很直接：推理這個詞本身很模糊，不同人有不同定義。但如果要給一個技術上的定義，現在所謂的「推理」和「強化學習」幾乎是同義詞，指的是透過後訓練讓模型變得更會思考。

這裡有一個關鍵的區分：On-Policy 和 Off-Policy。

傳統的監督式微調（SFT）是 Off-Policy 的做法。你拿一個更大、更強的模型產生的答案，讓小模型去模仿。這就像是叫小孩照著教科書的標準答案抄寫，他可能會抄對，但不一定真的懂。

On-Policy 則完全不同。模型自己產生答案，然後根據某個驗證機制得到獎勵或懲罰，再從這個回饋中學習。這就像是讓小孩自己去嘗試，做錯了就告訴他哪裡錯，讓他從自己的錯誤中學習。

Yi Tay 用了一個很生動的比喻：「這其實更像人類的學習方式。我們在世界中走動、犯錯，然後意識到『喔，這樣不對』。模仿學習則是別人告訴你該怎麼做，你就照抄。」

他甚至把這個觀念延伸到教養小孩：「我現在有小孩了，我希望她自己去嘗試，然後我告訴她哪裡做對、哪裡做錯，而不是叫她複製別人的做法。」

主持人接話說，這聽起來很像蒙特梭利教育法——給孩子一個安全的環境，讓他們自己探索，而不是硬塞給他們標準答案。

## 放棄專用系統的豪賭

2024 年，Google DeepMind 用 AlphaProof 系統在國際數學奧林匹亞拿下銀牌，只差一分就是金牌。按照常理，隔年應該繼續優化這個系統，把那一分補回來。

但 Yi Tay 他們做了一個大膽的決定：放棄 AlphaProof，改用 Gemini 作為端對端的通用模型來挑戰。

為什麼？

「如果模型連數學奧林匹亞金牌都拿不到，我們怎麼可能達到 AGI？」Yi Tay 說，「總有一天，我們必須用這些通用模型去挑戰這些奧林匹亞競賽。我們的目標是一個模型做所有事，而不是為每個問題打造專用系統。」

這個邏輯很清楚：專用系統沒有盡頭。你可以做一個數學引擎、一個化學引擎、一個物理引擎，但這樣下去永遠做不完。真正的突破是讓一個通用模型具備處理所有問題的能力。

這裡有一個有趣的技術細節。過去大家認為，數學證明需要專門的符號系統來驗證，比如 Lean 這樣的形式化證明語言。但 Yi Tay 他們發現，這些驗證邏輯其實可以被「編碼」進模型的參數裡。

「以前 LLM 連計算機都當不好，現在可以了。所以技術上來說，計算機這個工具已經被編碼進模型的參數裡。」Yi Tay 說，「我們不知道極限在哪裡，但我們會一直推進這個邊界。」

## 跨時區的即時競賽

國際數學奧林匹亞不是一個你可以慢慢跑、慢慢調的基準測試。它是一場即時競賽，題目公布後就要開始解題。

Yi Tay 描述了當時的緊張狀態：團隊有四個「隊長」，兩個在倫敦、一個在山景城、他自己在新加坡。他們跨時區協作，沒有固定的工作流程，就是有人要上飛機了就交接給下一個人。

「有時候程式出 bug、任務當掉，就得有人去修。」他說，「這比跑基準測試刺激多了，因為這是真的即時比賽。」

更有趣的是，金牌的門檻不是固定分數，而是根據當年所有參賽者的表現來決定。所以 Yi Tay 發現自己竟然在關注人類參賽者的成績，因為 Gemini 能不能拿金牌，某種程度上取決於人類考得好不好。

最後結果揭曉時，團隊的反應不是驚訝，而是一種「終於」的感覺。「這代表我們整個團隊——所有在 Gemini 上工作的工程師和研究者——真的有很大的進步。」Yi Tay 說，「如果你把現在的 AI 能力拿給五年前的人看，他們大概會覺得我們已經達到某種形式的 AGI 了。」

## Pokemon 圖鑑：意想不到的 AI 測試場

訪談中有一段很有趣的討論：Pokemon 遊戲作為 AI 的測試場。

Yi Tay 是 Pokemon 的老玩家，他認為這是一個很好的長期規劃測試。但他提出了一個更有挑戰性的目標：不是破關，而是收集完整的圖鑑。

為什麼這更難？因為有些 Pokemon 必須透過交換才能進化。模型不只要會玩遊戲，還要懂得上網查資料、到論壇發文找人交換。這需要整合遊戲操作、網路搜尋、社交互動等多種能力。

「破關其實很線性，」Yi Tay 說，「但完成圖鑑需要大量的規劃、研究，還有一些如果不上網查就永遠不會知道的資訊。」

這個觀察很有意思：我們現在的 AI 能力評估，往往還停留在「能不能完成任務」的層次。但真正的智慧測試，可能是看 AI 能不能主動發現自己缺少什麼資訊，然後想辦法去取得。

---

## 我的觀察

### 「一個模型做所有事」vs 工具整合路線

Yi Tay 非常明確地站在「一個模型做所有事」這一邊。他認為專用系統沒有盡頭，真正的突破是讓通用模型把所有能力都學進去。

但產業界目前的主流做法似乎是另一條路：讓 AI 學會使用工具。MCP、Function Calling、Agent 架構，這些都是在教 AI 怎麼呼叫外部系統來完成任務。

這兩條路會走向哪裡？我認為短期內工具整合路線會更實用，因為它能快速解決現實問題。但長期來看，Yi Tay 的直覺可能是對的：如果工具的邏輯能被編碼進模型參數，那最終還是會走向一個通用模型。

這就像智慧型手機的演進。早期我們需要計算機、相機、MP3 播放器各種獨立裝置，後來全部整合進一支手機。AI 的發展軌跡，或許也會類似。

### 讓模型從錯誤中學習的人生隱喻

Yi Tay 用教養小孩來比喻 On-Policy 學習，這個比喻其實有很深的含義。

模仿學習的問題是：你學到的是別人的路徑，不是自己的路徑。那條路對別人有效，不代表對你也有效。更重要的是，你永遠學不會如何應對那些標準答案沒有涵蓋的情況。

On-Policy 學習則是讓你走自己的路，犯自己的錯，從自己的經驗中學習。這樣學到的東西更紮實，也更能應對未知的挑戰。

主持人提到的蒙特梭利教育法正是這個理念的實踐。有趣的是，機器學習的研究反過來印證了這種教育哲學的有效性。

這讓我想到一個問題：我們自己的學習，是 On-Policy 還是 Off-Policy？我們是在走自己的路、從自己的錯誤中學習，還是一直在模仿別人的成功路徑？

如果連 AI 都發現從自己的錯誤中學習更有效，或許我們也該重新思考自己的學習方式。
