---
title: "Ilya Sutskever 離開 OpenAI 後首次深度訪談：10 個關鍵洞見"
date: 2025-12-25T11:30:00+08:00
description: "OpenAI 共同創辦人 Ilya Sutskever 在離開後接受 Dwarkesh Patel 專訪，談論 AI 產業的典範轉移、模型泛化能力的根本問題、超級智慧的願景，以及他創辦 SSI 的真正目標。這是他離開 OpenAI 後最完整的一次公開發言。"
tags: ["Ilya Sutskever", "SSI", "OpenAI", "AGI", "超級智慧", "Podcast"]
categories: ["AI 產業動態"]
source_url: "https://www.youtube.com/watch?v=aR20FWCCjAs"
source_name: "Dwarkesh Podcast"
draft: false
---

> 本文整理自 Dwarkesh Podcast 2024 年 12 月播出的單集。

{{< youtube aR20FWCCjAs >}}

---

Ilya Sutskever 是深度學習革命的核心人物。從 2012 年的 AlexNet 到 GPT 系列，他參與了過去十年幾乎所有重要的 AI 突破。2024 年，他離開 OpenAI，創辦了專注於超級智慧的公司 SSI（Safe Superintelligence Inc.）。

這是他離開後首次接受深度訪談，時長超過一個半小時。以下是十個最重要的洞見。

---

## 1. 我們正從 Scaling 時代回到研究時代

「2012 到 2020 年是研究時代。2020 到 2025 年是 Scaling 時代。現在，我們又回到研究時代了——只是電腦變大了。」

Ilya 認為，過去幾年靠「更多資料、更大模型、更多算力」推動進步的公式正在失效。資料是有限的，而就算有 100 倍算力，也不會帶來質變。接下來的突破需要真正的研究創新，不是單純堆資源。

---

## 2. 現在的世界是「公司比想法多」

這句話很尖銳。Ilya 指出，因為 Scaling 的邏輯太強大，所有公司都在做差不多的事情。「Scaling 吸走了房間裡所有的空氣。」大家都在追同一個遊戲，導致差異化很小。

這也解釋了為什麼 AI 公司估值這麼高，但真正不同的東西很少。

---

## 3. 模型在評測上很強，經濟影響卻落後

這是 Ilya 認為最讓人困惑的現象。模型在各種 benchmark 上拿高分，但實際使用時會犯奇怪的錯誤，整體經濟影響也沒有評測分數暗示的那麼大。

他的解釋是：研究者不自覺地追著評測跑，RL 訓練讓模型變成「考試機器」，能力不見得能遷移到真實世界。

---

## 4. 人類的泛化能力遠超模型，這是根本問題

「這些模型的泛化能力比人類差太多了。這很明顯，也很根本。」

一個五歲小孩用極少資料就能發展出足以支撐駕駛任務的視覺能力。一個青少年十小時學會開車。人類不需要可驗證的獎勵信號就能學會複雜技能。模型做不到這些。

Ilya 認為這指向某種「更好的機器學習」，而他有想法但不能公開談。

---

## 5. 價值函數可能是關鍵缺失

Ilya 花了不少時間討論「價值函數」的概念。在強化學習中，價值函數告訴模型「這個狀態好不好」，讓它不用等到最終結果就能學習。

他認為人類的情感可能扮演類似角色。一個失去情感處理能力的人，認知功能看似正常，卻無法做決定。這暗示情感是我們內建的「價值函數」，幫助我們在不確定中導航。

目前的模型訓練很少使用價值函數。Ilya 預期這會改變。

---

## 6. Pre-training 的問題是「無法控制學到什麼」

Pre-training 的優點是你不用選資料——答案就是「全部」。但缺點是你很難理解模型到底學到了什麼，為什麼在某些情況下會失敗。

「很難理解模型依賴 pre-training 資料的方式。每當模型犯錯，你會想：是不是某些東西在 pre-training 資料中支援度不夠？」

這種不透明性是目前方法論的根本限制。

---

## 7. AGI 這個詞過時了，應該談「持續學習」

Ilya 認為「AGI」這個詞其實是對「狹義 AI」的反動。過去的 AI（西洋棋、圍棋）很窄，所以人們想要「通用」的 AI。

但人類也不是 AGI——我們不是生下來就會所有事情。我們靠的是持續學習。Ilya 設想的超級智慧更像「一個很會學習的十五歲，你讓他去當程式設計師、去當醫生，他會很快上手」，而不是一個已經會所有事情的系統。

這個定義對安全問題有重要影響：部署本身就會涉及學習和適應。

---

## 8. SSI 的算力其實沒有想像中劣勢

SSI 募了 30 億美元，看起來比 OpenAI 或 Anthropic 少很多。但 Ilya 算了一筆帳：

大公司的大量資金用於推論（inference）。他們需要龐大的工程團隊維護產品。很多研究資源用於產品功能開發。

「當你看真正用於研究的資源，差距比表面上小很多。」而且如果你做的是不同的事情，不需要最大規模來證明想法可行。

---

## 9. 讓 AI 「關心有感知的生命」可能比「只關心人類」更容易

這是一個有趣的技術觀點。Ilya 認為，如果 AI 本身有感知能力，讓它關心所有有感知的生命可能比只關心人類更自然。

他提到人類的鏡像神經元——我們用同一套迴路來理解自己和理解他人，這可能是同理心的來源。如果 AI 也用類似方式建模自己和他人，「關心有感知的生命」可能是自然的副產品。

---

## 10. 研究品味是什麼？「美感、簡潔、對大腦的正確理解」

訪談最後，主持人問 Ilya：你怎麼培養研究品味？

Ilya 的回答：「我有一種對 AI 應該如何的美學。我思考人類是怎樣的——但要正確地思考。」

他舉例：人工神經元的概念直接受大腦啟發，而且是對的。為什麼神經元重要？因為數量多。為什麼要有會改變連結的學習規則？因為大腦似乎這樣做。

「美感、簡潔、優雅、對大腦的正確啟發——這些要同時存在。它們存在得越多，你對一個自上而下的信念就越有信心。這種信念能在實驗結果不如預期時支撐你。」

---

## 我的觀察

這場訪談最讓我印象深刻的，是 Ilya 對目前主流方法的懷疑程度。他不是說現在的模型不會進步——會的。但他認為有一個天花板，而突破那個天花板需要的是對學習本質的更深理解，不是更多資源。

他反覆暗示有一些想法不能公開談。這本身就是資訊：他認為這些問題有具體的技術路徑，而不只是哲學討論。

如果 Ilya 是對的，接下來幾年 AI 產業的競爭邏輯可能會改變。不是誰有最多 GPU 誰贏，而是誰先找到那個「更好的機器學習」。這對小團隊來說可能是機會。

當然，他也可能是錯的。但當這樣一個人說「我們可能走錯路了」，至少值得認真聽。
