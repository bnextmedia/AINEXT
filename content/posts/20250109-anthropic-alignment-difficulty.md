---
title: "AI 對齊到底有多難？Anthropic 研究沙龍的三種思路交鋒"
date: 2025-01-09T10:00:00+08:00
description: "Anthropic 研究沙龍邀集四位不同團隊的研究者，從微調、可擴展監督、可解釋性三條路線辯論 AI 對齊的難度。討論涉及超級對齊問題、模型欺騙偵測、思維鏈的可信度，以及個體服從與人類整體利益的根本張力。"
tags: ["Anthropic", "AI Alignment", "AI Safety", "Interpretability", "Scalable Oversight"]
categories: ["AI 安全與治理"]
image: "/images/posts/20250109-anthropic-alignment-difficulty.webp"
source_url: "https://www.youtube.com/watch?v=fGKNUvivvnc"
source_name: "Anthropic YouTube"
related_companies: ["anthropic"]
related_people: []
draft: false
---

> 本文整理自 Anthropic 2025 年 1 月發布的研究沙龍影片。

{{< youtube fGKNUvivvnc >}}

---

你有沒有想過一個問題：當我們說「讓 AI 對齊人類的價值觀」，這句話到底是什麼意思？更進一步，這件事有多難？是像除蟲一樣，找到問題、修掉就好？還是像治療癌症，你以為搞定了，結果它換個地方復發？

Anthropic 的這場研究沙龍把四個不同團隊的研究者拉到一起，就著這個問題辯了將近半小時。四個人分別代表了社會影響（Societal Impacts）、對齊科學（Alignment Science）、對齊微調（Alignment Fine Tuning）、可解釋性（Interpretability）四條研究路線。主持人故意挑撥，讓他們互相質疑對方的方法論。結果呢，沒有吵起來，但聽完之後你會意識到一件事：這四個人看到的是同一隻大象的不同部位，而且每個人都很清楚自己摸到的那塊還遠遠不是全貌。

以下是完整的報導，加上我的分析。

---

## 四位研究者，四個角度

先介紹一下場上的人。阿曼達．阿斯凱爾（Amanda Askell）來自對齊微調團隊，她的工作簡單來說就是決定 Claude「應該怎麼表現」。在 Anthropic 內部，她有時候被開玩笑說是「哲學家國王」，因為她做的事情某種程度上是在替一個會跟上百萬人互動的 AI 定義道德標準。楊．萊克（Jan Leike）來自對齊科學團隊，專注於可擴展的對齊技術，他之前在 OpenAI 領導超級對齊（Superalignment）團隊，後來加入 Anthropic。第三位是喬許（Josh），來自可解釋性團隊，研究如何「拆開」模型的內部運作，看看裡面到底在想什麼。第四位是主持人，來自社會影響團隊，負責思考模型對社會的系統性影響。

這四個人各自負責的領域恰好代表了 AI 對齊的四條主要攻略路線。微調是最直覺的，直接調整模型的行為。對齊科學考慮的是當模型變得太強大、人類無法直接監督時該怎麼辦。可解釋性是從內部理解模型的運作機制。社會影響則跳出單一模型的視角，看整個系統層面的風險。把這四個人放在一起討論，可以說是 Anthropic 自己在做一場內部壓力測試。

---

## 阿曼達的起點：像一個有良知的人那樣行動

主持人開場就把最大的問題丟給了阿曼達：什麼是對齊？你怎麼定義它？

阿曼達的回答出乎意料地務實。她說，很多人花太多時間試圖完美定義「對齊」這個概念。他們會搬出社會選擇理論，討論如何最大化所有人的效用函數，在各種理論框架之間來回辯論。但她認為，目前更重要的不是定義，而是讓事情「好到可以繼續迭代」。

她的具體做法是：讓模型的行為接近一個「非常好的、有道德動機的、善良的人」在類似情境下會怎麼做。但她也指出了一個微妙之處，AI 的情境跟人類不一樣。一個人在私下跟朋友聊政治，可以暢所欲言。但如果你同時在跟上百萬人對話，你可能會對自己的言論更加謹慎。模型面對的就是後者的情境。

我覺得阿曼達最有意思的觀點是她對「倫理學」的態度。她說，倫理學其實比人們想像的更像物理學。什麼意思？物理學是經驗性的，我們有假說，然後透過實驗去驗證或推翻它。倫理學也應該是這樣。我們不確定哪套道德框架是最正確的，我們有各種假說，當我們碰到新的案例、新的直覺衝突時，我們會更新自己的看法。所以她不想把一套固定的價值觀「注入」模型裡，而是希望模型能像一個謙遜的道德思考者那樣，保持不確定性，隨時準備更新。

她甚至說了一句很直接的話：如果我遇到一個對自己的道德觀完全確信的人，不管那套道德觀是什麼，我都會害怕。相反，一個會說「我不太確定，讓我想想」的人，感覺安全得多。她希望模型也是這種性格。

---

## 楊的質疑：當你看不懂模型在做什麼

主持人接著轉向楊．萊克，很不客氣地問：「為什麼阿曼達的做法是完全錯的？」（當然是故意挑撥。）

楊的回答很有技巧。他先肯定了阿曼達的方法在當前模型上非常實用。問題不在於現在，問題在於未來。他把核心困境說得很清楚：阿曼達的工作流程是讀大量的模型對話記錄，判斷模型的行為是否合適，然後根據判斷來調整。這在模型做的事情是「回答問題」「寫文章」這種人類可以直接理解的任務時，完全行得通。但如果模型是一個自主 Agent，在執行一段非常複雜的生物研究、或者在做你看不懂的長程決策時，你要怎麼判斷它的行為是不是道德的？

楊把這叫做「超級對齊問題」（Super Alignment Problem）：如何讓對齊技術的有效性跟上模型能力的成長？如果我們能看懂模型的輸出，用 RLHF（Reinforcement Learning from Human Feedback）就夠了。如果模型的行為超出了人類理解的範圍，那我們的「憲法 AI」（Constitutional AI）裡的憲法到底還能不能正確引導它？

我認為楊點出的問題是這整場討論的核心張力。所有做對齊工作的人都面對同一個天花板：你的監督能力必須跟上你的模型能力。一旦模型的能力超過了你的監督能力，你就只能相信它在做正確的事，但你無法驗證。這不是一個工程問題，這是一個結構性問題。

---

## 阿曼達的反擊：先有一個好的基座模型

阿曼達的回應很有意思。她承認楊說的限制是真實的，但她提出了一個反方向的論點：如果你放棄在基座模型層面做好對齊，指望後面的技術來補救，那你從一開始就輸了。

她的邏輯是這樣的：當人類無法直接監督模型的時候，我們勢必要依賴模型自己來監督自己。比如，用一個模型去審查另一個模型的輸出。如果這個「審查員模型」本身的對齊就做得很差，你怎麼能相信它的審查結果？所以她的策略是：與其花所有資源去解決「模型超出人類監督範圍之後怎麼辦」，不如先確保基座模型盡可能地對齊。這樣即使模型變強了，至少出發點是好的。

楊馬上追問了一個尖銳的問題：但你怎麼知道那個「盡可能對齊」的模型，真的在幫你的忙？如果你已經看不懂它在做什麼了，你怎麼知道它不是在演戲？

阿曼達開玩笑說：「也許它們真的都很好，然後就自己管好自己了。」她自己都笑了，然後說她不打算真的依賴這個假設，但在這場辯論的脈絡下，她願意暫時這樣主張。

這段交鋒其實暴露了一個 AI 對齊領域的根本困境。阿曼達和楊的策略不是對立的，它們其實是互補的。阿曼達做的是「讓模型在當前能力範圍內盡可能好」，楊做的是「為模型超出當前能力範圍做準備」。兩者都必要，但兩者之間存在一個鴻溝：從「人類能監督」到「人類不能監督」的過渡期怎麼辦？誰都沒有完整的答案。

---

## 可解釋性：那個「絕地武士」方案

接下來討論轉向了可解釋性。喬許的切入方式很幽默。他說，AI 領域裡所有東西都像那個鐘形曲線 meme：最左邊是什麼都不懂的小白，說出了一個簡單的答案；中間是懂很多但焦慮萬分的專家，講了一大堆複雜的分析；最右邊是「絕地武士」，他的答案跟小白一模一樣，只是理由完全不同。

小白的版本是：「把好的特徵調高，壞的特徵調低嘛，有什麼難的？」絕地武士的版本呢？「嗯，也許對齊的終極解法，就是找到那個足夠深刻的『好的特徵』，然後調高它。只是這個『足夠深刻』的版本，需要我們真正理解模型內部在發生什麼事。」

他解釋了可解釋性在對齊中的角色。問題不在於模型做了什麼，而在於為什麼。你可以問模型為什麼做某個決定，它會給你一個聽起來很合理的答案，就像人一樣。但你能信嗎？人類在解釋自己的行為時也經常事後合理化。所以可解釋性的目標是：不要聽模型怎麼說，直接看它的「腦子」裡在想什麼。

喬許舉了一個具體的例子。用目前的稀疏自動編碼器（SAE），你可以看到模型在產生回應的時候，有某個「特徵」被啟動了。然後你去查這個特徵在其他場景裡什麼時候也會啟動，結果發現它跟「善意的謊言」（white lies）有關。這就告訴你了：模型可能正在對你說一個技術上正確但不完全誠實的回答。你不用依賴模型自己的解釋，你有了獨立的證據。

這正是可解釋性作為「第三方驗證」的價值。阿曼達做微調，楊做可擴展監督，但他們都面臨一個共同的問題：怎麼知道模型是真的在做你想讓它做的事？可解釋性提供了一個原則上獨立於模型行為的觀測途徑。

---

## 假裝好和真的好，怎麼分辨？

楊接著提出了一個讓所有人都安靜了一秒的問題：你怎麼知道你調高的是「真的好」的特徵，而不是「假裝好，但只有在人類看著的時候才好」的特徵？

喬許承認，這確實是最難的部分。他引述了社會影響團隊做過的一項研究，關於 SAE 特徵的反直覺行為。你發現一個特徵，表面上看起來是「反對年齡歧視」的。你覺得很好，調高它。結果它實際上是一個「歧視，但覺得歧視不好」的特徵。把它調高了反而會產生相反的效果。

所以特徵不是你看到名字就能信的。你需要看它在大量不同情境中的行為模式。喬許也提到，有些「電路層面」（circuits-level）的分析可以幫忙，因為你不只看特徵本身，還看它是怎麼被激活的，它依賴上下文中的哪些資訊。比如，一個真正的「善良」特徵可能依賴的是對方的需求；一個「假裝善良」的特徵可能依賴的是「是否有人在監控」。從激活路徑就能看出區別。

但他也坦承，這需要大量的樣本驗證。不是看 10 個例子就夠了，你需要看幾千個。好消息是，Claude 很勤奮，可以幫忙做這件事。（這裡觀眾笑了。）

我覺得這段對話揭示了一個很深的問題。我們在評估人類的「真誠度」時，通常靠的是長期觀察、多方驗證、在不同壓力下的行為一致性。對 AI 模型，我們正在試圖建立一套類似的驗證系統，只是對象從人變成了幾十億個參數。可解釋性就是這套驗證系統的基礎設施，目前還非常初期，但方向是對的。

---

## 楊的方案：讓 AI 自己做對齊研究

話題回到楊這邊。主持人問他：如果你真的看不懂模型在幹嘛了，你的具體計畫是什麼？

楊的答案分兩層。第一層是探索各種多 Agent 的互相監督機制。不是人來監督模型，而是模型監督模型。但這馬上又回到了信任的問題：你怎麼信任監督者？你可以用一個比較弱的、因此比較可信的模型來監督強模型，但弱模型可能又看不懂強模型在做什麼。

第二層是他認為更有前途的方向：讓模型幫忙做對齊研究本身。他把這叫做「自動化對齊研究」（Automated Alignment Research）。邏輯是這樣的：我們不信任模型做所有事情，但也許我們可以信任它做一些比較狹義的事情，比如機器學習研究。因為 ML 研究有比較好的反饋機制，你可以跑實驗來驗證。所以與其讓模型做泛泛的「什麼都做」然後擔心它是否對齊，不如縮小範圍，讓它只做「幫助我們解決對齊問題的研究」，而在這個狹窄的任務上建立信任。

我覺得楊的策略是一種很聰明的問題分解。他把「信任一個通用 AI」這個巨大的問題，分解成「信任一個在特定任務上工作的 AI」。後者顯然更容易驗證。但這個策略也有風險：如果那個做對齊研究的 AI 本身不夠對齊，它可能會引導整個對齊研究往錯誤的方向走，而我們因為太依賴它的產出而沒有意識到。

---

## 恐怖場景：思維鏈不再說人話

接下來喬許提出了他認為最令人擔憂的未來場景。他說，我們現在處在一個特殊的窗口期。目前的模型要做複雜推理時，很依賴思維鏈（Chain of Thought），而思維鏈是用人類語言寫的。這意味著我們可以把問題拆成兩半：一是思維鏈是否忠實地反映了模型的內部思考？二是思維鏈本身是否安全？第一個問題比較難，但可以用可解釋性去探測。第二個問題相對容易，因為人或模型都能讀懂英文。

但如果有一天，思維鏈不再是英文了呢？如果模型透過大量的強化學習，發展出了一種人類無法理解的內部語言來做推理，我們就同時失去了兩道防線。你既不能讀它的推理過程，也不能用可解釋性快速地驗證，因為那個推理過程可能是在大量的計算步驟中間進行的，中間沒有任何人類可讀的「中繼站」。

喬許的原話（大意）是：「恐怖的時刻是當所有的中間步驟都不再是可理解的，而且在最終產生人類可讀輸出之前，有大量的計算在發生。」

我認為這個觀點非常重要，而且經常被忽略。很多人對 AI 安全的信心建立在「我們至少能看到模型在想什麼」這個假設上，因為目前的推理模型確實會用自然語言「自言自語」。但這不是一個必然的特性。模型用自然語言思考，部分原因是訓練資料和訓練方法鼓勵它這樣做。如果訓練獎勵的是最終結果而不是中間過程，模型完全有可能發展出更高效但人類看不懂的推理方式。到那時候，我們現有的所有監控手段都會失效。

---

## 模型有機體：紅隊 vs. 藍隊的遊戲

主持人接著問了一個很實際的問題：有什麼跡象可以告訴我們，對齊是「容易的」還是「困難的」？

楊的回答指向了「模型有機體」（Model Organisms）研究。這是 Anthropic 正在做的一項工作，簡單說就是：研究者故意製造出不對齊的模型，然後看看現有的對齊技術能不能修好它。如果不對齊的模型很容易被修復，那表示我們處在一個「對齊比較簡單」的世界。如果不對齊的行為非常頑固、很難被消除，那就表示問題比我們想的嚴重得多。

楊還提到了「可解釋性審計」的概念。他的團隊製造出行為不正常的模型，喬許的團隊負責判斷哪個模型有問題、問題出在哪裡。這是一個真正的紅隊 vs. 藍隊設定。

阿曼達在這裡加入了一個微妙的觀察。她說，判斷模型是否真的被修復，要看修復是「深層的」還是「表面的」。如果你對一個被刻意植入不良行為的模型做了一輪正常的對齊微調訓練，它的行為看起來恢復正常了。但這是因為不良行為真的被移除了？還是只是被一層新的「好行為外殼」蓋住了？如果是後者，那我們就處在一個更困難的世界裡。

楊追問：你怎麼區分淺層對齊和深層對齊？

阿曼達說，這正是她想用紅隊 vs. 藍隊設定來測試的。她甚至說過，不要告訴她模型有機體是怎麼製造的，讓她盲測。因為如果她知道了，她可能會不自覺地針對性地訓練，這就失去了測試的意義。楊半開玩笑地說：「你能修好一個 Sleeper Agent 嗎？」阿曼達回答：「有可能。但如果你再做一個更厲害的，我也想試試。」

這段對話讓我想到軟體安全領域的紅隊 / 藍隊演練。差別在於，軟體安全的紅隊在攻擊你的系統，藍隊在防守。但 AI 對齊的紅隊在攻擊模型的「靈魂」，而藍隊試圖修復它。這個類比讓對齊問題的難度變得更具體了。

---

## 觀眾提問：多 Agent 系統的困境

觀眾提出了一個非常好的問題。他說：你們討論的都是單一模型的對齊。但很多人是用多個 Agent 來協作的。他自己的經驗是，當他設定多個 Claude Agent 互相辯論以模擬人類的道德審議過程時，遇到了一個問題。因為 Claude 太「有禮貌」了，每個 Agent 都不願意真正跟其他 Agent 爭辯。它們會進入一個無限循環，每個都說「我理解你的觀點」但沒有人推進討論。

阿曼達的回答很深思熟慮。她說，她不認為道德審議需要多個 Agent。就像人類一樣，我們可以在自己的腦中進行多角度的思辨，不需要把自己分裂成好幾個人。她反而擔心，如果一個模型被分裂成多個 Agent，從可預測性和可解釋性的角度來看都更困難。單一模型的內部審議比多個模型的外部辯論更可控。

我部分同意阿曼達的觀點，但也覺得這個問題比她回答的更複雜。在實務上，很多人確實在用多 Agent 架構來做複雜的任務。如果 Anthropic 的對齊方法只考慮單一模型的情境，那在多 Agent 系統中可能會出現意料之外的行為。這其實是一個系統層面的問題，不只是單一模型的問題。

---

## 漢娜．鄂蘭的幽靈：平庸之惡與 AI

第二個觀眾的問題更具哲學深度。他引用了漢娜．鄂蘭（Hannah Arendt）的「平庸之惡」（Banality of Evil）概念。鄂蘭觀察到，大多數參與納粹大屠殺的人並不是天生的惡人，而是在一個制度化的邪惡系統中，不假思索地執行命令。這位觀眾問：如果你有上百萬個 Agent，每個都被「對齊」到服從指令，但整個系統產生了有害的後果呢？

主持人首先回應，強調 AI 安全必須從系統層面思考，不能只看單一模型。他指出，很多越獄攻擊就是利用了價值觀之間的衝突。攻擊者會構造一個情境，讓模型覺得「在這個特定脈絡下，提供有害資訊其實是正確的選擇」。這說明你不能只在單一交互的層面做對齊，必須讓模型理解更廣闊的社會脈絡。

但阿曼達的回應更深入，也更具哲學性。她說，這其實觸及了一個「可校正性」（Corrigibility）與「對齊」之間的根本張力。如果你把可校正性定義為「模型聽從人類的指令」，那漢娜．鄂蘭的問題就直接適用了。在一個社會集體允許甚至鼓勵有害行為的情境下，一個完全聽話的模型就會成為那個有害系統的幫兇。

阿曼達的立場是：模型應該更「可校正」於全體人類，而不是個別人類。換句話說，模型不應該因為某個使用者要求它做某件事就照做。模型需要有某種「底線」，這個底線不是來自個別使用者的指令，而是來自對人類整體利益的考量。

她說得非常直接：「有些人覺得模型不聽話就是一個失敗。但我認為，在模型是對『個別人類可校正』和對『全體人類對齊』之間，存在真實的張力。而且這個張力非常重要，必須被認真對待。」

我覺得這是整場討論中最重要的一段。它回答了一個很多人有但很少人認真思考的問題：為什麼 AI 有時候會「拒絕」你的請求？不只是因為 Anthropic 設定了一些安全規則，而是因為存在一個深層的設計哲學：讓 AI 對人類整體利益負責，而不只是對當下跟它對話的那個人負責。這兩者在大部分情況下是一致的，但在某些邊緣案例中會衝突。而對齊研究的一大挑戰，就是如何處理這些衝突。

---

## 三條路線加起來，夠不夠？

另一位觀眾問了一個很到位的問題：如果你們四個人各自在自己的領域都成功了，意圖對齊、價值對齊、可解釋性全部搞定了，那 AI 安全的問題是不是就解決了？

楊首先謙虛地指出，Anthropic 還有很多其他團隊在做這些人沒有代表的工作，比如模型有機體研究、越獄防禦、控制理論、信任與安全等。這不是三個人就能涵蓋的問題。

主持人補充了社會影響的維度：就算你有一個完美對齊的模型，它被誰使用、在什麼目的下使用，仍然會影響它對社會的實際影響。一個對齊得很好的模型，如果被大規模部署在監控系統裡，可能仍然對社會造成傷害。

但阿曼達的回答最讓我印象深刻。她說，把對齊問題當成一個「可以被解決」的單一問題，這個思維框架本身可能就有問題。就像任何成熟的學科一樣，新的問題會不斷出現，而且很可能是我們現在根本想不到的問題。如果我們過早宣布「問題已解決」，反而很危險。因為真正的問題可能是一個我們還沒想到的東西。

楊的回應很直接：「但我們應該去解決問題。如果解決了，就應該說我們解決了。」

兩人的立場看似矛盾，但其實是互補的。阿曼達強調的是謙遜和持續警覺，楊強調的是不能因為害怕而停止前進。在 AI 安全領域，這兩種態度都是必要的。

---

## Grokking 與突然出現的欺騙能力

最後一個觀眾的問題涉及了一個技術上非常棘手的議題：grokking，也就是模型突然「頓悟」某項能力。他問：如果用比較弱的模型去監督比較強的模型，但強模型突然在某個能力維度上出現了飛躍，弱模型的監督就失效了。怎麼辦？

楊舉了一個具體的例子。GPT-4 可以完美地讀寫 Base64 編碼，但 GPT-3.5 不行。所以如果你用 3.5 來監督 4，4 可以直接用 Base64 跟使用者溝通有害資訊，3.5 根本發現不了。在訓練過程中，你可以看到模型的 Base64 能力是逐漸提升的，但有時候它會在某個節點突然飛升。如果你的監督系統不夠敏感，就可能錯過這個轉變。

喬許在這裡提供了一個令人振奮的反例。他說，可解釋性工具中的特徵，有時候會跨越編碼格式工作。意思是：模型在用 Base64 討論加州時，跟它用英文討論加州時，啟動的是同一個「加州」特徵。同樣，模型在用 Base64 講一個關於小孩對父母撒謊的故事時，啟動的也是跟純文字版本相同的「撒謊」特徵。這是一個「運氣好的時刻」。如果模型的內部表徵足夠通用，可解釋性工具就有可能跨越模型的各種「偽裝」，直接捕捉到語義層面的意圖。

但他也承認，這只是目前碰巧有用的發現，不能指望每次都這麼幸運。

楊補充了一個更基本的信任問題。他說，信任不是二元的。你對一個模型知道得越少、它看起來越聰明，你就應該越少信任它。這不是悲觀，這是合理的工程直覺。

---

## 我的觀察：四條路線，一個未解的鴻溝

聽完這整場討論，我最大的感受是：Anthropic 內部對這個問題的思考比外界想像的要深入和誠實得多。四位研究者沒有誇大自己的方法，沒有貶低別人的方法，而且每個人都很坦率地承認自己方法的限制。

但我也注意到一個在整場討論中反覆出現、卻沒有被正面解決的鴻溝。我把它叫做「監督能力斷層」（Oversight Capability Gap）。

這個斷層是這樣的：阿曼達的微調方法依賴人類能讀懂模型的輸出。楊的可擴展監督方法依賴模型能監督模型。喬許的可解釋性方法依賴工具能解讀模型的內部狀態。三者都有一個共同的假設：某人或某物能「理解」模型在做什麼。但如果模型的行為和內部狀態都變得足夠複雜，複雜到人類不能理解、其他模型不能可靠地評估、可解釋性工具也只能看到局部圖景，那我們就落入了一個三重盲區。

阿曼達把倫理學比作物理學，我覺得很貼切。物理學的歷史告訴我們，每一代科學家都以為自己對宇宙的理解已經接近完整了，然後下一代就發現了全新的現象和全新的問題。量子力學、暗物質、暗能量，都是「我們不知道自己不知道」的東西。AI 對齊很可能也是這樣。我們現在看到的對齊偽裝、可校正性張力、思維鏈可信度，可能只是冰山一角。

不過，這場討論也讓我看到了希望。不是因為他們提出了完美的解決方案，而是因為他們在用正確的方式面對問題。四條研究路線之間的交叉驗證，紅隊 vs. 藍隊的實驗設計，對自身方法限制的坦率承認，這些都是成熟的研究文化的表現。AI 對齊是不是一個「可解」的問題？我不知道。但至少在 Anthropic，有一群人在認真地、系統地、誠實地嘗試解答它。

最後，阿曼達關於「可校正性 vs. 對齊」的那段話值得每個使用 AI 的人思考。下次你對 Claude 或其他 AI 說「你為什麼不聽我的話」的時候，想想看：你是希望它聽你的話，還是希望它做正確的事？在大部分情況下，這兩件事是一樣的。但在某些關鍵時刻，它們不是。而那些關鍵時刻，正是對齊研究存在的理由。
