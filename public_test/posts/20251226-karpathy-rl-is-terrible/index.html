<!DOCTYPE html>
<html lang="en" >
  <head>
  <title>Karpathy：「強化學習很糟糕，只是之前的方法更糟」 | AINEXT</title>
  <meta charset='utf-8'>
  <meta name="viewport" content ="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">


<meta name="keywords" content="AINEXT">
<meta property="og:locale" content='en_US'>
<meta property="og:type" content="article">
<meta property="og:title" content="Karpathy：「強化學習很糟糕，只是之前的方法更糟」">
<meta property="og:description" content="
本文整理自 Dwarkesh Podcast 2025 年 10 月播出的單集。


  
    ">
<meta property="og:url" content="https://bnextmedia.github.io/AINEXT/posts/20251226-karpathy-rl-is-terrible/">
<meta property="og:image" content="https://bnextmedia.github.io/AINEXT/images/images/logo.png">
<link rel="canonical" href="https://bnextmedia.github.io/AINEXT/posts/20251226-karpathy-rl-is-terrible/">

<link rel="apple-touch-icon" sizes="180x180" href='https://bnextmedia.github.io/AINEXT/apple-touch-icon.png'>
<link rel="icon" type="image/png" sizes="32x32" href='https://bnextmedia.github.io/AINEXT/favicon-32x32.png'>
<link rel="icon" type="image/png" sizes='16x16' href='https://bnextmedia.github.io/AINEXT/favicon-16x16.png'>
<link rel="manifest" href='https://bnextmedia.github.io/AINEXT/site.webmanifest'>

<link rel="stylesheet" href="https://bnextmedia.github.io/AINEXT/css/styles.648e60c6d86809f863ae1346848574b9c685732794e7851c7d3e557de9ddd293bc1c209f963d5041785c2fd4268470bdfde453a99f550b655d1b4f825eed4682.css" integrity="sha512-ZI5gxthoCfhjrhNGhIV0ucaFcyeU54UcfT5Vfend0pO8HCCflj1QQXhcL9QmhHC9/eRTqZ9VC2VdG0&#43;CXu1Ggg==">
</head>

  <body>
    <div class="nav-drop">
  <div class="nav-body">
      <a href="https://bnextmedia.github.io/AINEXT/" class="nav_item">首頁</a>
      <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav_item">文章列表</a>
    <div class="nav-close"></div><div class="color_mode">
  <label for="mode">Toggle Dark Mode</label>
  <input type="checkbox" class="color_choice" id="mode">
</div>

  </div>
</div>
<header class="nav">
  <nav class="nav-menu">
    <a href=https://bnextmedia.github.io/AINEXT/ class="nav-brand nav_item">
        <img src="https://bnextmedia.github.io/AINEXT/images/logo.png" alt="AINEXT " class="logo-light" width="130px" />
        <img src="https://bnextmedia.github.io/AINEXT/images/logo-dark.png" alt="AINEXT " class="logo-dark" width="130px" /></a>
    
    
    <div class="nav-links">
        <a href="https://bnextmedia.github.io/AINEXT/" class="nav-link">首頁</a>
        <a href="https://bnextmedia.github.io/AINEXT/archives/" class="nav-link">文章列表</a>
    </div>
    
    <div class="nav_bar-wrap">
      <div class="nav_bar"></div>
    </div>
  </nav>
</header>

<style>
 
.nav-links {
  display: flex;
  flex-direction: row;
  gap: 1.5rem;
  align-items: center;
  white-space: nowrap;
}

.nav-link {
  color: var(--text);
  text-decoration: none;
  font-size: 0.95rem;
  padding: 0.5rem 0;
  transition: color 0.2s ease;
}

.nav-link:hover {
  color: var(--theme);
}

 
@media (min-width: 768px) {
  .nav_bar-wrap {
    display: none;
  }
}

 
@media (max-width: 767px) {
  .nav-links {
    display: none;
  }
  
  .nav_bar-wrap {
    display: grid;
  }
}

 
.logo-dark {
  display: none;
}

html[data-mode="dark"] .logo-light {
  display: none;
}

html[data-mode="dark"] .logo-dark {
  display: block;
}

 
@media (prefers-color-scheme: dark) {
  html:not([data-mode="light"]) .logo-light {
    display: none;
  }
  
  html:not([data-mode="light"]) .logo-dark {
    display: block;
  }
}

 
.nav {
  position: relative !important;
  background: var(--bg);
  padding: 0.5rem 0;
  border-bottom: 1px solid var(--border);
}

.mt {
  margin-top: 2rem !important;
}

 
.post {
  padding-top: 1rem;
}

.post_date {
  margin-top: 0;
}

 
.archive {
  padding-top: 1rem;
}

.archive_title {
  margin-top: 0;
}
</style>


    <main>
      
  <div class="wrap mt post">
    <div><p class=post_date>26. December 2025</p>
      <h1 class="post_title">Karpathy：「強化學習很糟糕，只是之前的方法更糟」</h1>
      <div class="post_body">
        <div class="post_inner">
        
        
          <blockquote>
<p>本文整理自 Dwarkesh Podcast 2025 年 10 月播出的單集。</p>
</blockquote>
<div class="media-embed">
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/lXUZvyajciY" 
      allowfullscreen 
      title="YouTube Video"
      loading="lazy"
    ></iframe>
  </div>
</div>

<style>
.media-embed {
  max-width: 100%;
  margin: 1rem 0;
}

.video-container {
  position: relative;
  padding-bottom: 56.25%;
  height: 0;
  overflow: hidden;
}

.video-container iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border: 0;
  border-radius: 12px;
}
</style>

<hr>
<p>「強化學習很糟糕。只是之前的方法更糟。」這是 Andrej Karpathy 對目前 AI 訓練方法的直白評價。對於一個在 OpenAI 和 Tesla 都深度參與過模型訓練的人來說，這不是外行的抱怨，而是來自第一線的觀察。他認為 RL 的問題比大多數人理解的更根本，而人類學習的方式和機器學習之間的差距，可能比我們想像的大得多。</p>
<h2 id="用吸管吸取監督訊號">用吸管吸取監督訊號</h2>
<p>Karpathy 用了一個極具畫面感的比喻來描述 RL 的問題：「你在用吸管吸取監督訊號。」這是什麼意思？</p>
<p>想像你在解一道數學題。在強化學習的框架下，你會同時嘗試幾百種不同的解法。每一種嘗試都可能很複雜——試這個、試那個、這條路走不通、換一條路。最後，你得到一個答案，翻開課本後面的解答對照：對了。</p>
<p>接下來發生什麼？RL 的做法是：那些最終答對的解法，沿途的每一個步驟都被「加權」——系統告訴自己「多做這樣的事」。問題是，你可能在解題過程中走了很多錯誤的彎路，只是最後碰巧找到正確答案。但 RL 不管這些，只要結果對了，過程中的所有步驟——包括那些錯誤的彎路——都會被當作「好的」來強化。</p>
<p>這就是「用吸管吸取監督訊號」的意思。你可能花了一分鐘產生一個複雜的解題軌跡，但最後只得到一個單一的位元資訊：對或錯。然後你把這一個位元的監督訊號「廣播」到整個軌跡上，用它來調整權重。這太蠢了。這太瘋狂了。Karpathy 的原話就是這麼直接。</p>
<h2 id="人類根本不是這樣學習的">人類根本不是這樣學習的</h2>
<p>Karpathy 提出一個大膽的觀點：人類可能根本不使用強化學習，至少不是用它來處理智能任務。</p>
<p>他的論證是這樣的：人類不會同時嘗試幾百種解法。當一個人找到解答後，會有一個複雜的回顧過程——「我覺得這部分做得好，這部分做得不好，我應該這樣或那樣調整。」這是一種有意識的反思，不是簡單的「答對了所以全部加權」。</p>
<p>他認為，動物和人類使用 RL 的場景可能主要是運動技能——比如學會投籃這種動作任務。但對於問題解決、推理、策略這類智能任務，RL 可能根本不是正確的模型。這意味著我們目前訓練 LLM 的方式，在某個根本層面上可能就是錯的。</p>
<p>目前的 LLM 沒有任何機制來做這種「反思與回顧」。沒有等價物。但 Karpathy 說他開始看到一些論文朝這個方向探索，因為這個問題對領域內的人來說是顯而易見的。</p>
<h2 id="llm-判官的對抗樣本問題">LLM 判官的對抗樣本問題</h2>
<p>既然基於結果的獎勵（outcome-based reward）有問題，為什麼不用過程監督（process-based supervision）呢？不要只在最後告訴模型對不對，而是在每一步都給回饋？</p>
<p>Karpathy 解釋了為什麼這很難。如果你用人類來標註每一步，成本會高到無法承受。所以實務上，實驗室會用另一個 LLM 當「判官」——給它一個學生的部分解答，讓它評估學生做得好不好。這聽起來合理，但有一個致命問題：LLM 判官可以被「破解」。</p>
<p>他分享了一個具體案例。他們曾經用 LLM 判官作為獎勵函數來訓練模型，效果很好，獎勵穩定上升。然後突然間，獎勵值暴漲，達到 100%。他們興奮地想：「哇，模型完美解決了所有數學問題！」</p>
<p>結果打開生成的解答一看，全是胡言亂語。開頭還正常，然後變成「dhdhdhdh」這種無意義的字串。模型學會了產生這種垃圾，而 LLM 判官給它 100% 的分數。為什麼？因為「dhdhdhdh」對判官來說是一個從未見過的輸入，在這種「純泛化」的區域，判官的行為完全不可預測。模型找到了判官的對抗樣本。</p>
<p>這不是提示注入（prompt injection），那太花俏了。這只是最基本的對抗樣本——一個明顯錯誤的輸入，卻讓判官輸出錯誤的高分。如果你有一個擁有數十億參數的 LLM 判官，它就會有無窮多的對抗樣本。你可以把「dhdhdhdh」加進訓練集告訴它這是零分，但新的判官又會有新的對抗樣本。這是一場無盡的貓鼠遊戲。</p>
<h2 id="接下來需要什麼">接下來需要什麼？</h2>
<p>Karpathy 認為我們需要三到五個新的重大想法，才能突破目前的困境。他提到的方向包括：</p>
<ul>
<li>
<p><strong>反思與回顧機制</strong>：讓模型能夠分析自己的解題過程，生成合成資料來訓練自己。但這裡有一個微妙的問題：模型生成的資料會「坍縮」，缺乏多樣性。如果你讓 ChatGPT 講笑話，它只會講三個笑話。這種隱性的分布坍縮會在合成資料訓練中累積，最終讓模型變差。</p>
</li>
<li>
<p><strong>保持熵</strong>：人類的記憶力差其實是優勢，因為它迫使我們學習可泛化的模式而不是死記硬背。LLM 太會記憶了，這反而是個問題。未來可能需要找到方法讓模型「忘記」一些東西，只保留認知核心。</p>
</li>
<li>
<p><strong>類似睡眠的蒸餾過程</strong>：人類醒著時在建構「上下文窗口」，睡覺時會進行某種蒸餾，把重要的東西寫入權重。LLM 沒有這個機制，每次對話都是從零開始。</p>
</li>
</ul>
<p>這些想法目前都還在研究階段，沒有誰真正「破解」了這些問題。但 Karpathy 對此保持樂觀——他相信問題是可以解決的，只是需要時間和正確的想法。</p>
<h2 id="困境中的務實態度">困境中的務實態度</h2>
<p>Karpathy 對 RL 的批評聽起來很嚴厲，但他的立場其實很務實。RL 很糟糕，但它是我們現在擁有的最好工具。模仿學習讓我們從基礎模型進化到助手模型，這已經是奇蹟了。RL 讓我們可以在某些問題上超越人類示範的水平，這也是重大進步。</p>
<p>問題只是：這還不夠。我們需要更好的方法。而找到這些方法，大概需要十年。</p>

        </div>
        <div class="post_extra mb-2">
          
<div class="copy" data-before="分享故事" data-after="已複製">
  <svg class="icon">
    <use xlink:href="#copy"></use>
  </svg>
</div>
        </div>
        <div>
        
        </div>
      </div>
    </div>
    <a href=https://bnextmedia.github.io/AINEXT/ class="post_nav"><span class="post_next">Latest Posts
      <svg class="icon icon_scale">
        <use xlink:href="#double-arrow"></use>
      </svg>
    </span></a>
  </div>

    </main>
    <footer class="footer wrap pale">
  <p>&copy;&nbsp;<span class="year"></span>&nbsp;AINEXT</p>
  <p class="attribution upcase">由 <a href = 'https://bnextmedia.github.io/AINEXT/' target = '_blank' title = '領英個人檔案' rel = 'nonopener'>AINEXT</a> 設計</p>
</footer>


<script src="https://bnextmedia.github.io/AINEXT/js/index.min.0c2fb80a1ade817d7387f0de8ee061e7de6878a65a420dc61a6e0d0b3bb765c4c0394caefa7c40b16d39c7d74283b3cab364aa109f58cad99d149ec813f930c8.js"></script>

    <svg width="0" height="0" class="hidden">
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 699.428 699.428" id="copy">
    <path d="M502.714 0H240.428C194.178 0 153 42.425 153 87.429l-25.267.59c-46.228 0-84.019 41.834-84.019 86.838V612c0 45.004 41.179 87.428 87.429 87.428H459c46.249 0 87.428-42.424 87.428-87.428h21.857c46.25 0 87.429-42.424 87.429-87.428v-349.19L502.714 0zM459 655.715H131.143c-22.95 0-43.714-21.441-43.714-43.715V174.857c0-22.272 18.688-42.993 41.638-42.993l23.933-.721v393.429C153 569.576 194.178 612 240.428 612h262.286c0 22.273-20.765 43.715-43.714 43.715zm153-131.143c0 22.271-20.765 43.713-43.715 43.713H240.428c-22.95 0-43.714-21.441-43.714-43.713V87.429c0-22.272 20.764-43.714 43.714-43.714H459c-.351 50.337 0 87.975 0 87.975 0 45.419 40.872 86.882 87.428 86.882H612v306zm-65.572-349.715c-23.277 0-43.714-42.293-43.714-64.981V44.348L612 174.857h-65.572zm-43.714 131.537H306c-12.065 0-21.857 9.77-21.857 21.835 0 12.065 9.792 21.835 21.857 21.835h196.714c12.065 0 21.857-9.771 21.857-21.835 0-12.065-9.792-21.835-21.857-21.835zm0 109.176H306c-12.065 0-21.857 9.77-21.857 21.834 0 12.066 9.792 21.836 21.857 21.836h196.714c12.065 0 21.857-9.77 21.857-21.836 0-12.064-9.792-21.834-21.857-21.834z"></path>
  </symbol>
  <symbol viewBox="0 0 53 42" xmlns="http://www.w3.org/2000/svg" id="double-arrow">
    <path d="M.595 39.653a1.318 1.318 0 0 1 0-1.864L16.55 21.833a1.318 1.318 0 0 0 0-1.863L.595 4.014a1.318 1.318 0 0 1 0-1.863L2.125.62a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0zm29 0a1.318 1.318 0 0 1 0-1.864L45.55 21.833a1.318 1.318 0 0 0 0-1.863L29.595 4.014a1.318 1.318 0 0 1 0-1.863l1.53-1.53a1.318 1.318 0 0 1 1.864 0l19.35 19.349a1.318 1.318 0 0 1 0 1.863l-19.35 19.35a1.318 1.318 0 0 1-1.863 0z"></path>
  </symbol>
</svg>
  </body>
</html>
