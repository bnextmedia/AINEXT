---
title: "AI 基準測試革命——為什麼通用評測無法告訴你 AI 能不能用在你的業務"
date: 2025-12-25T14:30:00+08:00
description: "當 OpenAI 發布新模型，我們會看到各種基準測試分數：編程能力提升 20%、數學推理進步 15%。但這些數字對企業導入 AI 的意義有限。前麥肯錫 QuantumBlack Labs 主管 Matt Fitzpatrick 指出，企業需要的是針對特定任務的客製化評測——這個領域正在爆發創業機會。"
tags: ["AI Benchmark", "AI 評測", "Custom Evals", "Matt Fitzpatrick", "Invisible Technologies", "Podcast"]
categories: ["AI 技術前沿"]
source_url: "https://www.youtube.com/watch?v=7GFKB0oKd9A"
source_name: "Moonshots Podcast"
draft: false
---

> 本文整理自 Moonshots Podcast 2024 年 12 月播出的單集。

{{< youtube 7GFKB0oKd9A >}}

---

每當 OpenAI、Anthropic 或 Google 發布新模型，科技媒體都會報導一串基準測試數字：HumanEval 編程能力提升 20%、GSM8K 數學推理進步 15%、MMLU 知識測試創新高。這些數字讓我們知道模型在「變強」，但它們回答不了一個關鍵問題：這個模型能不能用在我的業務上？

「大部分公眾關注的焦點，到目前為止都放在大型公開基準上，比如編程能力測試，」Matt Fitzpatrick 在 Moonshots Podcast 上說。他曾在麥肯錫待了超過十年，領導 QuantumBlack Labs 的 AI 產品開發，現在是 Invisible Technologies 的 CEO。「這些基準對於衡量『模型整體上有沒有進步』很有用。但問題是，如果你是企業或小公司，你的基準不是寬泛的認知測試——而是特定任務的準確度，或者與人類表現的等效程度。」

## 為什麼 80% 準確度在企業場景不夠用

假設你要導入一個 AI 系統來處理保險理賠審核。這個任務需要閱讀理賠申請文件、交叉比對保單條款、判斷理賠是否成立、計算應賠金額。一個在通用基準上表現優異的大型語言模型，能做到這件事的 80% ——聽起來不錯，對吧？

問題在於，那 20% 的錯誤會在哪裡發生。可能是把應該拒絕的理賠批准了（公司虧錢），可能是把合理的理賠拒絕了（客戶投訴、聲譽受損），可能是計算金額算錯（法律糾紛）。在企業場景中，80% 的準確度通常意味著系統不可用——風險太高，不如不用。

Fitzpatrick 用一個直接的問題來檢驗 AI 用例是否真的可行：「你願意把年終獎金押在這個用例能成功嗎？」如果答案是否定的，代表你還沒有足夠的信心相信這個系統。而要建立這種信心，你需要的不是通用基準，而是針對你的特定任務設計的評測方法。

這就是所謂的「客製化評測」（custom evals）。以客服中心為例，你需要建立一套基準，裡面包含你最好的客服人員會如何處理各種類型的來電，然後測試 AI 代理人能否達到同等水準。這套基準會針對你的產品、你的客戶群、你的處理流程來設計，而不是用某個通用的客服對話資料集。

## 從理賠處理到投資備忘錄：基準的碎片化

通用基準之所以有用，是因為它們有清晰的對錯定義。編程題目有標準答案——程式碼能不能通過測試案例。數學題目有正確解答。但企業任務往往沒有這麼清楚的邊界。

以投資備忘錄為例。不同的投資公司，對於備忘錄的格式、長度、內容重點都有不同偏好。有的公司習慣寫十頁，有的寫四十頁。有的強調財務分析，有的著重市場競爭態勢。你要怎麼判斷一份 AI 生成的投資備忘錄「夠好」？沒有標準答案，只有「這家公司的專業人士覺得可以接受」這個標準。

這意味著，幾乎每一個企業 AI 用例，都需要建立自己的評測基準。保險理賠審核有保險理賠審核的基準，法律合約審查有法律合約審查的基準，醫療診斷建議有醫療診斷建議的基準。這些基準不能通用，因為每個領域、每家公司、每個任務的「正確」定義都不一樣。

Fitzpatrick 估計，這代表我們需要「數以千計的新型窄領域基準，來涵蓋每一個勞動類別、每一個產業垂直領域」。這是一個巨大的工作量，但也是一個巨大的機會。

## 誰擁有基準，誰就擁有話語權

Podcast 主持人 Dave Friedberg 點出了一個有趣的現象：在 AI 時代，定義基準的人可能會成為產業領袖。他舉了一個假設性的例子——產權保險（title insurance）。這是一個高度專業化的領域，外人很難判斷 AI 在這個領域做得好不好。如果有人既懂這個產業又懂 AI，主動建立了一套公認的評測標準，他就自動成為這個領域的權威。

「如果你宣稱自己是這個基準的擁有者，然後公開這個基準的細節，」Friedberg 說，「目前的證據顯示，你會立刻成為明星。因為沒有人在搶這些主題的話語權。如果你先到達那裡，你就會成為即時的權威。」

這個觀察揭示了一個創業機會。在 post-training（後訓練）越來越商品化的時代，如果你擁有高品質的評測基準，你往往也擁有了訓練出優秀專業模型的能力。因為基準不只是測試工具，它也定義了「什麼是好」，而這個定義可以用來指導訓練過程。擁有基準的人，可以利用現有的開源模型進行專業微調，產出在特定領域表現卓越的產品。

Invisible Technologies 正是在做這件事的一部分。他們為企業客戶建立特定任務的基準，然後用這些基準來評估和改進 AI 代理人的表現。這不是一次性的工作，而是持續的過程——隨著任務需求變化、模型能力提升，基準也需要不斷更新。

## 從「證明 AI 行」到「證明 AI 對你的任務行」

通用基準告訴我們的是：AI 作為一種技術，在過去三年裡取得了 50% 到 100% 的進步。這很重要，因為它支撐了整個產業對未來的信心，也支撐了數十億美元的資本支出。如果通用基準停滯不前，投資人會開始質疑 AI 是否已經到達瓶頸。

但對於試圖導入 AI 的企業來說，通用基準只是入門門檻。它告訴你「AI 有可能做到這件事」，但不告訴你「AI 在你的特定情境下能做到什麼程度」。這兩者之間的差距，正是企業導入失敗率居高不下的原因之一。

MIT 的研究顯示，只有 5% 的企業 AI 模型最終進入生產環境。其中一個關鍵原因是：企業在做概念驗證時，往往沒有建立嚴謹的評測方法。系統「看起來能用」，但沒人能量化「用得多好」。上線後出問題，才發現當初的評估太粗糙。

這個問題的解決方案不是等 AI 變得更強——AI 已經夠強了。解決方案是在每一個用例上，花時間建立真正有意義的評測基準。這需要領域專家的參與（只有他們知道什麼叫「做對了」），需要足夠的測試案例（涵蓋各種邊緣情況），需要持續的監控和更新（因為需求會變化）。

這聽起來很麻煩，但這正是把 AI 從「展示用」變成「生產用」的必經之路。誰能在自己的領域建立起可靠的評測體系，誰就能真正把 AI 的能力轉化為業務價值。
