---
title: "2026 AI 展望：前沿模型趨於平庸，小模型和開源才是真正的變局"
date: 2026-02-04T11:00:00+08:00
description: "QLoRA 發明人 Tim Dettmers 和 FlashAttention 共同開發者 Dan Fu 對 2026 年 AI 發展做出預測：前沿模型進步放緩、小模型效能逼近大模型、開源將迎來新一波能力躍進，以及 Together AI 的 Megakernels 如何把推論加速 2 到 3 倍。"
tags: ["AI 2026", "小模型", "開源 AI", "Together AI", "AI2", "Podcast"]
categories: ["AI 產業動態"]
image: "/images/posts/20260204-gpu-scaling-debate-agent-era.webp"
source_url: "https://www.youtube.com/watch?v=XCCkgRzth6Q"
source_name: "The MAD Podcast with Matt Turck"
related_companies: ["together-ai"]
related_people: []
draft: false
---

> 本文整理自《The MAD Podcast with Matt Turck》2026 年 1 月播出的單集。

{{< youtube XCCkgRzth6Q >}}

{{< spotify "episode/5fdqgGAgTqchtz9mUYBbEr" >}}

---

## 前沿模型的同質化困境

在節目尾聲，主持人 Matt Turck 請兩位來賓對 2026 年做出具體預測。QLoRA 發明人、卡內基美隆大學（Carnegie Mellon University）助理教授暨艾倫人工智慧研究所（AI2）研究員提姆．德特默斯（Tim Dettmers），給出了一個出人意料的答案：他認為 2026 年在前沿模型這個層級，會比大家期望的更無聊。

他分享了一個很有畫面感的小故事。前幾天他用自己配置的 coding agent 工作，一直覺得手感很好，以為自己在用 Claude Opus 4.5。直到他某個時刻才發現，其實他用的是 GLM 4.7，一個開源模型。兩個模型用起來的感覺已經非常接近了。在 Dettmers 看來，這不是偶然，而是整個前沿模型正在經歷的系統性趨勢：不同公司、不同架構訓練出來的頂級模型，在使用體感上越來越難以區分。

原因並不難理解。預訓練資料已經接近枯竭，各家實驗室本質上都在用相同的網際網路資料訓練模型。合成資料能在一定程度上彌補，這就像是在已有的「肌肉」上做平滑處理。但 Dettmers 從自己的效率研究視角觀察到，這個平滑的邊際效益正在遞減。他不認為 coding agent 在 2026 年會變得「好非常多」，使用體驗會改善，但底層能力的差距正在縮小。

## AI2 即將發布的開源 Coding Agent

但 Dettmers 對另一件事非常興奮：他在 AI2 正在開發的開源 coding agent 即將發布，而且有幾個關鍵特性可能改變遊戲規則。

第一個是訓練成本。傳統上，要訓練一個高品質的 coding agent，你需要大量合成資料的生成和模型微調，成本不低。Dettmers 的團隊開發了一套方法，讓訓練成本降低到原來的百分之一，但仍能達到最先進的效能水準。

第二個更重要。他稱之為開源模型的「聖杯」：你可以把這套方法指向任何私有程式碼庫，它會自動生成訓練資料，不需要你準備測試案例或理解如何生成資料。流程是全自動的。訓練完成後，你會得到一個大約 320 億參數的 agent，效能堪比前沿模型，但可以在本地部署。你可以針對不同的程式碼庫、不同的任務建立一支專業化的「小模型軍團」。

第三個是科學化。Dettmers 的團隊在研究 coding agent 的過程中，發現了大量論文裡沒有提到的混淆因子和隱藏變數。他們正在建立 coding agent 的 scaling law，釐清什麼因素真正重要、什麼不重要。他認為，一旦這些「秘密」被公開，整個社群在 coding agent 上的進步速度會加快很多。

這個發布對臺灣的軟體開發生態特別有意義。如果你是一家有自己程式碼庫的公司，你不再需要把程式碼送到 Anthropic 或 OpenAI 的伺服器上才能獲得 AI 輔助。一個在本地跑的 320 億參數模型，經過你自己程式碼庫的專業化訓練，可能比通用的前沿模型更懂你的系統。

## 小模型的崛起：效能逼近前沿，成本遠低一截

Dettmers 的 2026 預測有一個核心論述：前沿模型的進步會放緩，但小模型的進步會加速。

邏輯很直觀。大模型已經訓練好了，你可以把大模型的知識蒸餾（distill）到小模型裡。蒸餾的概念像師傅帶徒弟：師傅（大模型）示範怎麼做，徒弟（小模型）照著學，不需要自己從零摸索。1000 億參數等級的模型已經相當強大，可以在一台配有 RTX 6000 或同等級 GPU 的工作站上運行，硬體成本大約 6,000 美元。對很多公司來說，這意味著你不需要依賴前沿模型的 API。小模型可能因為專業化的緣故，在你的特定任務上甚至比大模型更好。

但 Dettmers 也指出了一個關鍵瓶頸。Anthropic 執行長 Dario Amodei 曾提過，強大的開源模型很多，但幾乎沒有人在用，因為部署太複雜了。一旦模型大到需要 8 張以上的 GPU 才跑得動，你就需要分散式推論系統，把模型切成好幾塊分給不同的 GPU 跑，然後再把結果拼回來。這就像一道菜太大，一個爐子放不下，得用三個爐子同時煮不同部分再合在一起。目前沒有開源方案能把這件事做好。Dettmers 認為，如果有人能為 8 張 GPU 的機器把這套系統做好，小模型的效率就能真正逼近前沿模型。這是 2026 年最值得關注的基礎設施缺口之一。

## Together AI 的技術武器：Megakernels 和 Atlas

加州大學聖地牙哥分校（UC San Diego）助理教授、Together AI GPU Kernel 工程副總裁丹尼爾．傅（Dan Fu）則從另一個角度補充了小模型和效率的願景。Together AI 的核心使命就是讓 AI 模型跑得更快更便宜，他們服務的客戶包括 Cursor 這樣的頂級 AI 編碼工具。

Dan Fu 分享了兩個正在研發中的技術。第一個叫 Megakernels。要理解這個技術，需要先理解 GPU kernel 的傳統做法：一個模型有幾百個運算步驟，傳統做法是每個步驟寫一支獨立的小程式，就像工廠流水線上每一站各有一台獨立的機器。但 Together AI 的 Megakernels 是把整個模型，不管多少億參數，塞進一個單一的 GPU kernel 裡，等於把整條流水線整合成一台超級機器，中間不用搬貨、不用等待。這讓過去不可能的細粒度最佳化變得可行。Dan Fu 說，效果是讓 NVIDIA 的 GPU 在行為上更像 Cerebras 或 SambaNova 那種專用 AI 晶片，推論速度比已經高度優化的推論引擎還快 2 到 3 倍。

第二個叫 Together Atlas，是推測性解碼（speculative decoding）的進階版。推測性解碼的概念像餐廳裡的學徒猜師傅下一步要放什麼調味料，先幫他備好。猜對了，師傅直接用，省下取料的時間；猜錯了，師傅自己拿，也沒有額外損失。傳統的做法是用一個小模型去猜大模型下一步會產生什麼 token，猜對的 token 等於免費得到的。Together Atlas 在此基礎上多做了一件事：小模型會學習你的使用模式。你用得越久，它越了解你常問什麼、你的表達習慣是什麼，小模型的猜測準確率會隨時間提升，模型的回應速度也會越來越快。

## 硬體多元化：不只有 NVIDIA

Dan Fu 同時觀察到硬體生態正在快速多元化。AMD 正在推出 400 系列 GPU，軟體抽象層也在逐漸成熟，他在學術圈的合作者最近發布了一個叫 Hip Kittens 的函式庫，專門研究如何在 AMD GPU 上寫出高效的 kernel 程式。有趣的是，即使 AMD 和 NVIDIA 的 GPU 規格表看起來很像，底層的軟體抽象其實很不一樣，需要不同的程式設計方法。

更大的變化在推論端。訓練和推論是非常不同的計算任務，可能需要不同的晶片。推論端的計算可以發生在手機上、筆電上。Dan Fu 注意到他幾年前的 iPhone，運算能力已經超過他讀博士時用的 GPU。如果小模型的能力持續提升，在手機或筆電本地跑模型會變得越來越實用。

在架構層面，Dan Fu 認為 2026 年會看到更多樣化的模型架構。NVIDIA 最近釋出了一系列優秀的混合模型（hybrid model），結合了 Transformer 和狀態空間模型這兩種不同的「大腦結構」。Transformer 像是一個可以同時看到整篇文章所有字的讀者，非常強但很耗資源；狀態空間模型更像是一個快速瀏覽的讀者，從頭到尾掃一遍，效率很高但可能漏掉遠處的細節。混合兩者可以兼得優點。DeepSeek 的 MLA 壓縮借鑑了狀態空間模型的概念，MiniMax 的模型用了線性注意力。他特別注意到中國的 AI 實驗室在架構創新上更大膽。原因是中國沒有一家「中國的 OpenAI」那樣把產品、模型、收入全部整合在一起的公司，各家實驗室得靠差異化來競爭，架構創新就是其中一個手段。

## 開源的下一波躍進

Dan Fu 的另一個預測是，開源模型在 2026 年會再經歷一次能力大躍進。GLM 4.7 這類模型已經開始逼近當前最好的前沿模型了，他認為這只是開始。配合新硬體、更好的訓練技術、以及更多元的架構選擇，開源社群的上限會被大幅推高。

這會直接衝擊整個 AI 產業的權力結構。如果一個開源的 1000 億參數模型，經過專業化訓練後，在特定任務上能打平甚至超過 Anthropic 或 OpenAI 的 API，那企業就有更多理由把 AI 能力留在自己的基礎設施裡。成本更低、隱私更好、客製化程度更高。

## 我的觀察

這場對話裡最讓我關注的趨勢，用一句話總結就是：前沿模型在天花板附近互相追趕，真正的戰場正在往下移。

Dettmers 那個「以為自己在用 Opus 4.5 結果是 GLM 4.7」的故事，是一個非常有力的信號。當一個 AI 效率專家都分不出前沿商業模型和開源模型的差別時，「付費用最貴的模型」這件事的合理性就開始動搖了。

對臺灣的企業來說，2026 年可能是認真評估「自建 vs. 外購」的轉折點。AI2 即將發布的開源 coding agent 如果真能做到「指向你的程式碼庫就自動專業化」，那每一家有自己軟體系統的公司都應該認真測試。320 億參數的模型在一台工作站上就能跑，不需要雲端 API，程式碼不用出公司。

Together AI 的 Megakernels 和 Atlas 則代表了另一個趨勢：推論成本正在快速下降。Dan Fu 提到目前推論的硬體利用率不到 5%，這意味著光靠軟體最佳化就還有巨大的降價空間。對於任何正在計算 AI 應用 ROI 的企業來說，2026 年底的推論成本可能只有年初的三分之一甚至更低。

最後，硬體多元化和架構多元化是一體兩面的事。NVIDIA 在訓練端的主導地位短期內不會動搖，但推論端的競爭已經白熱化。AMD、Cerebras、TPU、甚至手機端的 NPU，都在爭奪這塊市場。架構方面，狀態空間模型、混合模型、線性注意力機制正在被更多實驗室認真對待。對臺灣的半導體和系統整合廠商來說，這是一個值得密切追蹤的技術分散化趨勢。
