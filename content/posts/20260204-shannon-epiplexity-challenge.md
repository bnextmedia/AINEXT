---
title: "當 Shannon 不夠用：六位頂尖研究者聯手挑戰七十年資訊理論"
date: 2026-02-04T10:00:00+08:00
description: "Shannon 資訊理論統治了通訊和計算科學超過七十年，但一篇來自 NYU 和 CMU 的新論文指出，它在 AI 時代有根本性的盲點。這群橫跨學界與 OpenAI、Anthropic 的研究者，提出了「epiplexity」框架來修補這個缺口。"
tags: ["Shannon", "epiplexity", "資訊理論", "Zico Kolter", "研究論文"]
categories: ["AI 技術前沿"]
image: "/images/posts/20260204-shannon-epiplexity-challenge.webp"
source_url: "https://arxiv.org/abs/2601.03220"
source_name: "arXiv"
related_companies: ["openai", "anthropic"]
related_people: []
draft: false
---

![封面圖](/images/posts/20260204-shannon-epiplexity-challenge.webp)

1948 年，Bell Labs 的數學家 Claude Shannon 發表了一篇叫做《通訊的數學理論》的論文，徹底改變了人類理解「資訊」的方式。Shannon 熵成了衡量資訊量的黃金標準，從電話線路的訊號壓縮到今天大型語言模型的損失函數，幾乎所有跟資訊處理有關的領域都建立在他的理論上。但七十多年後，一群 AI 研究者決定正面挑戰這套理論的基本假設。

2026 年 1 月發表在 arXiv 上的論文《From Entropy to Epiplexity》，作者來自紐約大學（NYU）和卡內基美隆大學（CMU），團隊成員同時橫跨 OpenAI 和 Anthropic。他們的核心主張很直接：Shannon 的資訊理論假設觀察者有無限的計算能力，但這個假設在 AI 時代行不通。他們提出了一個新框架叫 epiplexity（認知複雜度），用來衡量「計算有限的智慧體」能從資料中實際學到什麼。

## Shannon 的世界裡沒有計算限制

要理解這篇論文在挑戰什麼，得先回到 Shannon 理論的核心假設。

Shannon 熵衡量的是一個隨機變數的不確定性。它的數學形式簡潔，性質也很好用，但它有一個隱含的假設：觀察者可以執行任意複雜的計算。在這個假設下，資訊量只取決於機率分布本身，跟你用什麼方法去處理資料無關。一個密碼學上安全的偽隨機數產生器（CSPRNG）的輸出，在 Shannon 的框架裡跟真正的隨機數序列「看起來」幾乎一樣。如果你有無限的計算能力，你可以區分它們；但如果你沒有，它們就是一樣的。

這篇論文正好從這裡切入。在真實世界中，沒有任何系統有無限算力。人腦的神經元數量是有限的，GPU 叢集的 FLOPS 是有限的，訓練時間是有限的。當你引入計算限制之後，Shannon 理論的幾個基本性質就開始崩潰。

論文列出了三個具體的矛盾。第一，Shannon 理論說確定性轉換不能增加資訊量，但 AlphaZero 從確定性的棋盤規則中「創造」了大量的策略知識。第二，Shannon 熵跟資料的排列順序無關，但大型語言模型從左到右讀文本的學習效果明顯優於從右到左。第三，最大似然學習理論上只是在匹配訓練資料的分布，但實際上模型經常學會超出訓練分布的泛化能力。

這三個矛盾聽起來可能像是吹毛求疵，但它們指向一個根本性的問題：我們用來衡量「資訊」的標準工具，無法區分「對有限計算能力的學習者而言有用的模式」和「純粹的隨機雜訊」。

## Epiplexity：給有限算力的觀察者一個新度量

論文的核心貢獻是 epiplexity 這個概念。Epiplexity 來自 epistemic complexity，直譯是「認知複雜度」。

它的定義建立在「有時間限制的最小描述長度」上。傳統的最小描述長度原則（MDL）是 Kolmogorov 複雜度的實用版本：給定一組資料，找到最短的「程式 + 殘差」組合來描述它。Epiplexity 在這個基礎上加了一個限制：程式必須在給定的時間預算內執行完畢。

在這個框架下，一筆資料可以被分解成兩個部分。第一個部分是 epiplexity，也就是最佳壓縮模型本身的描述長度，代表資料中可被有限算力學習者提取的「結構性資訊」。第二個部分是 time-bounded entropy，也就是模型壓縮不了的剩餘部分，代表在那個計算預算內無法被預測的「隨機性」。

這個分解為什麼重要？因為同一筆資料，在不同的計算預算下，結構和隨機的比例會不同。給更多算力，模型能學到更多結構，epiplexity 上升，entropy 下降。一個密碼學上安全的偽隨機序列，對多項式時間的觀察者來說 entropy 幾乎是最大值、epiplexity 幾乎為零（看起來就是隨機的）。但如果你給觀察者指數級的計算時間，它就能看穿偽隨機的結構，entropy 驟降，epiplexity 飆升。

論文證明了幾個重要的數學性質。CSPRNG 的輸出在多項式時間限制下有接近最大值的 entropy 和接近零的 epiplexity，這跟直覺完全吻合。在假設單向函數存在的條件下，存在 epiplexity 至少以對數速率增長的隨機變數（高 epiplexity 確實存在）。而且整個框架跟 Shannon 熵之間有清楚的不等式關係：Shannon 熵永遠不超過 epiplexity 加上 time-bounded entropy。

## 三個悖論的統一解答

有了 epiplexity，前面提到的三個矛盾就各有各的解釋了。

AlphaZero 的情況：西洋棋規則的 Kolmogorov 複雜度很低（規則本身可以用很短的程式描述），但從規則推導出最優策略所需的計算量是天文數字。對多項式時間的觀察者來說，從規則到策略之間有巨大的 epiplexity 落差。AlphaZero 透過自我對弈，實質上是在用大量的計算來「提取」這些結構性資訊。確定性過程沒有在 Shannon 的意義上創造資訊，但它創造了 epiplexity，也就是對有限算力觀察者而言的可學習結構。

論文用細胞自動機的 Rule 110 做了另一個有力的示範。Rule 110 的規則極其簡單（一行就寫得完），但它產生的模式複雜到被證明是圖靈完備的。如果你有無限算力，你可以直接模擬 Rule 110 的每一步，不需要任何「學習」。但對有限算力的觀察者來說，這些模式包含大量可學習的結構，也就是高 epiplexity。

資料順序的問題：從左到右和從右到左讀英文文本，在 Shannon 的框架裡資訊量完全相同。但 epiplexity 不同。英文的語法和語義結構天然適合從左到右處理（主詞在前、動詞在中、受詞在後），有限算力的模型在這個方向上能更有效率地提取結構。這跟密碼學的不對稱性是同一個道理：正向計算容易、反向計算困難，不是因為資訊量不同，而是因為計算的複雜度不同。

模型泛化的問題：用高 epiplexity 的資料訓練，模型學到的壓縮表示包含更多可遷移的結構。真實西洋棋對局的 epiplexity 高於隨機棋盤位置，因為真實對局反映了策略邏輯、位置評估、時序規劃等結構。這些結構在棋謎中同樣適用，所以模型能泛化。而隨機棋盤位置的 epiplexity 低，模型學到的大多是表面的統計規律，沒有可遷移的深層結構。

## 橫跨學界與產業前線的作者群

要挑戰一個統治了七十年的理論，你需要足夠的學術分量。這篇論文的六位作者不只有分量，他們的組合本身就值得分析。

這是一場紐約大學（NYU）和卡內基美隆大學（CMU）兩個頂尖機器學習實驗室的聯手。NYU 這邊的帶頭人是 Andrew Gordon Wilson，Courant 數學科學研究所和資料科學中心的教授。Wilson 的研究風格很特別：他堅持用嚴謹的數學和物理訓練來理解深度學習，而不是只靠實驗去「試」。他的實驗室在貝氏深度學習和泛化理論上有大量的經典工作，包括損失函數表面的連通性研究、隨機權重平均（SWA）、以及被廣泛使用的 GPyTorch 高斯過程庫。他拿過 2022 年 ICML 最佳論文獎和 NSF CAREER Award，近幾年把研究重心放在 scaling laws 和大型語言模型的泛化行為上。他的 2026-2027 博士招生頁面上明確列出「epiplexity」作為實驗室的代表性研究方向，可以看出這篇論文對他來說不是副業，而是核心議程。

CMU 這邊的帶頭人 J. Zico Kolter 可能是六人中公眾知名度最高的。他是 CMU 機器學習系主任（這個系本身就是全球機器學習研究的聖殿之一），Google Scholar 引用超過 58,000 次。他在史丹佛大學拿到博士學位，之後在 MIT 做博士後，學術血統非常正統。他最有名的學術貢獻是對抗性穩健性：他的團隊最早開發出具有可證明穩健性保證的深度學習訓練方法，這在 AI 安全領域影響深遠。

但 Kolter 真正引起廣泛關注的是他的產業角色。他是 OpenAI 安全委員會（Safety and Security Committee）的主席和 OpenAI 非營利董事會成員。2025 年底 OpenAI 的公司重組中，加州和德拉瓦州的監管機構要求這個委員會擁有實質權力。Kolter 現在可以在認為模型不安全時暫停 OpenAI 的模型發布。美聯社、財星雜誌（Fortune）、US News、Wired 都對他做過專題報導。一個學術研究者同時握有世界上最大 AI 公司的「煞車權」，這讓他發表的每一篇論文都多了一層解讀空間：他關心什麼問題，某種程度上反映了 AI 安全社群認為什麼問題重要。他同時也是 Bosch AI 研究的首席科學家，跨足學界、產業和治理三個領域。

第一作者 Marc Finzi 是 Wilson 在 NYU 的博士畢業生。他的博士論文主題是如何把數學歸納偏差嵌入神經網路，包括對稱性、等變性、哈密頓力學等。他在 Harvey Mudd College 念物理學出身，之後在 Cornell 拿了作業研究碩士，這個背景讓他的研究帶有明顯的數學物理風味。他在 NeurIPS 2023 共同發表的「大型語言模型是零樣本時間序列預測器」引起了廣泛關注，因為它證明了 LLM 不需要任何時間序列資料的專門訓練，就能做出有競爭力的預測。博士畢業後他到 CMU 跟 Kolter 做博士後研究，epiplexity 論文正是他在兩個實驗室之間搭建合作的成果。

共同作者 Pavel Izmailov 的職涯軌跡可能是六人中最有故事性的。他也是 Wilson 實驗室的博士畢業生，跟 Wilson 一起拿了 2022 年 ICML 最佳論文獎。畢業後他先去了 OpenAI，參與了 o1 推理模型的開發，同期做的弱到強泛化（weak-to-strong generalization）研究被 Wired、MIT Technology Review、TechCrunch 等媒體廣泛報導。之後短暫待過馬斯克的 xAI，然後轉到 Anthropic 擔任研究科學家，參與了 Claude 3.7 Sonnet 和 Claude 4 的開發。現在他同時在紐約大學擔任助理教授，開始帶自己的博士生。不到三年走遍了 OpenAI、xAI、Anthropic 三家前線實驗室，同時維持學術研究的產出，這種橫跨度在 AI 領域非常罕見。他的研究涵蓋推理、可解釋性、AI 對齊，全都是現在最熱門的前沿方向。

兩位博士生共同作者也不是打雜的角色。Shikai Qiu 是 Wilson 的博四學生，拿了 Two Sigma 博士獎學金（這是量化金融界最著名的獎學金之一）。他在柏克萊（UC Berkeley）念物理和電腦科學時曾參與 CERN 的 ATLAS 實驗，這個合作獲得了 2025 年基礎物理突破獎。他 2024 年在 ICML 拿到 oral 的「Scaling Collapse」論文，發現在計算最優訓練下，不同規模模型的損失曲線會坍縮到同一條普適曲線上，這個發現對理解 scaling laws 有直接意義。他也在 Google Research、Google DeepMind、Amazon AWS、Meta AI 都做過研究實習。

Yiding Jiang 是 Kolter 在 CMU 的博士生，拿了 Google 博士獎學金。在進 CMU 之前，他在柏克萊念完電機和電腦科學學士後，去 Google Research 當了兩年 AI Resident。他的研究方向正好是「資料如何影響模型的行為」，2024 年在 ICLR 拿到 oral 的論文探討模型、資料和特徵之間的交互作用，2025 年在 ICLR 發表的「Adaptive Data Optimization」論文直接研究如何用 scaling laws 做動態樣本選擇。他的研究議程跟 epiplexity 這篇論文高度吻合，不是臨時拼湊的合作，而是長期耕耘同一個問題。

六位作者的合計引用數超過十萬次。但真正重要的不是這個數字，而是這個團隊的結構：Wilson 實驗室提供泛化理論的數學根基，Kolter 實驗室提供穩健性和安全的視角，Finzi 和 Izmailov 帶來 OpenAI 和 Anthropic 的第一手工程經驗，兩位博士生各自在 scaling laws 和資料選擇上有深入的研究。這不是一群象牙塔裡的理論家在寫論文，而是正在打造和守護最強 AI 系統的人，回頭去思考最基礎的問題。

## 從衡量不確定性到衡量可學習性

Shannon 在 1948 年解決的問題是：如何用最少的位元傳遞一個訊號？這個問題的答案改變了整個通訊產業。七十多年後的今天，AI 領域面對的問題不太一樣：不是「如何傳遞資訊」，而是「什麼資訊值得學習」。

Epiplexity 框架目前還在早期階段。估測方法的計算成本不低，大規模的實證驗證也還有限。但它指出了一個重要的方向：衡量資料價值的正確方式，不是看資料裡有多少「不確定性」（Shannon 熵），而是看有多少「可學習的結構」（epiplexity）。

這對 AI 開發的實務意義是：在計算預算固定的前提下，選對資料比堆更多資料重要。而「選對」現在有了一個數學上可以操作的定義，即使這個定義目前還需要更多計算才能精確估測。七十年前 Shannon 的論文改變了我們理解通訊的方式，這篇論文是否能改變我們理解學習的方式，還需要時間驗證。但它提出的問題本身，已經夠重要了。
