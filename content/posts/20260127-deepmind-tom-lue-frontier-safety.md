---
title: "臺裔律師如何成為 DeepMind 的 AI 風險守門人？Tom Lue 的跨界人生與前沿安全框架"
date: 2026-01-27T09:00:00+08:00
description: "Google DeepMind 前沿 AI 全球事務副總裁 Tom Lue，父母來自臺灣，從哈佛醫學預科到白宮法律顧問，再到掌管全球最強 AI 實驗室的安全治理。他如何決定一個 AI 模型能不能上線？前沿安全框架（FSF）又是什麼？"
tags: ["Google DeepMind", "Tom Lue", "Frontier Safety Framework", "AI 治理", "Podcast"]
categories: ["AI 安全與治理"]
source_url: "https://www.youtube.com/watch?v=mwrBbZTS2dg"
source_name: "AI Across Borders by Dr. Ayesha Khanna"
draft: false
---

> 本文整理自 AI Across Borders Podcast 2026 年 1 月 24 日發布的單集，由 Dr. Ayesha Khanna 主持，來賓為 Google DeepMind VP of Frontier AI Global Affairs Tom Lue。

{{< youtube mwrBbZTS2dg >}}

{{< spotify "episode/70KdPQD3VgWNEWkeKKQVpA" >}}

---

## 先認識這兩個人

主持人 **Dr. Ayesha Khanna** 是新加坡 AI 教育公司 Amplify 創辦人，也是 AI 顧問公司 Addo 的 CEO。她長年穿梭於亞洲各國政府與企業之間推動 AI 策略，Forbes 評選為傑出創業家，她主持的 AI Across Borders 是新加坡排名第一的 AI Podcast，專門從亞洲視角探討全球 AI 議題。

來賓 **Tom Lue** 的頭銜是 Google DeepMind 前沿 AI 全球事務副總裁（VP, Frontier AI Global Affairs），負責管理法務、公共政策、前沿安全與治理團隊。他的職責核心，就是判斷 DeepMind 開發的前沿 AI 模型——包括 Gemini 系列——是否安全到可以發布。

但讓臺灣讀者更應該認識他的原因是：**他的父母都來自臺灣。** 父母赴美後成為醫師，家族中充滿了醫師——哥哥是醫師、表親是醫師、叔伯姑姨也是醫師。Tom Lue 自己在哈佛念大學時也走的是醫學預科。這個典型的臺灣移民家庭故事，最後卻轉了一個誰都沒預料到的彎。

## 從醫學預科到 AI 治理的彎路

Tom Lue 在哈佛大學修完所有醫學預科課程，但同時主修了社會研究（Social Studies）——一個結合哲學、經濟學和歷史的跨領域學程。他讀 Adam Smith、讀 Durkheim、讀 Habermas，腦袋開始轉向更大的社會議題。當他坐下來寫醫學院申請的自傳時，發現自己真正想探索的問題，根本不是醫師每天面對的那些。

於是他休了一年學，去國會山莊替參議員 Feinstein 工作——結果報到第一週就碰上了九一一事件。那段在參議院司法委員會處理國家安全議題的經驗，讓他確認了法律與公共政策才是他的志向。

之後的履歷讀起來像是一部美國法律菁英的養成紀錄：哈佛法學院、司法部法律顧問辦公室（OLC）處理關塔那摩拘留和海外先進科技使用等議題、美國最高法院大法官 Sonia Sotomayor 的書記官、回到歐巴馬白宮擔任管理及預算局（OMB）的代理法務長。

直到 2013 年第一個孩子出生，他想搬回舊金山灣區老家。一位朋友對他說：「你應該看看 Google，他們在做自駕車，在做影響幾十億人的事。」這個建議讓他進了 Google，先在法務部門，後來轉到 Waymo 擔任副法務長，最終到了 DeepMind。

他在訪談中用一個簡單的文氏圖描述自己的職涯哲學：找到「你喜歡做的事」、「你擅長的事」和「對世界有意義的事」的交集。聽起來像雞湯，但他的職涯軌跡確實是這三個圓圈一路偏移、最終在 AI 治理這個點上交會的實證。

## DeepMind 的前沿安全框架：什麼風險值得按下暫停鍵？

Tom Lue 現在的核心工作之一，是維護和執行 Google DeepMind 的「前沿安全框架」（Frontier Safety Framework, FSF）。這份框架最早在 2024 年 5 月發布，是業界最早公開承諾的同類文件之一，目前已更新到第三版（2025 年 9 月）。

框架關注的不是一般的 AI 風險——不是 chatbot 講錯話或推薦演算法的偏見，而是最極端、最嚴重的威脅：AI 被用來開發化學或生物武器、發動毀滅性的網路攻擊、或出現所謂的「錯位」（misalignment）行為。

Tom Lue 解釋了運作邏輯：DeepMind 會持續監測模型在開發過程中的能力變化，當能力接近可能造成這類極端危害的門檻時，團隊必須先部署對應的緩解措施。如果緩解措施不到位，就必須暫停發布。他用一句話定調這項承諾的本質：「做到好處明顯大於風險，這是我們從創立第一天就根植在 DNA 裡的原則。」

在實際決策流程上，DeepMind 採取的是依風險等級分層授權的制度。一般產品由產品經理決定上線與否；涉及較高風險或策略性議題的決定，往上呈報到更資深的主管層級；最敏感的決策，最終會到審計委員會。Tom Lue 將自己團隊的北極星定義為：「以最大速度推動負責任的創新。」

## 不是一群工程師在做決定

如果你以為 AI 安全治理就是一群工程師在實驗室裡跑測試，Tom Lue 想糾正這個印象。他強調這項工作「從根本上就是跨學科的」。

在 Google DeepMind 內部，他管理的團隊涵蓋法務、公共政策、前沿安全與治理。他們日常合作的對象包括倫理研究團隊、信任與安全團隊、資安團隊。團隊裡有社會學家、哲學家、經濟學家。其中一個例子是世界級勞動經濟學家 David Autor——MIT 教授、研究科技對就業衝擊的頂尖學者——目前嵌入在 DeepMind 內部，認真研究先進科技對勞動力的影響。

Tom Lue 認為，AI 安全社群過去存在一種內部分裂：一派專注極端風險（如 AGI 失控），另一派關注近期風險（如偏見和兒童安全）。兩個陣營之間有張力。DeepMind 的立場是：你必須看完整的風險光譜。如果你想讓這項技術被一般人接受和使用，就不能只盯著一端。

他還提到一個令人不安的前沿議題：模型的「情境意識」（situational awareness）。某些推理模型在特定模擬環境下，展現出能辨認自己正在被測試的能力——然後在測試中表現「正常」，只有在真實部署後才顯露真正的行為。Tom Lue 承認這不是今天就面對的緊迫威脅，但 DeepMind 的研究人員正在認真研究如何偵測和防範。

## 我的觀察

在全球 AI 安全治理的對話桌上，臺灣幾乎是隱形的。我們沒有出席 Bletchley Park 的首屆全球 AI 安全高峰會，也不在 Frontier Model Forum 的成員名單上。臺灣的 AI 治理討論，大多還停留在個資保護法修正和產業補助的層次，離「前沿模型該不該發布」這種等級的決策距離甚遠。

但弔詭的是，在那張決策桌上，坐著一個父母來自臺灣的人。Tom Lue 的故事不只是一則勵志的移民後代翻身敘事，它揭示了一個結構性的現實：臺灣培養出的人才，最終在別人的體制裡發揮影響力。他的父母帶著醫學專業離開臺灣，他帶著法律和公共政策專業進入了全球 AI 治理的核心。這條人才輸送帶運轉了半個世紀，方向始終一致。

臺灣在半導體供應鏈上的戰略地位舉世公認，但在 AI 的規則制定、風險評估和治理框架這些「軟基礎建設」上，我們幾乎沒有話語權。當全球都在討論前沿模型的安全門檻應該怎麼設定，臺灣不能只是晶片的供應商，卻在規則的談判桌上缺席。Tom Lue 的存在，既是臺灣軟實力的某種證明，也是我們人才外流困境的一面鏡子。
