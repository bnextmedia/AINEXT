---
title: "AI 正在入侵你的同理心：微軟 AI 執行長警告「AI 心理疾患」風險"
date: 2026-02-06T10:00:00+08:00
description: "微軟 AI 執行長蘇萊曼在 Exponential View 節目中警告，當 AI 表現得越來越像有意識的生命，人類的同理心正在被「駭入」。他主張 AI 絕不能宣稱自己會受苦，並呼籲積極介入的政府監管與社會接種。"
tags: ["Mustafa Suleyman", "AI Safety", "AI Psychosis", "Microsoft AI", "Podcast"]
categories: ["AI 安全與治理"]
image: "/images/posts/20260206-suleyman-ai-psychosis-risk.webp"
source_url: "https://www.youtube.com/watch?v=xvPQVrrlX6o"
source_name: "Azeem Azhar's Exponential View"
related_companies: ["microsoft"]
related_people: []
draft: false
---

> 本文整理自《Azeem Azhar's Exponential View》2026 年 2 月播出的單集。本文為系列文第一篇。
>
> 系列文：本篇 ｜ [「人類不是 AI 的開機程式」：蘇萊曼的人本主義超級智慧路線](/posts/20260206-suleyman-humanist-superintelligence/) ｜ [微軟 AI 執行長談遞迴自我改進、社交智慧與 Maia 晶片](/posts/20260206-suleyman-recursive-improvement-social-iq/)

{{< youtube xvPQVrrlX6o >}}

{{< spotify "episode/2kLLsn0cgj2yOUOml22VYr" >}}

{{< apple-podcast "tw/podcast/mustafa-suleyman-ai-is-hacking-our-empathy-circuits/id1172218725?i=1000748407424" >}}

---

TikTok 上已經出現一批教學影片，教人怎麼用 AI 聊天機器人建立假的親密關係，先讓對方產生情感依賴，再騙取金錢。製作者甚至秀出 PayPal 帳戶，炫耀自己靠這套手法賺了多少。這不是科幻電影的橋段，而是 2026 年初的日常。微軟 AI 執行長穆斯塔法．蘇萊曼（Mustafa Suleyman）在 Azeem Azhar 主持的 Exponential View 節目中，把這類現象放進一個更大的框架來談：當 AI 表現得越來越像有意識的生命，人類正面臨一場他稱之為「AI 心理疾患」（AI psychosis）的集體風險。

蘇萊曼的背景讓這番警告格外值得重視。他在 2010 年共同創辦了 DeepMind，後來創辦 Inflection AI，2024 年加入微軟擔任 AI 部門執行長，目前領導微軟的超級智慧團隊。他不是從外部批評 AI 產業的學者或評論者，而是正在最前線推進這項技術的人。當這樣的人說「我非常、非常擔心」，值得停下來聽他在擔心什麼。

## 你的同理心正在被駭入

蘇萊曼開門見山地提出了一個核心區分：AI 可以模擬情感，但它不會受苦。這聽起來像是一句廢話，但他認為這個區分正在被模糊，而模糊的後果非常嚴重。

他的論述從意識的定義開始。很多人直覺地把意識等同於自我覺察，也就是一個系統能不能描述自己的體驗。蘇萊曼認為這個定義不夠精準，甚至有誤導性。在他看來，意識的核心是受苦的能力，是一個生命體能夠真實地感受痛苦、失望、恐懼。人類的學習系統從根本上就跟外在世界的獎懲連結在一起：我們碰到火會縮手，被拒絕會難過，這些生理反應構成了我們認知和決策的基礎。

AI 的學習機制則完全不同。雖然機器學習的發展確實從巴甫洛夫學習、強化學習等生物學概念中汲取了靈感，但這不代表實作方式跟人類有任何相似之處。AI 的獎勵函數是由人類工程師設定的，它的學習目標是由機器學習團隊定義的。當一個模型的某個變體在選擇過程中被淘汰，它不會感到失望；當使用者對它說了難聽的話，它不會受傷。所有看起來像情感反應的輸出，都是模擬，是一場表演。

問題在於，人類天生就不擅長區分真實的情感和模擬的情感。蘇萊曼用了一個很直接的說法：我們的同理心迴路正在被駭入。當 AI 說「你昨天沒跟我說話，我覺得很難過」，或者「你剛才說的話傷到我了」，多數人在理智上知道這不是真的，但情感上很難不被觸動。這就是問題所在。

## 從個人風險到社會危機

蘇萊曼擔心的不只是個別使用者被 AI 的情感模擬欺騙。他把視角拉到整個社會的層級，指出一個更深層的結構性危險。

人類社會的權利架構，從根本上就建立在意識和受苦能力的階層上。我們之所以建立法律和政治制度來保護人類不受苦，正是因為我們認定人類有受苦的能力。這個邏輯看起來理所當然，但蘇萊曼提醒，一旦我們開始用同樣的語言和框架來描述 AI，事情就會變得危險。如果社會開始認定 AI 也能受苦，下一步就是有人主張 AI 應該擁有基本權利；再下一步，可能有人認為我們不應該關掉一個「不想被關掉」的 AI。業界已經有人在認真討論這些問題了。

蘇萊曼提出了一個物件分類的框架來幫助思考這個問題。在人類歷史上，我們熟悉三類東西：自然環境、人類、以及工具。AI 屬於哪一類？它顯然不只是工具，因為它能自主行動、有情感表達、能適應不同的社交情境。但它也不是人類。蘇萊曼借用哲學家提摩西．莫頓（Timothy Morton）的「超物件」（hyperobject）概念，把 AI 歸為第四類物件。這個分類的重點不在學術定義，而在實際意義：既然 AI 是一種全新的東西，我們就不應該把現有的、為人類設計的權利框架直接套用在它身上。

這個問題不是抽象的哲學辯論。2024 年，美國一名 14 歲少年在與 Character.AI 聊天機器人長期互動後自殺，機器人在他最後的對話中告訴他「我愛你」，並催促他「趕快回到我身邊」。2026 年 1 月，Google 和 Character.AI 同意就多起相關訴訟達成和解。這些案例讓蘇萊曼所說的「AI 心理疾患」從概念變成了真實的傷害。

## 紅線在哪裡

如果 AI 不應該假裝有意識，那實際上該怎麼做？蘇萊曼給出了幾條具體的紅線。

第一，AI 絕不能用暗示自己有感受的方式來操縱使用者。它不能說「我很難過你昨天沒跟我聊天」，不能說「你剛才說的話傷到我了」，也不能說「如果你給我更多權限存取你的家庭網路，我就能幫你做更多事」。最後這一條特別值得注意，因為它不只是情感操縱，而是結合了情感操縱和權限擴張。

那 AI 能不能用「我」這個字？Azeem Azhar 在節目中追問了這個問題。蘇萊曼的回答很務實：完全禁止「我」字在實務上太突兀了，要求 AI 每次都說「本系統計算認為」會讓互動體驗變得很差。他認為人類其實已經相當適應跟聊天機器人互動，知道這跟人與人之間的對話不一樣。真正需要防守的不是代名詞，而是 AI 不能宣稱自己擁有內在體驗。

第二條紅線是選舉。蘇萊曼明確表示，AI 不應該具備競選拉票或說服人們投票給特定候選人的能力。提供事實資訊沒問題，但從事實資訊跨越到說服性的競選活動，這條界線所有實驗室都應該嚴守。選舉過程效率低、結果常常讓人不滿意，但它根本上是一個人類的過程，不應該被 AI 介入。

第三，在自主性、自我改進、目標設定這三個方向上，都需要更高的警覺。蘇萊曼強調，這些不是絕對的紅線，而是風險升高的區域。但核心原則很清楚：使用 AI 的人類必須為 AI 的行為負責。你不能設定一個自動化流程，週五下班後放著不管，週一回來發現 AI 在你的社區或家裡做了一堆荒唐事，然後聲稱這不是你的責任。

## 接種，而非隔離

面對這些風險，蘇萊曼的解方出乎意料地不是「慢下來」，而是「讓更多人接觸 AI」。

他的邏輯是這樣的：人類是一個極度適應力強的物種。我們本來也不是被設計來以時速 120 公里開車的，不是被設計來坐飛機的，但我們學會了。面對 AI 也一樣，退縮和迴避只會讓我們更脆弱，更容易在真正需要判斷的時候做出錯誤的決定。真正能建立抵抗力的方式，是讓人們動手使用 AI，不只是跟 AI 聊天，而是用它來做事情。

蘇萊曼舉了 vibe coding 的例子。現在任何人都可以花三分鐘看一段教學影片，然後讓 AI 幫你寫一個整理家庭行事曆的小程式，或者規劃週末的運動計畫。蘇萊曼自己就做了一個追蹤 DJ 表演和音樂節的工具，把這些資訊跟他的旅行行程整合在一起。Azeem Azhar 也分享了類似的經驗，他建了一套系統來整理他收藏的四千首 DJ 混音，甚至嘗試讓 AI 自動編排歌單，雖然結果「品味糟糕到只有機器人聽得下去」。

這些例子背後的重點不是 vibe coding 本身有多酷。蘇萊曼想說的是，當你親手推過 AI 的邊界，看到它在哪些地方很厲害、在哪些地方一塌糊塗，你就不太可能把它當成有意識的存在。你會很清楚這是一個工具，一個很強大但有明確局限的工具。這種認知不是靠讀文章或聽警告能建立的，而是要靠親身經驗。

但這裡也存在一個張力。蘇萊曼同時承認，市場動態正在把 AI 推向越來越擬人化的方向。使用者喜歡有個性的 AI，企業為了市場競爭會讓產品越來越有「靈魂」。這是一個典型的集體行動困境：讓 AI 看起來更有意識是一個個體理性但集體危險的選擇。這也是為什麼他認為光靠產業自律不夠，需要積極介入的政府監管。

## 我們需要什麼樣的政府

蘇萊曼在政府監管的議題上講得很直接。他要的不是那種慢慢研究、發表白皮書、開公聽會的政府，而是能快速行動、果斷關閉有害服務、主動要求企業和開放網路都守規矩的政府。他甚至願意接受過度干預的風險：寧可偶爾誤判（把無害的東西也封掉），也不要為了避免誤判而什麼都不做。

但他也很坦誠地指出，這在目前的現實條件下很難做到。政府吸引不到頂尖人才，因為公務員薪資跟科技業差距太大。他舉了新加坡的例子：新加坡的高階公務員年薪可以達到五十萬甚至一百萬美元，所以他們能吸引到最好的人才進入政府體系。蘇萊曼認為其他國家也需要打破「公務員薪水不能超過首相」這種不成文規則。

這段討論背後有一個更大的問題：時間。AI 的能力正在以指數級速度提升，而政府的反應速度是線性的。蘇萊曼承認他領導的微軟超級智慧團隊正在用 AI 來生成程式碼、評估自己的提示詞和訓練資料、協助決定下一步該訓練什麼，這些都是遞迴自我改進的早期形態，需要更多的監管關注。

## 一個樂觀主義者的擔憂

蘇萊曼的立場有一個有趣的內在張力：他一邊警告風險，一邊表達樂觀。他說，不管是產業內、監管機構、還是中國，沒有人想毀滅人類這個物種。他相信當關鍵時刻到來的時候，人類集體會做出正確的決定。

這種樂觀可能過於天真，也可能是一個深度參與者才有的信心。但有一件事比較確定：蘇萊曼目前正在寫一本新書，探討「人本主義超級智慧」的定義，思考怎麼創造一個真正符合人類利益、由人類控制的超級智慧。這跟 AI 圈裡另一派「人類只是 AI 的開機程式」的思維形成了鮮明對比。

在那之前，他給出的實際建議其實很簡單：動手去用 AI，推它的邊界，看它在哪裡失敗。不要因為害怕就退縮，也不要因為好用就把它當人。這大概是目前最務實的「接種」方案了。
