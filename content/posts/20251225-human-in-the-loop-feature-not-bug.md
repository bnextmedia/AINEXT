---
title: "Human in the Loop 不是臭蟲，是功能——為什麼 AI 全自動化的狂熱走錯了路"
date: 2025-12-25T15:00:00+08:00
description: "Klarna 高調宣稱 AI 取代 700 名客服，一年後全面撤回。前麥肯錫 QuantumBlack Labs 主管 Matt Fitzpatrick 認為，問題不在 AI 能力不足，而在於『全自動化』本身就是錯誤目標。在遞迴自我改進到來前，人類在迴路中不是過渡方案，而是讓 AI 真正可用的核心設計。"
tags: ["Human in the Loop", "AI 自動化", "RLHF", "Klarna", "Matt Fitzpatrick", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=7GFKB0oKd9A"
source_name: "Moonshots Podcast"
draft: false
---

> 本文整理自 Moonshots Podcast 2024 年 12 月播出的單集。
> 🎧 收聯連結：[YouTube](https://www.youtube.com/watch?v=7GFKB0oKd9A)

2024 年初，Klarna 的公關團隊一定很得意。他們的 AI 客服系統在第一個月處理了 230 萬通電話，等同 700 名全職客服的工作量，預計年省 4000 萬美元。這個案例迅速成為業界最常引用的 AI 成功故事，出現在無數的簡報和演講中。

然後，大約一年後，Klarna 宣佈全面撤回這套系統，重新回到人工客服。

「整個演變過程讓我很困惑，」Matt Fitzpatrick 在 Moonshots Podcast 上說。他曾在麥肯錫待了超過十年，領導 QuantumBlack Labs 的 AI 開發，現在是 Invisible Technologies 的 CEO。「因為你本來就不應該想要讓一切都變成代理人自動處理。你會希望在幾乎每一個產業、幾乎每一個主題上，都有人類在迴路中。」

## 從「全人工」到「全自動」再回到「全人工」的困惑

Klarna 的錯誤不在於使用 AI，而在於他們設定的目標——用 AI 完全取代人類客服。Fitzpatrick 分析，一個設計良好的客服系統應該是這樣運作的：首先有一個分類機制，判斷這通電話屬於哪種類型；接著有一套驗證規則，判斷哪些電話適合讓 AI 處理、哪些需要轉給人類；最後有明確的升級路徑，當 AI 遇到無法處理的情況時知道如何求助。

換句話說，你永遠不會想要一個「全 AI」的系統。你會想要一個「AI + 人類」的系統，然後隨著時間推移，不斷調整兩者之間的比例。

Klarna 的問題在於，他們跳過了這個設計階段，直接宣佈「我們要全面轉向代理人」。這在公關上很有噱頭，但在實務上卻埋下了禍根。當遇到 AI 處理不好的邊緣案例——複雜的退款爭議、需要寫回源系統的操作、情緒激動的客戶——整個系統沒有後備方案。

「從全人工到全代理人再回到全人工，」Fitzpatrick 說，「這個擺盪讓所有人都很困惑。」

## 80/20 法則在 AI 導入上更加殘酷

Podcast 的共同主持人 Dave Friedberg 提供了一個來自實際測試的數據：在他們測試的客服場景中，80% 的使用者明顯更喜歡 AI 客服——速度更快、不用等待、全天候服務。但那 20% 不喜歡的使用者，會「把整件事折騰到死，讓公司覺得乾脆全部撤回比較省事」。

這就是 AI 導入的殘酷現實。技術上可行的事情，在組織上和商業上可能是災難。80% 的成功率聽起來很高，但如果那 20% 的失敗會造成客訴、媒體負評、甚至法律糾紛，企業往往會選擇放棄整個計畫，而不是去解決那 20% 的問題。

Fitzpatrick 認為，「可能有八種方法可以快速修復這個問題」，但這些修復不會來自 Google 或 OpenAI。這些都是高度情境相關的問題——需要理解這家公司的特定產品、特定客戶群、特定處理流程。通用模型不會解決這些問題，企業需要自己建立解決方案，或者找專門的合作夥伴來幫忙。

這正是「人類在迴路中」的價值所在。不是因為 AI 不夠聰明，而是因為每一個企業場景都有其獨特的邊緣案例，而這些邊緣案例的解決需要人類的判斷、經驗和靈活性。

## RLHF 不會消失：Alex Wang 的挑戰與 Matt 的反駁

在 Podcast 中，Scale AI 的創辦人 Alex Wang 提出了一個尖銳的問題。他指出，隨著強化學習微調（Reinforcement Fine-tuning）等技術的出現，數據效率越來越高。如果模型可以透過遞迴自我改進變得越來越強，是否意味著人類標註數據（RLHF）的需求會逐漸消失？

Fitzpatrick 的回答很直接：「這個論點人們已經講了五年了。」

他的反駁有幾個層次。首先，當你在訓練一個需要多步推理的任務時，幻覺（hallucination）的風險非常高。在這種情況下，有人類反饋參與會讓結果更可靠。其次，RLHF 發生在預訓練之後，相對於預訓練的龐大運算成本，RLHF 只佔總成本的很小比例，但它提供的價值卻極高。第三，隨著越來越多的專業領域需要訓練專門的代理人——法律、醫療、金融、各種語言——每一個新領域都需要專業的人類回饋。

「如果你明天想訓練一個模型來判斷 17 世紀法國建築的不同風格演變，用法語，」Fitzpatrick 舉例，「你會需要 RLHF 來驗證它。」這不是通用模型可以自己解決的問題。

他進一步指出，即使在機器人技術、強化學習環境等新興領域，人類回饋依然扮演關鍵角色。「Human in the loop 會是一個功能，不是臭蟲，而且會持續很長很長的時間。」

## 當 AI 夠強時，人類的價值在哪裡？

Alex Wang 的問題背後有一個更深層的焦慮：如果 AI 真的在兩到三年內達到遞迴自我改進，人類專家還有什麼價值？

Fitzpatrick 的觀點是：人類專家的價值不會消失，只會轉移。

以銷售為例。在一個「500 家公司都在賣 AI 銷售代理人」的世界裡，人與人之間的真實互動反而變得更有價值。最好的銷售員之所以成功，不是因為他們記住了更多產品規格，而是因為他們能建立信任、讀懂客戶的言外之意、在關鍵時刻做出正確判斷。這些能力目前沒有訓練數據可以讓 AI 學習，因為它們存在於人的經驗和直覺中，沒有被系統性地記錄下來。

類似的邏輯適用於許多專業領域。石油與天然氣產業的地震工程師、房地產產業的交易判斷專家、銀行業的複雜信貸決策者——這些角色涉及的不只是「查閱資訊並做出決定」，而是在高度不確定的情況下，結合多年經驗做出無法完全量化的判斷。

Fitzpatrick 提到一個有趣的統計：在美國過去一百年的歷史中，大約 25% 的高中畢業生會進入一個在他們讀高中時還不存在的行業。這不是 AI 時代特有的現象，而是技術進步的常態。行業會消失，新行業會出現，人類會適應。

他舉了社群媒體為例。五、六年前，付費媒體作為一個產業正在掙扎。但在大型語言模型出現後的這幾年，Substack、Medium 等平台讓付費內容重新變得有價值。現在有 9% 的美國公民是全職社群媒體創作者。這是一個二十年前根本不存在的職業類別。

## 這不是 AI vs 人類，是 AI + 人類 vs 問題

整場討論最後匯聚到一個核心觀點：把問題框架為「AI 取代人類」是錯誤的。正確的框架是「AI + 人類」作為一個整合系統，去解決以前無法解決或成本過高的問題。

Klarna 的失敗不是因為他們用了 AI，而是因為他們把 AI 當作「取代人類的工具」而不是「增強人類能力的工具」。如果他們的目標是「讓客服團隊能用更少的人處理更多的案件，同時維持甚至提升服務品質」，整個設計和實施路徑會完全不同。

這對企業領導者來說是一個重要的心態轉換。不要問「這個 AI 能取代多少人？」——這個問題會引導你走向 Klarna 的老路。要問的是「如果我的團隊每個人都有一個 AI 助手，他們能多做什麼以前做不到的事？」

Fitzpatrick 的公司 Invisible Technologies 正是基於這個理念運作。他們不只是賣 AI 解決方案，而是提供「AI + 專業人力」的整合服務。客戶付費的不是技術，而是成果——省下的成本、提升的效率、完成的任務。這個商業模式本身就體現了「人類在迴路中」的哲學。

遞迴自我改進可能在兩三年內到來，AGI 可能在五到十年內實現。但在那之前——甚至在那之後——把人類完全排除在外的想法，既不切實際，也不是最優解。AI 是工具，人類是使用工具的人。最強大的系統，是那些把兩者結合得最好的系統。
