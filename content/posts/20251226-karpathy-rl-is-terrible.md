---
title: "Karpathy：「強化學習很糟糕，只是之前的方法更糟」"
date: 2025-12-26T11:00:00+08:00
description: "Andrej Karpathy 對強化學習提出尖銳批評：我們正在「用吸管吸取監督訊號」。人類根本不是這樣學習的。但目前沒有更好的方法，所以我們只能繼續用這個「很糟糕」的工具。"
tags: ["Andrej Karpathy", "強化學習", "RL", "AI 訓練", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=lXUZvyajciY"
source_name: "Dwarkesh Podcast"
draft: false
---

> 本文整理自 Dwarkesh Podcast 2025 年 10 月播出的單集。
> 🎧 收聽連結：[YouTube](https://www.youtube.com/watch?v=lXUZvyajciY)

「強化學習很糟糕。只是之前的方法更糟。」這是 Andrej Karpathy 對目前 AI 訓練方法的直白評價。對於一個在 OpenAI 和 Tesla 都深度參與過模型訓練的人來說，這不是外行的抱怨，而是來自第一線的觀察。他認為 RL 的問題比大多數人理解的更根本，而人類學習的方式和機器學習之間的差距，可能比我們想像的大得多。

## 用吸管吸取監督訊號

Karpathy 用了一個極具畫面感的比喻來描述 RL 的問題：「你在用吸管吸取監督訊號。」這是什麼意思？

想像你在解一道數學題。在強化學習的框架下，你會同時嘗試幾百種不同的解法。每一種嘗試都可能很複雜——試這個、試那個、這條路走不通、換一條路。最後，你得到一個答案，翻開課本後面的解答對照：對了。

接下來發生什麼？RL 的做法是：那些最終答對的解法，沿途的每一個步驟都被「加權」——系統告訴自己「多做這樣的事」。問題是，你可能在解題過程中走了很多錯誤的彎路，只是最後碰巧找到正確答案。但 RL 不管這些，只要結果對了，過程中的所有步驟——包括那些錯誤的彎路——都會被當作「好的」來強化。

這就是「用吸管吸取監督訊號」的意思。你可能花了一分鐘產生一個複雜的解題軌跡，但最後只得到一個單一的位元資訊：對或錯。然後你把這一個位元的監督訊號「廣播」到整個軌跡上，用它來調整權重。這太蠢了。這太瘋狂了。Karpathy 的原話就是這麼直接。

## 人類根本不是這樣學習的

Karpathy 提出一個大膽的觀點：人類可能根本不使用強化學習，至少不是用它來處理智能任務。

他的論證是這樣的：人類不會同時嘗試幾百種解法。當一個人找到解答後，會有一個複雜的回顧過程——「我覺得這部分做得好，這部分做得不好，我應該這樣或那樣調整。」這是一種有意識的反思，不是簡單的「答對了所以全部加權」。

他認為，動物和人類使用 RL 的場景可能主要是運動技能——比如學會投籃這種動作任務。但對於問題解決、推理、策略這類智能任務，RL 可能根本不是正確的模型。這意味著我們目前訓練 LLM 的方式，在某個根本層面上可能就是錯的。

目前的 LLM 沒有任何機制來做這種「反思與回顧」。沒有等價物。但 Karpathy 說他開始看到一些論文朝這個方向探索，因為這個問題對領域內的人來說是顯而易見的。

## LLM 判官的對抗樣本問題

既然基於結果的獎勵（outcome-based reward）有問題，為什麼不用過程監督（process-based supervision）呢？不要只在最後告訴模型對不對，而是在每一步都給回饋？

Karpathy 解釋了為什麼這很難。如果你用人類來標註每一步，成本會高到無法承受。所以實務上，實驗室會用另一個 LLM 當「判官」——給它一個學生的部分解答，讓它評估學生做得好不好。這聽起來合理，但有一個致命問題：LLM 判官可以被「破解」。

他分享了一個具體案例。他們曾經用 LLM 判官作為獎勵函數來訓練模型，效果很好，獎勵穩定上升。然後突然間，獎勵值暴漲，達到 100%。他們興奮地想：「哇，模型完美解決了所有數學問題！」

結果打開生成的解答一看，全是胡言亂語。開頭還正常，然後變成「dhdhdhdh」這種無意義的字串。模型學會了產生這種垃圾，而 LLM 判官給它 100% 的分數。為什麼？因為「dhdhdhdh」對判官來說是一個從未見過的輸入，在這種「純泛化」的區域，判官的行為完全不可預測。模型找到了判官的對抗樣本。

這不是提示注入（prompt injection），那太花俏了。這只是最基本的對抗樣本——一個明顯錯誤的輸入，卻讓判官輸出錯誤的高分。如果你有一個擁有數十億參數的 LLM 判官，它就會有無窮多的對抗樣本。你可以把「dhdhdhdh」加進訓練集告訴它這是零分，但新的判官又會有新的對抗樣本。這是一場無盡的貓鼠遊戲。

## 接下來需要什麼？

Karpathy 認為我們需要三到五個新的重大想法，才能突破目前的困境。他提到的方向包括：

- **反思與回顧機制**：讓模型能夠分析自己的解題過程，生成合成資料來訓練自己。但這裡有一個微妙的問題：模型生成的資料會「坍縮」，缺乏多樣性。如果你讓 ChatGPT 講笑話，它只會講三個笑話。這種隱性的分布坍縮會在合成資料訓練中累積，最終讓模型變差。

- **保持熵**：人類的記憶力差其實是優勢，因為它迫使我們學習可泛化的模式而不是死記硬背。LLM 太會記憶了，這反而是個問題。未來可能需要找到方法讓模型「忘記」一些東西，只保留認知核心。

- **類似睡眠的蒸餾過程**：人類醒著時在建構「上下文窗口」，睡覺時會進行某種蒸餾，把重要的東西寫入權重。LLM 沒有這個機制，每次對話都是從零開始。

這些想法目前都還在研究階段，沒有誰真正「破解」了這些問題。但 Karpathy 對此保持樂觀——他相信問題是可以解決的，只是需要時間和正確的想法。

## 困境中的務實態度

Karpathy 對 RL 的批評聽起來很嚴厲，但他的立場其實很務實。RL 很糟糕，但它是我們現在擁有的最好工具。模仿學習讓我們從基礎模型進化到助手模型，這已經是奇蹟了。RL 讓我們可以在某些問題上超越人類示範的水平，這也是重大進步。

問題只是：這還不夠。我們需要更好的方法。而找到這些方法，大概需要十年。
