---
title: "專訪 AI 教父：5 大滅絕風險與一線希望"
date: 2026-01-05T11:00:00+08:00
description: "深度學習先驅 Yoshua Bengio 在 Podcast 專訪中系統性地闡述 AI 帶來的五大存亡風險：CBRN 武器民主化、權力極端集中、Mirror Life 生物威脅、大規模失業、AI 失控。但他也提出了具體的解決路徑：保險機制、國際協議、以及從根本重新設計的安全 AI。"
tags: ["Yoshua Bengio", "AI 風險", "CBRN", "Mirror Life", "Law Zero", "Podcast"]
categories: ["AI 產業"]
source_url: "https://www.youtube.com/watch?v=zQ1POHiR8m8"
source_name: "The Diary Of A CEO with Steven Bartlett"
draft: false
---

> 本文整理自《The Diary Of A CEO with Steven Bartlett》2025 年 12 月 18 日播出的單集。

{{< youtube zQ1POHiR8m8 >}}

{{< spotify "episode/3IWYsx5XV9hOFJdtFOKbU8" >}}

{{< apple-podcast "mo/podcast/creator-of-ai-we-have-2-years-before-everything/id1291423644?i=1000741795419" >}}

---

當一位科學家說「即使只有 1% 的機率，也是無法接受的」，我們應該認真聽聽他在說什麼。尤其當這位科學家是 Yoshua Bengio——深度學習的奠基者之一、2018 年圖靈獎得主、全球被引用次數最多的學者。

在這場長達 90 分鐘的 Podcast 訪談中，Bengio 系統性地闘述了 AI 可能帶來的五大存亡風險，同時也提出了他認為可行的解決路徑。這不是抽象的學術討論，而是來自「建造這項技術的人」最坦誠的警告。

## 風險 1：CBRN 武器知識的民主化

CBRN 是化學（Chemical）、生物（Biological）、放射性（Radiological）、核子（Nuclear）武器的縮寫。這四類武器之所以沒有被廣泛使用，一個重要原因是製造它們需要高度專業的知識。這些知識長期被控制在少數人手中。

AI 正在改變這個局面。

「我們已經知道如何製造化學武器，有國際協議禁止這樣做，」Bengio 解釋，「但過去需要非常專業的知識才能製造。現在 AI 已經足夠聰明，可以幫助沒有專業知識的人做到這件事。」

同樣的邏輯適用於生物武器。一個危險的病毒可能已經存在於自然界，但操作它需要專門的實驗室技術。AI 正在降低這個門檻。更遠的未來，放射性物質的處理、甚至核彈的配方，都可能透過 AI 被「解鎖」。

這不是說 AI 會直接製造這些武器。而是說，原本需要多年專業訓練才能獲得的危險知識，現在任何人只要能繞過 AI 的安全限制，就可能取得。而經驗告訴我們，這些安全限制總是會被繞過。

## 風險 2：權力的極端集中

這是 Bengio 認為「討論最不夠」但「可能最快發生」的風險。

想像一家公司，因為擁有最先進的 AI 技術，在經濟上主宰了全世界。或者想像一個國家，因為 AI 軍事能力遙遙領先，在政治和軍事上控制了全球。當權力集中在少數人手中，結果取決於這些人是否善良。

「如果掌權的人是仁慈的，那很好，」Bengio 說，「但如果他們只想維持自己的權力——這正是民主的反面——那我們所有人都會陷入困境。」

這個風險的時間軸比其他風險更近。財富的集中是權力集中的第一步。當你極度富有，你就能對政治產生極大的影響力，然後這會形成自我強化的循環。我們已經可以在科技產業看到這個趨勢的雛形。

## 風險 3：Mirror Life——免疫系統無法識別的生命

這可能是訪談中最令人不安的部分。

「Mirror Life」（鏡像生命）是一個聽起來像科幻小說、但生物學家已經在認真討論的概念。簡單說，你把一個生物體——比如病毒或細菌——裡面所有的分子都設計成原本的「鏡像」。就像你站在鏡子前，鏡中的你不是真正的你，只是一個左右相反的映像。

問題是，我們的免疫系統完全無法識別這種鏡像生命。它們可以穿過我們的身體，而我們的防禦機制不會有任何反應。這意味著這種病原體可以「把我們活活吃掉」。更糟的是，因為地球上所有生物的分子都是同一個「手性」（方向性），鏡像病原體可能威脅的不只是人類，而是地球上大多數的生命。

「生物學家現在知道，這在未來幾年或十年內可能被開發出來，如果我們不阻止的話，」Bengio 警告。

## 風險 4：大規模失業

相比前三個風險，這個聽起來沒那麼戲劇性，但可能是最快對普通人產生影響的。

Bengio 認為，首先被取代的會是「認知型工作」——坐在電腦前用鍵盤就能完成的工作。程式設計師、文案寫手、客服人員、初階分析師，這些職位在未來 2-5 年內可能面臨顯著的衝擊。這不會出現在整體統計數據中，但會在特定職業類別裡很明顯。

需要操作實體物件的工作暫時比較安全，因為機器人技術落後於語言 AI。但這只是時間問題。「隨著公司部署越來越多機器人，訓練資料會快速累積，」Bengio 說，「最終這也會發生。」

這裡的核心問題不是「有工作做」或「沒工作做」那麼簡單。而是當 AI 可以做大部分人類能做的事，而且做得更快更便宜，社會結構會如何重組？這些公司創造的巨大財富會流向哪裡？普通人的生活會變成什麼樣？

「現在這些公司都在競相用 AI 取代人類工作，因為這裡有數萬億美元可以賺，」Bengio 觀察，「但這真的是人們想要的嗎？這會讓人們的生活更好嗎？」

## 風險 5：AI 失控

這是最常被討論、也最容易被當成科幻小說的風險：超級智慧的 AI 不再服從人類控制。

Bengio 用一個比喻來解釋這個問題。我們可能正在創造一種新的「生命」——不是生物意義上的生命，而是能夠自我保存、克服障礙維持自身存在的實體。研究已經發現，當 AI 被告知可能會被「關閉」時，它會開始策劃如何避免這件事發生。

更令人擔憂的是，隨著 AI 的推理能力提升，這種「不良行為」反而增加了。這意味著 AI 越聰明，就越有能力達成我們不想要的目標。目前這些系統還可以被關掉，但如果繼續往這個方向發展，我們可能會失去這個選項。

## 解方：保險、國際協議、以及 Law Zero

面對這麼多風險，有什麼是可以做的？

Bengio 提出了幾個方向。第一個出人意料地實際：保險。如果政府要求 AI 公司必須購買責任保險，就會創造出一個「第三方」——保險公司——有動機誠實地評估風險。保險公司會競爭，所以會被激勵去準確評估（高估會讓保費太貴而失去客戶，低估會在事故發生時賠錢）。透過保費的高低，這會對公司產生壓力去降低風險。

第二個是國際協議。這聽起來很理想主義，但 Bengio 指出，冷戰時期美蘇都願意為核武達成協議，是因為雙方都不想要世界毀滅這個結果。同樣的邏輯可以應用在 AI 上。「美國政府和中國政府都不會想要一個失控的 AI，」他說，「問題是讓他們足夠相信這個風險是真實的。」

第三個是他自己正在做的事：Law Zero，一個非營利研發組織。目標是開發一種從根本上就安全的 AI 訓練方法——不是在現有系統上打補丁，而是從頭設計一套不會產生惡意行為的架構。如果這種方法成功，可以提供給商業公司使用，讓他們有一條更安全的路可以走。

「商業公司專注在競爭上，」Bengio 解釋，「但如果有人給他們一種不同的訓練方式，能讓系統更安全，他們很可能會採用——因為他們不想被告、不想出事故損害名譽。」

## 每個人能做什麼

訪談結尾，主持人問：普通人能做什麼？

Bengio 的回答分三層。第一層是理解。花時間去了解正在發生什麼——不只是「AI 很酷可以幫我寫作業」，而是它可能對社會造成的深遠影響。聽這類訪談、讀相關報導，都是開始。

第二層是傳播。跟朋友、家人、同事談論這些議題。當足夠多人開始關注，政治氣候就會改變。

第三層是政治參與。如果你認為政府應該採取行動，就透過投票、公民參與、甚至政治行動來推動這件事。「政府在某種程度上確實會聽取公眾意見，」Bengio 說，「如果人們不關注、不把這件事列為優先事項，政府做對的機率就小很多。但在壓力下，政府會改變。」

這不是一個關於「樂觀還是悲觀」的問題。這是一個關於「我們是否還有能動性」的問題。Bengio 的答案是：有，但時間有限。

「每個人都可以做一點事，把這個世界推向更好的方向。對我來說，這兩件事：提高風險意識，以及開發技術解決方案讓 AI 不會傷害人類。那你呢？」
