<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI 訓練 on AINEXT</title>
    <link>https://bnextmedia.github.io/AINEXT/tags/ai-%E8%A8%93%E7%B7%B4/</link>
    <description>Recent content in AI 訓練 on AINEXT</description>
    <generator>Hugo</generator>
    <language>zh-TW</language>
    <lastBuildDate>Fri, 26 Dec 2025 11:00:00 +0800</lastBuildDate>
    <atom:link href="https://bnextmedia.github.io/AINEXT/tags/ai-%E8%A8%93%E7%B7%B4/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Karpathy：「強化學習很糟糕，只是之前的方法更糟」</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251226-karpathy-rl-is-terrible/</link>
      <pubDate>Fri, 26 Dec 2025 11:00:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251226-karpathy-rl-is-terrible/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Dwarkesh Podcast 2025 年 10 月播出的單集。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/lXUZvyajciY&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;「強化學習很糟糕。只是之前的方法更糟。」這是 Andrej Karpathy 對目前 AI 訓練方法的直白評價。對於一個在 OpenAI 和 Tesla 都深度參與過模型訓練的人來說，這不是外行的抱怨，而是來自第一線的觀察。他認為 RL 的問題比大多數人理解的更根本，而人類學習的方式和機器學習之間的差距，可能比我們想像的大得多。&lt;/p&gt;&#xA;&lt;h2 id=&#34;用吸管吸取監督訊號&#34;&gt;用吸管吸取監督訊號&lt;/h2&gt;&#xA;&lt;p&gt;Karpathy 用了一個極具畫面感的比喻來描述 RL 的問題：「你在用吸管吸取監督訊號。」這是什麼意思？&lt;/p&gt;&#xA;&lt;p&gt;想像你在解一道數學題。在強化學習的框架下，你會同時嘗試幾百種不同的解法。每一種嘗試都可能很複雜——試這個、試那個、這條路走不通、換一條路。最後，你得到一個答案，翻開課本後面的解答對照：對了。&lt;/p&gt;&#xA;&lt;p&gt;接下來發生什麼？RL 的做法是：那些最終答對的解法，沿途的每一個步驟都被「加權」——系統告訴自己「多做這樣的事」。問題是，你可能在解題過程中走了很多錯誤的彎路，只是最後碰巧找到正確答案。但 RL 不管這些，只要結果對了，過程中的所有步驟——包括那些錯誤的彎路——都會被當作「好的」來強化。&lt;/p&gt;&#xA;&lt;p&gt;這就是「用吸管吸取監督訊號」的意思。你可能花了一分鐘產生一個複雜的解題軌跡，但最後只得到一個單一的位元資訊：對或錯。然後你把這一個位元的監督訊號「廣播」到整個軌跡上，用它來調整權重。這太蠢了。這太瘋狂了。Karpathy 的原話就是這麼直接。&lt;/p&gt;</description>
    </item>
    <item>
      <title>從資料標註到「養育人類的孩子」——AI 訓練的真相</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-raising-humanitys-children/</link>
      <pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-raising-humanitys-children/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Lenny&amp;rsquo;s Podcast 與 Surge AI 創辦人 Edwin Chen 的訪談。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;div class=&#34;media-embed&#34;&gt;&#xA;  &lt;div class=&#34;video-container&#34;&gt;&#xA;    &lt;iframe &#xA;      src=&#34;https://www.youtube.com/embed/dduQeaqmpnI&#34; &#xA;      allowfullscreen &#xA;      title=&#34;YouTube Video&#34;&#xA;      loading=&#34;lazy&#34;&#xA;    &gt;&lt;/iframe&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&lt;style&gt;&#xA;.media-embed {&#xA;  max-width: 100%;&#xA;  margin: 1rem 0;&#xA;}&#xA;&#xA;.video-container {&#xA;  position: relative;&#xA;  padding-bottom: 56.25%;&#xA;  height: 0;&#xA;  overflow: hidden;&#xA;}&#xA;&#xA;.video-container iframe {&#xA;  position: absolute;&#xA;  top: 0;&#xA;  left: 0;&#xA;  width: 100%;&#xA;  height: 100%;&#xA;  border: 0;&#xA;  border-radius: 12px;&#xA;}&#xA;&lt;/style&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;說到 AI 訓練，很多人腦中浮現的畫面是：一群人坐在電腦前，在貓的照片上標註「這是貓」，在狗的照片周圍畫框框。這種工作聽起來很無聊、很機械、很容易被取代。&lt;/p&gt;&#xA;&lt;p&gt;Edwin Chen 是 Surge AI 的創辦人，這家公司為所有主要 AI 實驗室提供訓練資料。他討厭「資料標註」這個詞。&lt;/p&gt;&#xA;&lt;p&gt;「資料標註讓人想到這種簡單工作——標註貓的照片、畫 bounding box。我們做的完全不同。」他說。「我覺得我們做的事情更像是養育孩子。養小孩不只是餵他吃東西。你在教他價值觀、創造力、什麼是美、無數關於什麼讓一個人變好的微妙事情。我們對 AI 做的就是這件事。」&lt;/p&gt;&#xA;&lt;p&gt;這個比喻不是誇張。如果你仔細理解現代 AI 是怎麼訓練出來的，你會發現這確實比「標註」複雜得多。&lt;/p&gt;&#xA;&lt;h2 id=&#34;不只是教會模型答案&#34;&gt;不只是「教會模型答案」&lt;/h2&gt;&#xA;&lt;p&gt;每一個你用過的大型語言模型——ChatGPT、Claude、Gemini——都經歷過一個叫「後訓練」（post-training）的階段。這個階段發生在模型已經從網路上學會大量文字知識之後。&lt;/p&gt;</description>
    </item>
    <item>
      <title>為什麼 Claude 寫程式碼這麼強？訓練 AI 的內幕人士揭露答案</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-why-claude-is-better/</link>
      <pubDate>Wed, 24 Dec 2025 01:52:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-why-claude-is-better/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Lenny&amp;rsquo;s Podcast 與 Surge AI 創辦人 Edwin Chen 的訪談。&#xA;收聽連結：&lt;a href=&#34;https://www.youtube.com/watch?v=dduQeaqmpnI&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;過去一年多，有個現象讓很多人困惑：Claude 在寫程式和寫作上，為什麼能領先其他模型這麼久？&lt;/p&gt;&#xA;&lt;p&gt;幾乎所有 AI 程式開發工具——Cursor、Windsurf、各種 coding agent——都把 Claude 當作首選模型。不是因為行銷，是因為它實際用起來就是比較好。這很奇怪，因為考慮到程式碼能力的經濟價值有多大，你會預期其他實驗室會很快追上來。OpenAI 的資源更多，Google 的資料更多，為什麼 Anthropic 一家相對小的公司能在這麼重要的能力上保持優勢？&lt;/p&gt;&#xA;&lt;p&gt;Edwin Chen 是 Surge AI 的創辦人，這家公司為所有主要 AI 實驗室提供訓練資料。四年內做到 10 億美元營收，靠的就是對「什麼讓 AI 變好」有獨到的理解。最近一次訪談中，他分享了一個不常被討論的答案：taste（品味）。&lt;/p&gt;&#xA;&lt;h2 id=&#34;不只是更多資料這麼簡單&#34;&gt;不只是「更多資料」這麼簡單&lt;/h2&gt;&#xA;&lt;p&gt;很多人以為 AI 模型的差異就是資料量的差異。誰有更多資料，誰就會更強。但 Edwin 認為這完全搞錯了問題的本質。&lt;/p&gt;&#xA;&lt;p&gt;「人們不理解的是，所有前沿實驗室在訓練模型時，面對的選擇幾乎是無限多的。」他解釋道。你要用純人類資料嗎？蒐集資料的方式是什麼？你要求產出資料的人具體創造什麼內容？在程式碼領域，你更在乎前端還是後端？如果是前端，你更在乎視覺設計，還是執行效率，還是純粹的正確性？要混入多少合成資料？要針對哪些 benchmark 優化？&lt;/p&gt;&#xA;&lt;p&gt;這些決策不是工程問題，而是品味問題。就像問「什麼是好的視覺設計」，不同人會有不同答案。有人在乎極簡主義，有人喜歡 3D 動畫效果，有人偏好復古風格。這些偏好會滲透到訓練資料的每一個選擇中，最終塑造出模型的「性格」。&lt;/p&gt;&#xA;&lt;p&gt;Edwin 用一個精準的說法來描述這件事：「後訓練（post-training）幾乎是一門藝術，不純粹是科學。當你決定要打造什麼樣的模型、它擅長什麼，這裡面有品味和精緻度（sophistication）的概念。」&lt;/p&gt;&#xA;&lt;h2 id=&#34;諾貝爾獎等級的詩-vs-勾選清單&#34;&gt;諾貝爾獎等級的詩 vs 勾選清單&lt;/h2&gt;&#xA;&lt;p&gt;為了說明「品味」如何影響資料品質，Edwin 舉了一個例子。&lt;/p&gt;&#xA;&lt;p&gt;假設你要訓練模型寫一首關於月亮的八行詩。什麼叫「好」？如果你不深入思考品質，檢查方式會是：這是詩嗎？有八行嗎？提到月亮嗎？這些條件都符合，那就是好詩。&lt;/p&gt;&#xA;&lt;p&gt;「但這跟我們要的完全不同。」Edwin 說。「我們要的是諾貝爾獎等級的詩。這首詩獨特嗎？有細膩的意象嗎？會讓你驚喜、觸動你的心嗎？會教你一些關於月光本質的事情嗎？會玩弄你的情緒、讓你思考嗎？」&lt;/p&gt;&#xA;&lt;p&gt;這就是差別所在。某些公司，你問他們什麼是好詩，他們會機械式地檢查一堆條件。符合指令，就是好詩。但那不是好詩。有品味和精緻度的實驗室會意識到，品質無法簡化成一組固定的勾選清單，他們會去考慮那些隱晦的、微妙的特質。&lt;/p&gt;&#xA;&lt;p&gt;這種思維差異會體現在一切地方。當你在選擇程式碼訓練資料時，你是要能跑的程式碼，還是優雅的程式碼？你是要符合規格的程式碼，還是考慮到邊界情況、有好的錯誤處理、註解清楚、結構乾淨的程式碼？這些選擇會累積，最終決定模型的水準。&lt;/p&gt;&#xA;&lt;h2 id=&#34;anthropic-做對了什麼&#34;&gt;Anthropic 做對了什麼&lt;/h2&gt;&#xA;&lt;p&gt;被問到哪家實驗室做得最好時，Edwin 明確表示他對 Anthropic 的印象最深刻。&lt;/p&gt;&#xA;&lt;p&gt;「我一直覺得 Anthropic 對於他們在乎什麼、不在乎什麼，以及他們希望模型如何表現，有非常有原則的看法。」他說。這種「有原則」（principled）是關鍵詞——它意味著 Anthropic 不是隨波逐流，不是看到什麼 benchmark 熱門就往那個方向優化，而是有一套清晰的價值觀來指導決策。&lt;/p&gt;</description>
    </item>
    <item>
      <title>訓練 ChatGPT 的公司，給 AI 時代創業者的一堂課</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-startup-lesson/</link>
      <pubDate>Wed, 24 Dec 2025 01:49:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251224-surge-ai-startup-lesson/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Lenny&amp;rsquo;s Podcast 與 Surge AI 創辦人 Edwin Chen 的訪談。&#xA;收聽連結：&lt;a href=&#34;https://www.youtube.com/watch?v=dduQeaqmpnI&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;4 年內營收突破 10 億美元。員工不到 100 人。從第一天就獲利。更重要的是——他們一毛錢的創投資金都沒拿過。&lt;/p&gt;&#xA;&lt;p&gt;這組數字放在任何產業都很驚人，放在矽谷更是近乎異端。當整個科技圈都在談「閃電擴張」、追逐獨角獸估值、用大量資金換取成長時，Surge AI 創辦人 Edwin Chen 選擇了一條完全相反的路。他不募資、不做公關、不追風口，只專注做一件事：提供訓練 AI 模型所需的高品質資料。&lt;/p&gt;&#xA;&lt;p&gt;結果是，他們成了史上最快達到 10 億美元營收的公司之一——而且你可能到最近才聽過這家公司的名字。&lt;/p&gt;&#xA;&lt;h2 id=&#34;他們到底在做什麼&#34;&gt;他們到底在做什麼&lt;/h2&gt;&#xA;&lt;p&gt;Surge AI 做的事情，用一句話說就是：「教 AI 什麼是好、什麼是壞。」&lt;/p&gt;&#xA;&lt;p&gt;每一個你用過的大型語言模型——ChatGPT、Claude、Gemini——都經過一個叫做「後訓練」（post-training）的過程。模型先從網路上學習大量文字，但這只讓它學會「預測下一個字」，不代表它知道什麼樣的回答才是好的。後訓練就是教會模型分辨品質的過程，包括 SFT（監督微調）、RLHF（人類回饋強化學習）、設計評估標準等等。&lt;/p&gt;&#xA;&lt;p&gt;Surge AI 就是這個後訓練環節的關鍵供應商。他們招募各領域的專家——物理學家、詩人、軟體工程師——讓這些人與 AI 模型互動，評估模型的回答，提供高品質的訓練資料。所有主要的 AI 實驗室都是他們的客戶。&lt;/p&gt;&#xA;&lt;p&gt;這聽起來像是「資料標註」，但 Edwin 很討厭這個詞。「資料標註讓人想到標註貓的照片、在汽車周圍畫框框這種簡單工作。但我們做的完全不同——我們是在養育人類的孩子。」他這樣形容。養小孩不只是餵他吃東西，而是教他價值觀、創造力、什麼是美。訓練 AI 也是一樣。&lt;/p&gt;&#xA;&lt;h2 id=&#34;品質的定義決定了一切&#34;&gt;品質的定義，決定了一切&lt;/h2&gt;&#xA;&lt;p&gt;Edwin 認為 Surge 成功的核心原因是：他們對「品質」的定義，跟其他人完全不一樣。&lt;/p&gt;&#xA;&lt;p&gt;「大部分人不理解品質在這個領域是什麼意思。他們以為可以用人海戰術，丟一堆人去做就能得到好資料。這完全錯了。」他舉了一個例子：假設你要訓練模型寫一首關於月亮的八行詩，什麼叫「好」？如果你不深入思考品質，你會這樣檢查——這是一首詩嗎？有八行嗎？有提到月亮嗎？都符合，那就是好詩。&lt;/p&gt;&#xA;&lt;p&gt;但這不是 Surge 追求的。「我們要的是諾貝爾獎等級的詩。這首詩獨特嗎？有細膩的意象嗎？會讓你驚喜、觸動你的心嗎？會教你一些關於月光本質的事情嗎？會玩弄你的情緒、讓你思考嗎？」這種品質很難測量，非常主觀，而且標準極高。但 Edwin 認為，這才是我們真正希望 AI 能做到的事情。&lt;/p&gt;&#xA;&lt;p&gt;為了測量這種品質，Surge 建立了複雜的系統，收集每個工作者的數千個信號——他們的背景、專業領域、打字速度、回答方式，以及最重要的：他們產出的資料是否真的讓模型變得更好。「我們最終會知道你擅長寫詩、還是擅長寫論文、還是擅長寫技術文件。」Edwin 說。這就像 Google 搜尋用無數信號來判斷網頁品質一樣——不只是過濾垃圾，更要找出最頂尖的內容。&lt;/p&gt;&#xA;&lt;h2 id=&#34;反矽谷的創業哲學&#34;&gt;反矽谷的創業哲學&lt;/h2&gt;&#xA;&lt;p&gt;「我一直很討厭矽谷的那套說法。」Edwin 說得很直接。&lt;/p&gt;&#xA;&lt;p&gt;標準的創業劇本是：快速找到產品市場契合度，可能每兩週就要 pivot 一次；用各種手段追求成長和互動；用閃電擴張的方式盡快招人。Edwin 完全不同意這些。他的建議是：不要 pivot，不要閃電擴張，不要請那些只想在履歷上加一家熱門公司的史丹佛畢業生。「就專心做一件只有你能做的事——一件沒有你的洞見和專業就不會存在的事。」&lt;/p&gt;</description>
    </item>
    <item>
      <title>Codex 用 Codex 來訓練自己——AI 自我改進的第一個徵兆</title>
      <link>https://bnextmedia.github.io/AINEXT/posts/20251224-openai-codex-trains-itself/</link>
      <pubDate>Wed, 24 Dec 2025 01:34:00 +0800</pubDate>
      <guid>https://bnextmedia.github.io/AINEXT/posts/20251224-openai-codex-trains-itself/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;本文整理自 Lenny&amp;rsquo;s Podcast 2024 年 12 月播出的單集，主持人 Lenny Rachitsky 專訪 OpenAI Codex 產品負責人 Alexander Embiricos。&#xA;收聽連結：&lt;a href=&#34;https://www.youtube.com/watch?v=z1ISq9Ty4Cg&#34;&gt;YouTube&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;ai-在值班&#34;&gt;AI 在值班&lt;/h2&gt;&#xA;&lt;p&gt;OpenAI 內部有一個有趣的現象：Codex 正在幫忙訓練 Codex。&lt;/p&gt;&#xA;&lt;p&gt;這不是一個概念性的描述，而是字面上的意思。Alexander Embiricos 在訪談中提到，Codex 寫了很多管理自己訓練運作的程式碼。更具體地說，他們讓 Codex「值班」——持續監控訓練過程中的各種圖表和指標，評估這些數據隨時間的變化，然後判斷需要採取什麼行動。&lt;/p&gt;&#xA;&lt;p&gt;這代表什麼？想像一下訓練大型語言模型的場景。訓練過程會產生大量監控數據：loss 曲線、梯度變化、記憶體使用、GPU 利用率。傳統上，這些數據需要人類工程師盯著看，發現異常時做判斷——該調整學習率嗎？哪裡有 bug？需要重啟某個節點嗎？&lt;/p&gt;&#xA;&lt;p&gt;現在，Codex 可以做這件事。它不只是被動地跑程式碼，而是主動地監控、分析、做決策。Embiricos 說，Codex 的 code review 功能已經抓到了不少錯誤，包括一些「相當有趣的配置錯誤」。這些是人類工程師可能會漏掉的、但 agent 因為持續監控而能夠發現的問題。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;karpathy-的-bug&#34;&gt;Karpathy 的 Bug&lt;/h2&gt;&#xA;&lt;p&gt;這種能力的具體威力，可以從 Andrej Karpathy 的經驗看出來。&lt;/p&gt;&#xA;&lt;p&gt;Karpathy 是 OpenAI 的共同創辦人、前特斯拉 AI 總監，是這個領域最頂尖的人之一。他在 Twitter 上分享過：他遇到最棘手的 bug——那種花好幾個小時也搞不清楚原因的問題——他會丟給 Codex，讓它跑一個小時。結果 Codex 把問題解決了。&lt;/p&gt;&#xA;&lt;p&gt;關鍵不是 Codex 比 Karpathy 聰明，而是它可以用不同方式工作。持續嘗試、不會累、不會分心、不會因為挫折失去耐心。當一個問題需要的是大量試錯和排查，而不是天才級的洞察，這種「持久力」就變成優勢。&lt;/p&gt;&#xA;&lt;p&gt;把這個能力應用到訓練監控上，你得到的是一個永遠不會疲倦的值班工程師。它可以 24 小時盯著訓練曲線，在任何異常發生的第一時間就做出反應。人類工程師需要睡覺、需要休息、注意力會分散。Codex 不需要。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;遞迴改進的早期形態&#34;&gt;遞迴改進的早期形態&lt;/h2&gt;&#xA;&lt;p&gt;這讓我想到一個更大的問題：我們是不是在看 AI 遞迴自我改進的早期形態？&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
