---
title: "一篇論文，拆出了 Anthropic：Scaling Laws 背後的人與路線之爭"
date: 2020-01-23T01:00:00+08:00
description: "Scaling Laws 論文的十位作者後來分裂成兩個陣營：一邊跟著 Dario Amodei 創辦 Anthropic，帶走了對規模的理解和對安全的執著；另一邊留在 OpenAI 繼續推動商業化。第一作者 Jared Kaplan 從弦理論物理學家變成 Anthropic 首席科學家，用六年前畫出的冪律曲線改變了一個產業，然後花接下來的時間確保它不會失控。"
tags: ["Scaling Laws", "Anthropic", "Dario Amodei", "Jared Kaplan", "AI 經典文獻"]
categories: ["AI 技術前沿"]
image: "/images/posts/20200123-scaling-laws-people-and-impact.webp"
source_url: "https://arxiv.org/abs/2001.08361"
source_name: "arXiv"
related_companies: ["openai", "anthropic"]
draft: false
---

> 本文為「AI 經典文獻回顧」系列第十篇（下），聚焦 Scaling Laws 論文作者群的故事與這篇論文對 AI 產業的深遠影響。上篇見〈一篇物理學家寫的論文，如何給了矽谷砸千億美元的數學底氣〉。

![封面圖](/images/posts/20200123-scaling-laws-people-and-impact.webp)

---

## 一篇論文，拆出了 Anthropic

2021 年初，一群 OpenAI 的核心研究者集體離開，創辦了 Anthropic。領頭的是 Dario Amodei，當時 OpenAI 的研究副總裁。跟著走的有 Jared Kaplan——Scaling Laws 論文的第一作者、約翰霍普金斯大學的理論物理學教授。還有 Sam McCandlish——Scaling Laws 論文的第二作者、另一位物理學博士。Tom Brown——GPT-3 論文的第一作者。以及 Jack Clark、Daniela Amodei、Benjamin Mann 等人，加起來大約十一個人。

這不是普通的跳槽。這群人帶走的不只是履歷上的光環，而是 OpenAI 當時最核心的技術資產：對 Scaling Laws 的深刻理解，以及把這個理解轉化為實際模型的工程能力。Scaling Laws 論文的前兩位作者和 GPT-3 論文的第一作者同時離開，等於是把 OpenAI 在「規模」這條路線上最懂行的人帶走了一半。

要理解這場出走為什麼發生、它為什麼重要，得先回到 Scaling Laws 論文背後那群人的故事。

## 從弦理論到語言模型：Jared Kaplan 的跨界之路

上一篇我們提到，Jared Kaplan 是約翰霍普金斯大學的理論物理學教授，研究弦理論和量子場論。但這只是他故事的一半。更值得追問的是：一個正在大學裡拿終身教職的物理學家，怎麼會跑去矽谷的 AI 實驗室？

答案跟 2018 年前後深度學習領域的一個微妙變化有關。那幾年，越來越多的物理學家開始注意到，深度學習中有些現象跟物理學裡的某些模式驚人地相似。神經網路的損失曲面（loss landscape）長得像統計力學中的能量地形，大模型的行為跟物理學中的臨界現象有類比關係，而模型表現隨規模變化的模式看起來很像物理學家熟悉的冪律。

Kaplan 不是第一個注意到這些相似性的人，但他可能是第一個認真拿物理學的分析工具去做系統性測量的人。他在 2019 年加入 OpenAI 擔任研究員，同時保留了約翰霍普金斯的教職。這個雙重身份本身就不尋常：他既是學術圈內有終身教職保障的物理學家，又是 AI 前沿實驗室的一線研究者。

在 OpenAI，Kaplan 很快找到了志同道合的人。Sam McCandlish 同樣有物理學博士背景，在加州大學柏克萊分校研究理論物理，博士論文也涉及統計力學。兩個物理學家在 AI 實驗室相遇，用他們從物理學帶來的方法論——系統性地測量、找冪律、建構理論框架——開始研究語言模型的行為。Scaling Laws 論文就是這個合作的直接產物。

這段經歷讓 Kaplan 看見了一件事：AI 研究不再只是工程和電腦科學的領地，它正在變成一門需要物理學思維的科學。不是「怎麼造一個更好的模型」，而是「模型的行為遵循什麼基本法則」。這個視角後來成了 Anthropic 的研究 DNA 的一部分。

## 論文的十位作者：一張 AI 產業地圖

Scaling Laws 論文掛了十個名字，但名字背後的故事更精彩。除了 Kaplan 和 McCandlish 之外，其他八位作者的後續去向，幾乎就是過去五年 AI 產業發展的縮影。

Dario Amodei 是論文的最後一位作者，在學術界這通常代表「通訊作者」或實驗室主持人的角色。他當時是 OpenAI 的研究副總裁，負責監督包括 Scaling Laws 在內的多個重要研究項目。Dario 的背景也很有意思：他在普林斯頓大學拿的是計算神經科學博士，研究的是生物神經網路，然後轉進 AI 領域。在 OpenAI 期間，他逐漸形成了一個越來越強烈的信念：大規模 AI 系統的安全問題是真實而緊迫的，而 OpenAI 在商業化壓力下越來越不願意認真面對這些問題。

Tom Brown 不在 Scaling Laws 的作者名單上（他的名字是 Tom B. Brown，在論文中列為 Tom Henighan），但他是同一時期另一篇改變歷史的論文的第一作者——GPT-3。Brown 的背景同樣跨界：他在麻省理工學院同時拿了電腦科學和腦與認知科學的雙碩士。他在 OpenAI 主導了 GPT-3 的工程實現，把 Scaling Laws 的理論預測變成了一個 1,750 億參數的實際模型。如果說 Kaplan 畫出了那條冪律曲線，Brown 就是驗證那條曲線的人。

Tom Henighan 是論文的第三作者，也是後來跟著 Dario 離開的人之一。Benjamin Chess 和 Rewon Child 同樣是 OpenAI 的研究員，後來分別走上了不同的道路。Alec Radford 是 GPT 系列模型的早期架構設計者，他最知名的貢獻是 GPT-1 和 GPT-2 的設計。Scott Gray 專精 GPU 計算優化。Jeffrey Wu 後來成了 InstructGPT（RLHF 方法的先驅）的核心貢獻者。

把這些人的去向連起來看，你會看到一個有趣的圖景：Scaling Laws 論文的作者群分裂成了至少兩個陣營。一邊跟著 Dario 去了 Anthropic，帶走了對 scaling 的深刻理解和對安全的執著。另一邊留在 OpenAI，繼續推動 GPT 系列的商業化。這兩家公司後來成了 AI 領域最激烈的競爭者，而它們的技術根基可以追溯到同一篇論文。

## 一場關於安全的路線之爭

Anthropic 的創辦，表面上是人才流動，深層原因是路線之爭。而 Scaling Laws 恰恰是這場路線之爭的核心。

Scaling Laws 論文告訴所有人一件事：AI 的能力會隨著規模的增加而可預測地提升。這聽起來是好消息——你可以規劃投資、可以預測進度。但它同時也意味著一件令人不安的事：如果你繼續沿著這條曲線走下去，遲早會訓練出能力遠超人類的系統。而且你大致知道什麼時候會到達那個點，因為曲線是可以外推的。

對 Dario Amodei 來說，這不是理論上的擔憂，而是一個迫在眉睫的工程問題。如果你知道更大的模型更強，你也知道更強的模型更危險，那在你繼續把模型做大之前，你最好先搞清楚怎麼控制它。他在 OpenAI 內部反覆提出這個觀點，但隨著公司從非營利轉向「受限營利」（capped-profit），商業壓力越來越大，快速發布新產品的需求開始壓過安全研究的需求。

Kaplan 和 McCandlish 有類似的感受。作為 Scaling Laws 的作者，他們比任何人都清楚這條曲線的含義。他們用數學證明了規模的力量，但他們也因此比其他人更早感受到規模帶來的風險。McCandlish 後來在訪談中提到，他們在 OpenAI 的時候就已經開始擔心：我們正在建造越來越強大的系統，但我們對這些系統的理解並沒有以同樣的速度增長。

2021 年初，這群人做出了決定。他們離開 OpenAI，帶著對 Scaling Laws 的理解和對安全的焦慮，創辦了 Anthropic。公司的名字來自「anthropic principle」（人擇原理），暗示他們關注的是 AI 對人類的影響。公司的使命很明確：繼續沿著 Scaling Laws 的方向前進——因為他們相信更強的 AI 會被造出來，不管是不是他們自己造的——但要用更嚴謹的安全方法來做。

## 從 Scaling Laws 到 Constitutional AI

在 Anthropic，Kaplan 的角色是首席科學家。他把在 OpenAI 研究 Scaling Laws 的經驗帶進了一個新的問題：如果模型的能力會隨規模可預測地增長，模型的安全性是否也能隨著投入更多資源而可預測地提升？

這個問題的答案催生了 Anthropic 最重要的技術貢獻之一：Constitutional AI（我們在這個系列後面會詳細介紹）。簡單來說，Constitutional AI 是一種讓 AI 系統自我糾正的方法：你給模型一套「憲法」——一組原則和規範——然後讓模型自己判斷什麼樣的回答違反了這些原則，並學著避免。

這個方法的哲學和 Scaling Laws 一脈相承：不是靠人類手工標註每一種可能的危險情境（那就是薩頓批評的「嵌入人類知識」的老路），而是設計一個通用的框架，讓系統自己從規模中學會安全。用 Kaplan 的話說，安全應該是一個能隨模型規模擴展的系統，而不是一堆需要工程師逐個修補的補丁。

到了 2024 年 10 月，Anthropic 做了一件意味深長的事：任命 Kaplan 為公司的「負責任擴展官」（Responsible Scaling Officer）。這個頭銜本身就是一個宣言——它把「scaling」和「responsible」綁在一起，承認了一個現實：他們不會停止擴展，但他們會為擴展設定安全閘門。Kaplan 的工作是評估每一代新模型的危險程度，並決定需要什麼級別的安全措施才能發布。

這讓 Kaplan 處於一個特殊的位置。他在 2020 年用數學證明了 scaling 的力量，然後花了接下來四年的時間建造安全護欄來約束這個力量。寫冪律方程式的人，現在負責決定什麼時候該踩煞車。

## Scaling Laws 如何改變了 AI 產業的權力結構

Scaling Laws 論文對 AI 產業的影響遠超過學術引用次數。它從根本上改變了這個產業的權力結構。

在 Scaling Laws 之前，AI 研究的競爭優勢主要來自創意和人才。一個小團隊如果有好的想法，可以在一張 GPU 上訓練出突破性的模型——就像 2012 年的 AlexNet，三個人用兩張遊戲顯示卡改變了整個領域。但 Scaling Laws 表明，模型的表現主要由三個可以用錢買到的東西決定：更大的模型（需要更多 GPU 記憶體）、更多的數據（需要更多儲存和處理能力）、更多的計算（需要更多 GPU 和更多電力）。

這意味著 AI 研究的門檻從「有好的想法」變成了「有很多錢」。訓練 GPT-3 的成本估計在 460 萬到 1,200 萬美元之間。GPT-4 的訓練成本據估計超過一億美元。到了 2025 年的前沿模型，單次訓練的成本可能已經超過十億美元。

這直接導致了 AI 研究的寡頭化。2012 年，一個博士生可以在臥室裡訓練出改變歷史的模型。2026 年，只有少數幾家公司——OpenAI、Anthropic、Google DeepMind、Meta AI——有能力訓練前沿語言模型。學術界被遠遠甩在後面，除非他們能拿到大公司的資源。

Scaling Laws 還催生了一場全球性的基礎設施軍備競賽。如果你知道更多算力等於更強的 AI，那控制算力就成了國家戰略層面的議題。美國對中國的晶片出口管制、各國政府投資主權 AI 算力、科技巨頭爭搶 GPU 供應和電力資源——這些在 2025 年和 2026 年佔據新聞頭條的事件，其底層邏輯都可以追溯到 Kaplan 在 2020 年畫出的那幾條冪律曲線。

微軟在 2023 年對 OpenAI 追加百億美元投資時，背後的計算很清楚：Scaling Laws 預測更大的模型表現更好，訓練更大的模型需要更多算力，更多算力需要更多錢。把錢投入 OpenAI，就是沿著冪律曲線買入 AI 的未來。Google、Amazon、Meta 各自跟進的投資邏輯如出一轍。到了 2025 年初，川普政府宣布的 Stargate 計畫——投入高達五千億美元建設 AI 基礎設施——更是把 Scaling Laws 的邏輯推到了國家政策的層面。

## 2026 年的 Kaplan：寫冪律的人在煞車

站在 2026 年 2 月回望，這篇論文發表至今剛好滿六年。六年間，它的作者們分散到了 AI 產業的不同角落，但他們在 2020 年 1 月共同畫出的那幾條冪律曲線，仍然是整個產業的底層作業系統。

Jared Kaplan 在 2025 年被《時代》雜誌選入「AI 領域最具影響力的 100 人」。他仍然同時擔任 Anthropic 的首席科學家和約翰霍普金斯大學的副教授，在大學裡教深度學習基礎課程。他的身分本身就是一個隱喻：一個腳踏學術界和產業界的人，一邊研究 AI 的基本法則，一邊為這些法則的後果設定安全界線。

Sam McCandlish 在 2025 年 10 月從 Anthropic 的研究領導者轉任首席架構師（Chief Architect），負責公司整體的技術架構。Tom Brown 仍然是 Anthropic 的核心技術領導者。Dario Amodei 則在 2025 年底發表了一篇長文，描繪了一幅強大 AI 可以加速科學研究、改善全球健康、推動經濟繁榮的樂觀願景——但前提是安全問題得到妥善解決。

回到 OpenAI 那一邊，Alec Radford 和其他留下來的 Scaling Laws 作者繼續推動了 GPT-4 和後續模型的開發。兩家公司在 2025 年和 2026 年的競爭愈發激烈，但有趣的是，它們的競爭方式——訓練更大的模型、投入更多的計算資源——恰恰是 Scaling Laws 預測的路線。

## 精確測量，但不理解

回顧整個 Scaling Laws 的故事，最值得深思的或許是一個弔詭。

Kaplan 用物理學家的方法精確測量了語言模型的行為規律，畫出了改變產業的冪律曲線。但到今天為止，沒有人能從第一原理出發解釋為什麼神經網路的表現會遵循冪律。這不像牛頓力學，你可以從質量和引力常數推導出行星軌道。冪律曲線是一個經驗觀察，不是一個理論預測。

我們在上一篇聊到薩頓的〈苦澀的教訓〉時，用過一個比喻：古埃及人能精確測量太陽的軌跡，但不知道地球繞著太陽轉。Scaling Laws 的處境非常類似。我們能精確預測增加十倍算力會帶來多少表現提升，但不知道為什麼。我們能用這條曲線規劃數千億美元的投資，但如果有一天曲線突然彎折了，我們甚至不知道該怎麼解釋。

這也是為什麼 Kaplan 後來的工作——負責任的擴展、安全評估、設定 AI 安全等級——如此重要。他比誰都清楚 Scaling Laws 既強大又脆弱：強大到足以支撐整個產業的投資邏輯，脆弱到可能在下一個數量級上出現意料之外的行為。

一個物理學家用六年前寫的一篇論文改變了一個產業，然後花接下來的時間試圖確保這個改變不會失控。這或許就是 Scaling Laws 這個故事最好的結尾——只不過它還遠遠沒有結束。

我們在下一篇會介紹 Gwern 的〈The Scaling Hypothesis〉，一位匿名部落客如何把 Scaling Laws 的經驗觀察推演成一個大膽的哲學假說。
